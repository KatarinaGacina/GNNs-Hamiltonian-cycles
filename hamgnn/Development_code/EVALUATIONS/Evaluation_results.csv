computed_by,date,description,evaluation_data,final_loss_exp,final_loss_trend,final_loss_var,id,iterations_in_epoch,learn_rate,loss_description,loss_name,model_name,model_raw,optimizer_name,optimizer_raw,perc_full_walks_found_10,perc_full_walks_found_100,perc_full_walks_found_15,perc_full_walks_found_20,perc_full_walks_found_25,perc_full_walks_found_30,perc_full_walks_found_35,perc_full_walks_found_40,perc_full_walks_found_45,perc_full_walks_found_5,perc_full_walks_found_50,perc_full_walks_found_60,perc_full_walks_found_70,perc_full_walks_found_80,perc_full_walks_found_90,perc_hamilton_found_10,perc_hamilton_found_100,perc_hamilton_found_15,perc_hamilton_found_20,perc_hamilton_found_25,perc_hamilton_found_30,perc_hamilton_found_35,perc_hamilton_found_40,perc_hamilton_found_45,perc_hamilton_found_5,perc_hamilton_found_50,perc_hamilton_found_60,perc_hamilton_found_70,perc_hamilton_found_80,perc_hamilton_found_90,perc_long_cycles_found_10,perc_long_cycles_found_100,perc_long_cycles_found_15,perc_long_cycles_found_20,perc_long_cycles_found_25,perc_long_cycles_found_30,perc_long_cycles_found_35,perc_long_cycles_found_40,perc_long_cycles_found_45,perc_long_cycles_found_5,perc_long_cycles_found_50,perc_long_cycles_found_60,perc_long_cycles_found_70,perc_long_cycles_found_80,perc_long_cycles_found_90,perc_long_walks_found_10,perc_long_walks_found_100,perc_long_walks_found_15,perc_long_walks_found_20,perc_long_walks_found_25,perc_long_walks_found_30,perc_long_walks_found_35,perc_long_walks_found_40,perc_long_walks_found_45,perc_long_walks_found_5,perc_long_walks_found_50,perc_long_walks_found_60,perc_long_walks_found_70,perc_long_walks_found_80,perc_long_walks_found_90,sample_method,train_description,train_epochs,train_graphs_seen,train_time(s),weights_path,train_example_details,batch_size
godot,2021/03/31 14:09:24,Embed-Process model trained with reinforcement for finding the optimal depth,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",12.976396240234374,0.7058000488281252,18.330062332217807,1617199764928511159#140585707288384,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
=================================================================
Total params: 4,192
Trainable params: 4,192
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
=================================================================
Total params: 4,192
Trainable params: 4,192
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.055,0.0,0.03,0.005,0.0,0.005,0.0,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.3,0.0,0.085,0.03,0.0,0.01,0.0,0.0,0.0,0.855,0.0,0.0,0.0,0.0,0.0,0.595,0.01,0.32,0.31,0.155,0.16,0.075,0.1,0.05,0.855,0.035,0.035,0.015,0.01,0.005,0.145,0.015,0.08,0.105,0.065,0.095,0.075,0.06,0.045,0.005,0.055,0.04,0.05,0.025,0.01,greedy,Train for 100 on 100,100.0,100.0,221.454278228,EVALUATIONS/WEIGHTS_BACKUP/1617199764928511159#140585707288384,,1.0
godot,2021/03/31 14:10:58,Embed-Process model trained with reinforcement for finding the optimal depth,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",21.791396240234373,0.8315996093750002,37.582233637462025,1617199858886782407#139953334183744,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,0.0,0.175,0.12,0.065,0.035,0.02,0.015,0.005,0.015,0.01,0.0,0.0,0.0,0.0,0.555,0.0,0.295,0.18,0.09,0.055,0.025,0.0,0.01,0.88,0.0,0.0,0.0,0.0,0.0,0.555,0.0,0.305,0.2,0.11,0.08,0.03,0.01,0.01,0.88,0.005,0.0,0.0,0.0,0.0,0.15,0.01,0.185,0.255,0.185,0.2,0.1,0.125,0.095,0.015,0.085,0.055,0.01,0.005,0.015,greedy,Train for 100 on 100,100.0,100.0,315.376074522,EVALUATIONS/WEIGHTS_BACKUP/1617199858886782407#139953334183744,,1.0
godot,2021/03/31 14:12:59,Embed-Process model trained with reinforcement for finding the optimal depth,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",15.695392944335936,-0.6389991455078124,36.14722975178458,1617199979987723452#139852851255104,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
=================================================================
Total params: 12,576
Trainable params: 12,576
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
=================================================================
Total params: 12,576
Trainable params: 12,576
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.35,0.0,0.31,0.195,0.085,0.075,0.03,0.035,0.015,0.045,0.015,0.005,0.0,0.0,0.0,0.23,0.0,0.075,0.035,0.02,0.01,0.015,0.01,0.005,0.85,0.0,0.0,0.0,0.0,0.0,0.34,0.035,0.185,0.175,0.105,0.1,0.095,0.09,0.065,0.85,0.035,0.035,0.075,0.055,0.035,0.355,0.335,0.32,0.445,0.31,0.36,0.285,0.375,0.305,0.045,0.42,0.405,0.35,0.405,0.36,greedy,Train for 100 on 100,100.0,100.0,436.430425452,EVALUATIONS/WEIGHTS_BACKUP/1617199979987723452#139852851255104,,1.0
godot,2021/03/31 14:14:35,Embed-Process model trained with reinforcement for finding the optimal depth,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",17.15659606933594,-2.8654033203125,69.88704988970329,1617200075734803859#140324628326208,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
=================================================================
Total params: 16,768
Trainable params: 16,768
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
=================================================================
Total params: 16,768
Trainable params: 16,768
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,0.0,0.155,0.115,0.135,0.055,0.035,0.03,0.01,0.02,0.005,0.01,0.0,0.0,0.0,0.47,0.0,0.29,0.145,0.07,0.055,0.035,0.02,0.01,0.875,0.01,0.0,0.0,0.0,0.0,0.51,0.0,0.335,0.22,0.095,0.065,0.065,0.04,0.04,0.875,0.03,0.0,0.0,0.0,0.0,0.145,0.48,0.16,0.395,0.385,0.52,0.43,0.625,0.475,0.02,0.565,0.575,0.545,0.53,0.495,greedy,Train for 100 on 100,100.0,100.0,532.17353424,EVALUATIONS/WEIGHTS_BACKUP/1617200075734803859#140324628326208,,1.0
godot,2021/03/31 14:13:51,Embed-Process model trained with reinforc for finding the optimal probability of Hamilton cycle existance in train graphs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",9.24999658203125,-0.0193999023437498,19.616021301224905,1617200031069948908#139953418659648,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.53,0.0,0.41,0.23,0.08,0.055,0.04,0.015,0.02,0.89,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9,0.28,0.72,0.755,0.585,0.62,0.49,0.555,0.425,0.89,0.49,0.415,0.41,0.335,0.33,greedy,Train for 100 on 100,100.0,100.0,487.504827771,EVALUATIONS/WEIGHTS_BACKUP/1617200031069948908#139953418659648,,1.0
godot,2021/03/31 14:17:04,Embed-Process model trained with reinforc for finding the optimal probability of Hamilton cycle existance in train graphs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",29.1766064453125,0.80540234375,23.708504154417483,1617200224502477693#140333003200320,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.355,0.0,0.25,0.185,0.095,0.025,0.025,0.005,0.005,0.645,0.02,0.0,0.005,0.0,0.0,0.355,0.005,0.255,0.205,0.12,0.035,0.035,0.03,0.015,0.645,0.025,0.01,0.01,0.0,0.0,0.285,0.015,0.19,0.15,0.09,0.175,0.1,0.105,0.055,0.0,0.07,0.07,0.02,0.025,0.035,greedy,Train for 100 on 100,100.0,100.0,680.875706838,EVALUATIONS/WEIGHTS_BACKUP/1617200224502477693#140333003200320,,1.0
godot,2021/03/31 14:15:24,Embed-Process model trained with reinforc for finding the optimal probability of Hamilton cycle existance in train graphs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",14.501394775390626,2.4678016357421875,26.045875961086523,1617200124291478150#140581845047104,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.18,0.0,0.16,0.14,0.135,0.07,0.035,0.04,0.025,0.015,0.015,0.0,0.0,0.0,0.0,0.49,0.0,0.37,0.185,0.075,0.08,0.025,0.045,0.015,0.88,0.005,0.0,0.005,0.0,0.0,0.74,0.15,0.555,0.54,0.365,0.41,0.285,0.31,0.215,0.88,0.215,0.18,0.155,0.175,0.175,0.205,0.295,0.255,0.33,0.355,0.36,0.35,0.395,0.45,0.015,0.455,0.41,0.32,0.375,0.305,greedy,Train for 100 on 100,100.0,100.0,580.722103621,EVALUATIONS/WEIGHTS_BACKUP/1617200124291478150#140581845047104,,1.0
godot,2021/03/31 14:16:25,Embed-Process model trained with reinforcement for finding the optimal depth,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",25.815000000000005,-1.0052004394531244,44.17846076937417,1617200185581816269#140064334923584,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.11,0.0,0.09,0.065,0.06,0.025,0.02,0.015,0.0,0.015,0.0,0.0,0.0,0.0,0.0,0.44,0.0,0.19,0.09,0.035,0.025,0.015,0.01,0.0,0.88,0.0,0.0,0.0,0.0,0.0,0.725,0.045,0.41,0.285,0.27,0.165,0.18,0.165,0.145,0.88,0.12,0.08,0.08,0.05,0.045,0.165,0.195,0.24,0.34,0.24,0.33,0.345,0.325,0.27,0.015,0.32,0.295,0.245,0.245,0.195,greedy,Train for 100 on 100,100.0,100.0,642.02116095,EVALUATIONS/WEIGHTS_BACKUP/1617200185581816269#140064334923584,,1.0
godot,2021/03/31 14:15:41,Embed-Process model trained with reinforc for finding the optimal probability of Hamilton cycle existance in train graphs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",16.879595458984376,0.3717996826171873,36.92294794975663,1617200141017424166#140542055536448,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.125,0.0,0.11,0.045,0.055,0.025,0.015,0.02,0.01,0.015,0.005,0.01,0.0,0.0,0.0,0.43,0.0,0.25,0.145,0.07,0.01,0.035,0.01,0.01,0.855,0.0,0.005,0.0,0.0,0.0,0.5,0.015,0.33,0.355,0.2,0.135,0.145,0.115,0.11,0.855,0.075,0.04,0.03,0.035,0.01,0.135,0.525,0.12,0.29,0.24,0.36,0.295,0.53,0.385,0.015,0.47,0.5,0.515,0.495,0.44,greedy,Train for 100 on 100,100.0,100.0,597.427320397,EVALUATIONS/WEIGHTS_BACKUP/1617200141017424166#140542055536448,,1.0
godot,2021/03/31 14:16:32,Embed-Process model trained with reinforc for finding the optimal probability of Hamilton cycle existance in train graphs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",30.08980029296875,5.86320166015625,38.65914829178894,1617200192126073500#139673320453952,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.565,0.0,0.385,0.255,0.175,0.05,0.065,0.01,0.025,0.88,0.01,0.0,0.0,0.005,0.0,0.565,0.0,0.39,0.255,0.18,0.065,0.065,0.025,0.035,0.88,0.025,0.005,0.005,0.005,0.005,0.15,0.295,0.145,0.37,0.3,0.405,0.345,0.375,0.345,0.0,0.395,0.335,0.35,0.32,0.285,greedy,Train for 100 on 100,100.0,100.0,648.534504199,EVALUATIONS/WEIGHTS_BACKUP/1617200192126073500#139673320453952,,1.0
godot,2021/03/31 14:20:14,Embed-Process model trained with reinforcement for finding the optimal depth,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",20.996197631835933,-2.2700024414062496,111.84359670903518,1617200414446756922#140328183093056,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.02,0.0,0.03,0.025,0.01,0.005,0.0,0.01,0.01,0.005,0.01,0.005,0.0,0.0,0.0,0.38,0.0,0.225,0.115,0.055,0.02,0.01,0.01,0.02,0.875,0.015,0.0,0.0,0.0,0.0,0.455,0.01,0.335,0.25,0.18,0.16,0.11,0.095,0.115,0.875,0.085,0.035,0.04,0.01,0.025,0.22,0.365,0.235,0.425,0.3,0.34,0.39,0.435,0.36,0.005,0.385,0.34,0.35,0.355,0.315,greedy,Train for 100 on 100,100.0,100.0,870.884693472,EVALUATIONS/WEIGHTS_BACKUP/1617200414446756922#140328183093056,,1.0
godot,2021/03/31 14:25:55,Embed-Process model trained with reinforcement for finding the optimal depth,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",33.9002021484375,-1.8794003906250003,16.853723644889214,1617200755437641499#140342751389504,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.0,0.425,0.27,0.24,0.125,0.07,0.06,0.055,0.79,0.02,0.015,0.005,0.0,0.0,0.585,0.01,0.45,0.36,0.29,0.2,0.12,0.15,0.125,0.79,0.06,0.085,0.025,0.025,0.03,0.13,0.175,0.145,0.23,0.205,0.27,0.265,0.305,0.195,0.0,0.315,0.185,0.22,0.25,0.185,greedy,Train for 100 on 100,100.0,100.0,1211.867351494,EVALUATIONS/WEIGHTS_BACKUP/1617200755437641499#140342751389504,,1.0
godot,2021/04/01 08:43:52,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",25.223512939453123,3.004603271484376,61.10432941778584,1617266632892235838#139930451814208,200.0,0.001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",0.045,0.0,0.025,0.02,0.005,0.0,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.28,0.105,0.065,0.035,0.005,0.005,0.005,0.88,0.0,0.0,0.0,0.0,0.0,0.47,0.0,0.285,0.155,0.1,0.05,0.01,0.005,0.005,0.88,0.005,0.0,0.0,0.0,0.0,0.135,0.0,0.22,0.205,0.095,0.105,0.09,0.075,0.025,0.0,0.04,0.025,0.005,0.0,0.0,greedy,Train for 200 on 200,200.0,200.0,3459.611151825,EVALUATIONS/WEIGHTS_BACKUP/1617266632892235838#139930451814208,,1.0
godot,2021/04/01 08:45:56,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",19.26690771484375,-3.697206298828125,53.97829192618451,1617266756564602201#140499014428480,200.0,1e-05,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.12,0.0,0.135,0.09,0.065,0.05,0.03,0.015,0.02,0.015,0.01,0.005,0.0,0.005,0.0,0.495,0.0,0.375,0.175,0.115,0.055,0.05,0.015,0.015,0.88,0.005,0.005,0.0,0.01,0.0,0.495,0.01,0.375,0.195,0.155,0.09,0.065,0.04,0.025,0.88,0.03,0.015,0.01,0.02,0.005,0.18,0.035,0.165,0.285,0.17,0.165,0.175,0.13,0.165,0.015,0.155,0.115,0.035,0.055,0.035,greedy,Train for 200 on 200,200.0,200.0,3583.258658392,EVALUATIONS/WEIGHTS_BACKUP/1617266756564602201#140499014428480,,1.0
godot,2021/04/01 08:42:20,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.2954969787597657,-2.167801025390625,2.0632893740837286,1617266540167267647#139837945018176,200.0,0.001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",0.04,0.0,0.02,0.03,0.0,0.005,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28,0.0,0.105,0.04,0.005,0.015,0.0,0.0,0.0,0.85,0.0,0.0,0.0,0.0,0.0,0.63,0.03,0.39,0.3,0.25,0.26,0.13,0.135,0.075,0.85,0.07,0.04,0.075,0.035,0.03,0.125,0.11,0.125,0.205,0.12,0.175,0.12,0.19,0.1,0.0,0.17,0.12,0.125,0.08,0.095,greedy,Train for 200 on 200,200.0,200.0,3366.738098149,EVALUATIONS/WEIGHTS_BACKUP/1617266540167267647#139837945018176,,1.0
godot,2021/04/01 08:52:06,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",30.188305908203127,-0.2094995117187494,116.9169225610258,1617267126451466351#139664382154560,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.06,0.0,0.095,0.105,0.09,0.09,0.07,0.09,0.08,0.005,0.06,0.035,0.015,0.015,0.005,0.68,0.0,0.61,0.39,0.275,0.195,0.15,0.11,0.075,0.88,0.08,0.02,0.02,0.0,0.0,0.69,0.0,0.62,0.42,0.3,0.235,0.175,0.13,0.09,0.88,0.1,0.04,0.025,0.005,0.005,0.12,0.175,0.135,0.26,0.225,0.3,0.315,0.365,0.35,0.005,0.365,0.35,0.305,0.32,0.27,greedy,Train for 200 on 200,200.0,200.0,3953.149060185,EVALUATIONS/WEIGHTS_BACKUP/1617267126451466351#139664382154560,,1.0
godot,2021/04/01 09:08:59,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",27.885511718750003,0.8561005859375002,1.607799259773515,1617268139381947768#139963277420352,200.0,1e-05,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.16,0.0,0.14,0.15,0.155,0.095,0.085,0.075,0.03,0.015,0.025,0.02,0.0,0.0,0.01,0.595,0.0,0.435,0.275,0.205,0.085,0.085,0.085,0.05,0.88,0.05,0.025,0.0,0.0,0.0,0.66,0.05,0.51,0.36,0.27,0.18,0.15,0.18,0.16,0.88,0.12,0.075,0.06,0.02,0.03,0.185,0.055,0.17,0.21,0.23,0.255,0.245,0.165,0.125,0.015,0.19,0.08,0.08,0.08,0.085,greedy,Train for 200 on 200,200.0,200.0,4965.878299038,EVALUATIONS/WEIGHTS_BACKUP/1617268139381947768#139963277420352,,1.0
godot,2021/04/01 09:18:39,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",30.11150119018555,2.2019985351562505,209.01054000435224,1617268719515673396#139723871647552,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.115,0.005,0.12,0.235,0.2,0.195,0.225,0.195,0.135,0.015,0.105,0.05,0.03,0.015,0.025,0.685,0.005,0.585,0.37,0.28,0.18,0.145,0.12,0.08,0.88,0.085,0.01,0.015,0.0,0.0,0.685,0.005,0.595,0.395,0.29,0.215,0.16,0.12,0.09,0.88,0.09,0.02,0.015,0.005,0.0,0.2,0.185,0.235,0.355,0.355,0.415,0.49,0.445,0.44,0.015,0.45,0.33,0.315,0.24,0.22,greedy,Train for 200 on 200,200.0,200.0,5546.016764331,EVALUATIONS/WEIGHTS_BACKUP/1617268719515673396#139723871647552,,1.0
godot,2021/04/01 10:02:14,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",25.729105957031248,0.1813007812500004,54.81726556063279,1617271334900463914#140286546433856,200.0,0.001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",0.145,0.0,0.195,0.145,0.135,0.09,0.08,0.075,0.08,0.015,0.04,0.015,0.02,0.005,0.0,0.575,0.005,0.45,0.36,0.205,0.16,0.095,0.065,0.06,0.88,0.035,0.02,0.02,0.01,0.005,0.63,0.015,0.5,0.39,0.26,0.215,0.15,0.11,0.095,0.88,0.1,0.035,0.035,0.01,0.02,0.21,0.12,0.25,0.3,0.275,0.26,0.285,0.305,0.225,0.015,0.205,0.2,0.125,0.17,0.125,greedy,Train for 200 on 500,500.0,500.0,8161.590831088,EVALUATIONS/WEIGHTS_BACKUP/1617271334900463914#140286546433856,,1.0
godot,2021/04/01 10:31:08,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",28.032114990234373,-1.56539697265625,42.467630054250776,1617273068402526947#140714347845440,200.0,1e-05,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.125,0.0,0.15,0.145,0.14,0.105,0.08,0.03,0.07,0.015,0.045,0.015,0.005,0.0,0.0,0.63,0.0,0.515,0.345,0.23,0.13,0.075,0.055,0.015,0.88,0.02,0.015,0.005,0.0,0.0,0.67,0.075,0.56,0.49,0.37,0.325,0.24,0.265,0.165,0.88,0.15,0.195,0.125,0.12,0.085,0.135,0.195,0.175,0.25,0.26,0.26,0.27,0.28,0.285,0.015,0.29,0.26,0.28,0.23,0.245,greedy,Train for 200 on 500,500.0,500.0,9895.000300105,EVALUATIONS/WEIGHTS_BACKUP/1617273068402526947#140714347845440,,1.0
godot,2021/04/01 10:44:51,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",34.767610351562496,-1.782697265625,21.05782149181709,1617273891096853894#139624487171904,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.0,0.085,0.07,0.04,0.05,0.05,0.065,0.045,0.01,0.065,0.01,0.03,0.01,0.02,0.585,0.0,0.495,0.41,0.27,0.21,0.145,0.08,0.06,0.88,0.045,0.025,0.015,0.005,0.0,0.585,0.01,0.505,0.43,0.28,0.23,0.145,0.085,0.07,0.88,0.05,0.035,0.02,0.015,0.0,0.105,0.185,0.085,0.145,0.13,0.245,0.195,0.24,0.2,0.01,0.31,0.19,0.215,0.175,0.155,greedy,Train for 200 on 500,500.0,500.0,10717.717450261,EVALUATIONS/WEIGHTS_BACKUP/1617273891096853894#139624487171904,,1.0
godot,2021/04/01 11:29:09,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",26.36330895996094,-0.3367000732421878,88.94784196782871,1617276549325834516#139686103308096,200.0,1e-05,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.085,0.0,0.075,0.05,0.025,0.015,0.01,0.005,0.005,0.01,0.0,0.0,0.0,0.0,0.0,0.49,0.0,0.31,0.12,0.05,0.04,0.005,0.005,0.0,0.88,0.0,0.0,0.0,0.0,0.0,0.545,0.0,0.345,0.19,0.12,0.14,0.075,0.05,0.015,0.88,0.015,0.01,0.0,0.005,0.0,0.175,0.0,0.125,0.205,0.175,0.15,0.09,0.055,0.055,0.01,0.05,0.04,0.005,0.01,0.005,greedy,Train for 200 on 500,500.0,500.0,13375.778927398,EVALUATIONS/WEIGHTS_BACKUP/1617276549325834516#139686103308096,,1.0
godot,2021/04/01 11:38:15,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",14.816003845214842,-2.714804321289063,159.39452744456568,1617277095297637022#140719417468736,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.505,0.0,0.36,0.205,0.115,0.055,0.035,0.03,0.01,0.745,0.015,0.01,0.0,0.0,0.0,0.505,0.0,0.365,0.21,0.12,0.055,0.04,0.03,0.01,0.745,0.015,0.01,0.0,0.0,0.0,0.22,0.01,0.08,0.16,0.115,0.13,0.085,0.06,0.05,0.0,0.07,0.02,0.025,0.0,0.005,greedy,Train for 200 on 500,500.0,500.0,13921.764978424,EVALUATIONS/WEIGHTS_BACKUP/1617277095297637022#140719417468736,,1.0
godot,2021/04/01 12:23:25,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",10.854507720947266,-5.834302825927734,51.87324759069172,1617279805738704673#140606006478656,200.0,0.001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",0.045,0.0,0.055,0.025,0.005,0.01,0.0,0.0,0.0,0.035,0.0,0.0,0.0,0.0,0.0,0.155,0.0,0.08,0.015,0.015,0.0,0.005,0.0,0.0,0.805,0.0,0.005,0.0,0.0,0.0,0.21,0.0,0.1,0.02,0.02,0.0,0.01,0.0,0.005,0.805,0.0,0.005,0.0,0.0,0.0,0.115,0.005,0.105,0.105,0.05,0.055,0.05,0.03,0.01,0.035,0.035,0.025,0.025,0.01,0.02,greedy,Train for 200 on 1000,1000.0,1000.0,16632.331074143,EVALUATIONS/WEIGHTS_BACKUP/1617279805738704673#140606006478656,,1.0
godot,2021/04/01 13:13:59,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",28.827515869140623,-0.5978994140625005,59.77917852733617,1617282839569138545#139932717430592,200.0,1e-05,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.105,0.0,0.17,0.17,0.16,0.17,0.155,0.135,0.08,0.015,0.065,0.05,0.035,0.005,0.0,0.655,0.0,0.54,0.41,0.255,0.175,0.125,0.14,0.085,0.88,0.075,0.02,0.015,0.02,0.0,0.655,0.01,0.55,0.435,0.295,0.22,0.175,0.185,0.12,0.88,0.115,0.09,0.05,0.07,0.03,0.11,0.39,0.17,0.265,0.3,0.4,0.425,0.42,0.345,0.015,0.4,0.37,0.355,0.34,0.265,greedy,Train for 200 on 1000,1000.0,1000.0,19666.142520702,EVALUATIONS/WEIGHTS_BACKUP/1617282839569138545#139932717430592,,1.0
godot,2021/04/01 13:14:30,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",30.00301025390625,0.6393955078124997,19.33672175068125,1617282870297664664#140680768530240,200.0,0.001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.455,0.0,0.265,0.1,0.09,0.03,0.02,0.0,0.01,0.865,0.005,0.0,0.0,0.005,0.0,0.465,0.01,0.285,0.145,0.135,0.06,0.05,0.035,0.045,0.865,0.04,0.03,0.01,0.02,0.005,0.235,0.205,0.19,0.31,0.255,0.245,0.235,0.26,0.24,0.0,0.335,0.29,0.23,0.2,0.21,greedy,Train for 200 on 1000,1000.0,1000.0,19696.637600988,EVALUATIONS/WEIGHTS_BACKUP/1617282870297664664#140680768530240,,1.0
godot,2021/04/01 13:27:38,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",27.920410400390622,-1.8556999511718744,114.5709517089931,1617283658679221224#140464141502272,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.24,0.0,0.29,0.305,0.195,0.18,0.115,0.095,0.02,0.045,0.045,0.01,0.005,0.0,0.0,0.585,0.0,0.395,0.225,0.09,0.02,0.025,0.015,0.005,0.85,0.01,0.005,0.0,0.0,0.0,0.59,0.005,0.42,0.255,0.105,0.05,0.045,0.025,0.01,0.85,0.025,0.015,0.005,0.015,0.0,0.325,0.42,0.38,0.56,0.56,0.63,0.505,0.67,0.535,0.045,0.59,0.51,0.465,0.465,0.385,greedy,Train for 200 on 1000,1000.0,1000.0,20485.259881582,EVALUATIONS/WEIGHTS_BACKUP/1617283658679221224#140464141502272,,1.0
godot,2021/04/01 16:35:00,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",32.27210009765625,-0.0858999023437505,108.75077802132932,1617294900603085129#139930451814208,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.085,0.0,0.07,0.07,0.035,0.07,0.06,0.05,0.01,0.015,0.03,0.01,0.015,0.005,0.005,0.7,0.0,0.575,0.41,0.265,0.255,0.14,0.11,0.08,0.88,0.065,0.015,0.02,0.005,0.0,0.725,0.015,0.6,0.465,0.315,0.32,0.22,0.17,0.13,0.88,0.13,0.06,0.035,0.03,0.03,0.19,0.285,0.195,0.31,0.295,0.34,0.355,0.425,0.355,0.015,0.41,0.39,0.35,0.345,0.245,greedy,Train for 200 on 1000,1000.0,1000.0,27969.334571691,EVALUATIONS/WEIGHTS_BACKUP/1617294900603085129#139930451814208,,1.0
godot,2021/04/01 16:47:15,Deep Embed-Process trained for longer to find convergence time,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",26.18459814453125,0.7898989257812502,171.22494666804744,1617295635198814706#140499014428480,200.0,1e-05,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.19,0.015,0.255,0.23,0.18,0.205,0.185,0.17,0.13,0.07,0.175,0.075,0.035,0.04,0.005,0.61,0.005,0.55,0.4,0.29,0.19,0.12,0.11,0.12,0.845,0.08,0.025,0.015,0.01,0.005,0.615,0.02,0.56,0.435,0.33,0.24,0.185,0.135,0.145,0.845,0.115,0.045,0.04,0.035,0.035,0.2,0.44,0.265,0.315,0.34,0.42,0.385,0.54,0.43,0.07,0.515,0.49,0.51,0.435,0.375,greedy,Train for 200 on 1000,1000.0,1000.0,28482.913314498,EVALUATIONS/WEIGHTS_BACKUP/1617295635198814706#140499014428480,,1.0
godot,2021/04/02 09:50:50,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.896041381835938,-0.0297183837890624,0.004084261559452,1617357050142091363#140608853026624,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.375,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.495,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.89,0.0,0.0,0.0,0.0,0.0,greedy,Train for 100x100 examples,100.0,10000.0,234.436215568,EVALUATIONS/WEIGHTS_BACKUP/1617357050142091363#140608853026624,10-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 09:50:43,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.1196689453125,-0.0068543395996093,0.0267679204494939,1617357043204811763#140054054295360,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.205,0.0,0.33,0.29,0.14,0.06,0.025,0.015,0.01,0.48,0.01,0.005,0.0,0.0,0.0,0.575,0.0,0.29,0.05,0.06,0.01,0.005,0.0,0.005,0.42,0.0,0.0,0.0,0.0,0.0,0.64,0.0,0.38,0.09,0.13,0.06,0.025,0.02,0.01,0.42,0.0,0.0,0.0,0.0,0.0,0.215,0.005,0.4,0.585,0.34,0.32,0.215,0.25,0.15,0.48,0.14,0.085,0.07,0.035,0.015,greedy,Train for 100x100 examples,100.0,10000.0,227.533158855,EVALUATIONS/WEIGHTS_BACKUP/1617357043204811763#140054054295360,10-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 10:32:09,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",11.681901611328124,-0.0535571289062499,0.0218344164416066,1617359529930460725#140348148295488,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.855,0.0,0.0,0.0,0.0,0.0,0.24,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.24,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.29,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.855,0.0,0.0,0.0,0.0,0.0,greedy,Train for 100x100 examples,100.0,10000.0,229.398329616,EVALUATIONS/WEIGHTS_BACKUP/1617359529930460725#140348148295488,10-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 10:32:12,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.18957455444336,-0.0228489990234374,0.0018457534881815,1617359532650963687#139747158427456,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.05,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.895,0.0,0.0,0.0,0.0,0.0,0.485,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.68,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.085,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.895,0.0,0.0,0.0,0.0,0.0,greedy,Train for 100x100 examples,100.0,10000.0,232.205017739,EVALUATIONS/WEIGHTS_BACKUP/1617359532650963687#139747158427456,10-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 10:35:54,Embed-Process model trained with reinforc for finding the optimal probability of Hamilton cycle existance in train graphs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",12.238596069335935,1.4663996582031251,22.18951756835256,1617359754926008474#139635501643584,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,0.0,0.115,0.04,0.05,0.025,0.01,0.01,0.0,0.02,0.005,0.0,0.0,0.0,0.0,0.355,0.0,0.19,0.07,0.035,0.01,0.005,0.0,0.0,0.875,0.0,0.0,0.0,0.0,0.0,0.675,0.025,0.405,0.34,0.205,0.195,0.185,0.175,0.115,0.875,0.065,0.09,0.08,0.055,0.035,0.225,0.325,0.275,0.41,0.4,0.405,0.39,0.485,0.33,0.02,0.495,0.4,0.375,0.385,0.31,greedy,Train for 100 iterations on each of 100 examples,100.0,100.0,454.657998221,EVALUATIONS/WEIGHTS_BACKUP/1617359754926008474#139635501643584,"ER(10, 0.2956369492127657)",1.0
godot,2021/04/02 10:36:11,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.936860473632812,0.026616943359375,0.0369488924053555,1617359771512288516#140373548214080,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.185,0.16,0.215,0.175,0.165,0.225,0.215,0.175,0.22,0.01,0.205,0.16,0.145,0.165,0.17,0.43,0.08,0.375,0.37,0.35,0.275,0.25,0.215,0.18,0.885,0.24,0.215,0.145,0.11,0.07,0.51,0.315,0.405,0.485,0.46,0.405,0.325,0.365,0.33,0.885,0.39,0.405,0.395,0.305,0.27,0.24,0.535,0.285,0.315,0.255,0.39,0.385,0.41,0.44,0.01,0.41,0.4,0.42,0.545,0.525,greedy,Train for 100x100 examples,100.0,10000.0,470.658850036,EVALUATIONS/WEIGHTS_BACKUP/1617359771512288516#140373548214080,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 10:37:56,Embed-Process model trained with reinforc for finding the optimal probability of Hamilton cycle existance in train graphs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",15.474193969726562,2.3275987548828128,25.62687800352961,1617359876081333027#139745525417792,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.34,0.0,0.335,0.255,0.165,0.135,0.08,0.045,0.02,0.135,0.02,0.01,0.005,0.005,0.0,0.32,0.0,0.19,0.105,0.04,0.015,0.0,0.0,0.0,0.76,0.005,0.0,0.0,0.0,0.0,0.46,0.015,0.305,0.245,0.145,0.105,0.065,0.1,0.075,0.76,0.095,0.06,0.04,0.05,0.01,0.415,0.475,0.525,0.52,0.54,0.61,0.52,0.57,0.55,0.135,0.535,0.485,0.525,0.5,0.48,greedy,Train for 100 iterations on each of 100 examples,100.0,100.0,575.7578249899999,EVALUATIONS/WEIGHTS_BACKUP/1617359876081333027#139745525417792,"ER(10, 0.389236717647074)",1.0
godot,2021/04/02 10:39:50,Embed-Process model trained with reinforc for finding the optimal probability of Hamilton cycle existance in train graphs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",34.329609375000004,-0.0787988281250008,1.4590079271981722,1617359990726777944#140595498923840,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.36,0.0,0.265,0.165,0.09,0.04,0.015,0.015,0.005,0.61,0.01,0.005,0.0,0.0,0.0,0.39,0.0,0.305,0.21,0.115,0.105,0.045,0.05,0.02,0.61,0.03,0.03,0.0,0.0,0.0,0.22,0.06,0.165,0.155,0.225,0.175,0.14,0.17,0.165,0.0,0.11,0.1,0.05,0.07,0.045,greedy,Train for 100 iterations on each of 100 examples,100.0,100.0,690.2588831310001,EVALUATIONS/WEIGHTS_BACKUP/1617359990726777944#140595498923840,"ER(10, 0.5985538739504941)",1.0
godot,2021/04/02 10:38:52,Embed-Process model trained with reinforc for finding the optimal probability of Hamilton cycle existance in train graphs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",26.034198242187507,4.041803222656251,30.88973550068431,1617359932936381992#139776124294976,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.085,0.0,0.09,0.08,0.065,0.055,0.05,0.065,0.04,0.005,0.015,0.02,0.01,0.0,0.01,0.55,0.0,0.375,0.275,0.115,0.105,0.06,0.015,0.03,0.88,0.035,0.0,0.0,0.0,0.0,0.595,0.04,0.43,0.365,0.29,0.24,0.18,0.135,0.16,0.88,0.15,0.06,0.055,0.09,0.05,0.11,0.455,0.105,0.23,0.195,0.27,0.285,0.395,0.3,0.005,0.375,0.415,0.425,0.365,0.41,greedy,Train for 100 iterations on each of 100 examples,100.0,100.0,632.4922766569999,EVALUATIONS/WEIGHTS_BACKUP/1617359932936381992#139776124294976,"ER(10, 0.5151730583335019)",1.0
godot,2021/04/02 10:38:35,Embed-Process model trained with reinforc for finding the optimal probability of Hamilton cycle existance in train graphs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",14.303792358398436,-2.2310029296875,23.343133114520583,1617359915567284861#140397519025984,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.19,0.0,0.16,0.14,0.115,0.11,0.04,0.04,0.04,0.015,0.035,0.0,0.0,0.0,0.0,0.535,0.0,0.42,0.195,0.115,0.095,0.075,0.025,0.015,0.88,0.005,0.0,0.005,0.0,0.0,0.605,0.025,0.495,0.39,0.275,0.26,0.205,0.17,0.115,0.88,0.075,0.08,0.045,0.055,0.03,0.295,0.535,0.325,0.42,0.42,0.45,0.41,0.575,0.53,0.015,0.55,0.505,0.52,0.45,0.555,greedy,Train for 100 iterations on each of 100 examples,100.0,100.0,615.156285632,EVALUATIONS/WEIGHTS_BACKUP/1617359915567284861#140397519025984,"ER(10, 0.4630608857111916)",1.0
godot,2021/04/02 10:48:12,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.1547131347656245,0.0093951721191405,0.0131294045420471,1617360492042372212#140543355795264,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.055,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,0.54,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.54,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.225,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.92,0.0,0.0,0.0,0.0,0.0,greedy,Train for 500x100 examples,500.0,50000.0,1190.935790408,EVALUATIONS/WEIGHTS_BACKUP/1617360492042372212#140543355795264,10-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 10:49:40,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",38.1664365234375,-0.1315039062499991,0.0333509018821587,1617360580321336752#139635501643584,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.255,0.0,0.065,0.0,0.0,0.0,0.0,0.0,0.0,0.845,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.085,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.615,0.0,0.23,0.115,0.0,0.0,0.0,0.0,0.0,0.845,0.0,0.0,0.0,0.0,0.0,greedy,Train for 100x100 examples,100.0,10000.0,466.2081074099999,EVALUATIONS/WEIGHTS_BACKUP/1617360580321336752#139635501643584,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 10:48:04,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",1.5307655029296874,0.0112169799804687,0.0023209855900465,1617360484525773601#139812070238016,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.095,0.03,0.02,0.075,0.08,0.105,0.11,0.105,0.13,0.01,0.06,0.05,0.025,0.025,0.015,0.68,0.02,0.605,0.385,0.305,0.265,0.26,0.21,0.2,0.855,0.16,0.055,0.04,0.035,0.03,0.74,0.145,0.685,0.54,0.43,0.45,0.415,0.375,0.39,0.855,0.32,0.22,0.17,0.17,0.13,0.105,0.32,0.055,0.14,0.13,0.19,0.275,0.265,0.275,0.01,0.245,0.27,0.265,0.29,0.23,greedy,Train for 500x100 examples,500.0,50000.0,1183.520151157,EVALUATIONS/WEIGHTS_BACKUP/1617360484525773601#139812070238016,10-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 10:59:33,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",6.270192749023437,-0.0574200439453125,0.0141733381532773,1617361173304941596#140595498923840,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.275,0.11,0.285,0.195,0.205,0.175,0.15,0.225,0.125,0.015,0.11,0.115,0.075,0.12,0.08,0.42,0.04,0.27,0.215,0.18,0.14,0.13,0.08,0.08,0.885,0.08,0.025,0.07,0.055,0.035,0.51,0.18,0.335,0.34,0.275,0.255,0.235,0.185,0.195,0.885,0.24,0.165,0.205,0.265,0.17,0.3,0.595,0.365,0.355,0.32,0.38,0.375,0.46,0.38,0.015,0.415,0.475,0.455,0.44,0.495,greedy,Train for 100x100 examples,100.0,10000.0,913.099031777,EVALUATIONS/WEIGHTS_BACKUP/1617361173304941596#140595498923840,35-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 11:09:02,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",1.8342221069335936,-0.0013887939453125,0.0034136808313895,1617361742528402960#140267211765568,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.015,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.895,0.0,0.0,0.0,0.0,0.0,0.55,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.685,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.895,0.0,0.0,0.0,0.0,0.0,greedy,Train for 1000x100 examples,1000.0,100000.0,2440.6491823,EVALUATIONS/WEIGHTS_BACKUP/1617361742528402960#140267211765568,10-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 11:04:54,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",9.321459106445312,-0.0807061767578122,0.009989413780957,1617361494477368868#140543355795264,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.205,0.0,0.2,0.195,0.135,0.095,0.085,0.065,0.06,0.73,0.03,0.02,0.025,0.01,0.01,0.38,0.0,0.215,0.135,0.07,0.07,0.04,0.06,0.03,0.16,0.045,0.02,0.005,0.0,0.0,0.455,0.015,0.325,0.23,0.15,0.205,0.11,0.2,0.115,0.16,0.13,0.11,0.04,0.025,0.02,0.295,0.18,0.27,0.295,0.27,0.285,0.29,0.275,0.27,0.73,0.255,0.345,0.335,0.31,0.28,greedy,Train for 100x100 examples,100.0,10000.0,914.143829667,EVALUATIONS/WEIGHTS_BACKUP/1617361494477368868#140543355795264,35-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 11:08:58,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",1.4958290405273438,-0.0494083862304687,0.0073491284879985,1617361738178171170#140224512816960,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.0,0.06,0.045,0.02,0.035,0.025,0.015,0.005,0.0,0.015,0.0,0.0,0.0,0.0,0.685,0.0,0.485,0.295,0.175,0.085,0.08,0.04,0.03,0.88,0.01,0.0,0.0,0.0,0.0,0.73,0.0,0.595,0.505,0.305,0.3,0.25,0.22,0.16,0.88,0.09,0.025,0.01,0.005,0.0,0.13,0.0,0.075,0.075,0.1,0.13,0.17,0.125,0.125,0.0,0.13,0.07,0.015,0.0,0.0,greedy,Train for 1000x100 examples,1000.0,100000.0,2436.0323219210004,EVALUATIONS/WEIGHTS_BACKUP/1617361738178171170#140224512816960,10-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 11:14:11,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.041388305664063,-0.0214976196289062,0.0041327828480071,1617362051510138430#140348148295488,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.065,0.06,0.07,0.11,0.095,0.115,0.105,0.065,0.1,0.01,0.07,0.055,0.07,0.105,0.07,0.695,0.275,0.645,0.495,0.47,0.46,0.435,0.465,0.41,0.875,0.48,0.39,0.335,0.325,0.295,0.715,0.635,0.69,0.61,0.605,0.66,0.685,0.69,0.655,0.875,0.695,0.68,0.67,0.655,0.685,0.075,0.26,0.09,0.15,0.115,0.2,0.155,0.165,0.215,0.01,0.17,0.2,0.2,0.245,0.23,greedy,Train for 500x100 examples,500.0,50000.0,2443.763133736,EVALUATIONS/WEIGHTS_BACKUP/1617362051510138430#140348148295488,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 11:23:49,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.468241760253906,0.0372954711914061,0.0256255458063137,1617362629096693852#140373548214080,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.56,0.0,0.41,0.1,0.015,0.0,0.0,0.0,0.0,0.885,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.105,0.305,0.075,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.105,0.465,0.44,0.02,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.725,0.0,0.525,0.205,0.085,0.0,0.0,0.0,0.0,0.885,0.0,0.0,0.0,0.0,0.0,greedy,Train for 500x100 examples,500.0,50000.0,2453.775996587,EVALUATIONS/WEIGHTS_BACKUP/1617362629096693852#140373548214080,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 11:31:14,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",8.293749755859375,-0.1082274169921877,0.0447791226226996,1617363074834123937#140595498923840,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.185,0.11,0.185,0.165,0.165,0.185,0.17,0.15,0.095,0.015,0.115,0.11,0.1,0.105,0.1,0.28,0.04,0.21,0.16,0.17,0.16,0.13,0.14,0.145,0.905,0.13,0.09,0.085,0.045,0.065,0.435,0.235,0.31,0.3,0.26,0.33,0.25,0.31,0.27,0.905,0.27,0.33,0.235,0.27,0.245,0.25,0.55,0.24,0.29,0.3,0.365,0.325,0.37,0.35,0.015,0.44,0.405,0.44,0.485,0.545,greedy,Train for 100x100 examples,100.0,10000.0,1491.182176848,EVALUATIONS/WEIGHTS_BACKUP/1617363074834123937#140595498923840,50-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 11:38:48,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",150.4929140625,-0.09150390625,0.0284616658645973,1617363528565888761#140224512816960,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.25,0.0,0.035,0.035,0.01,0.005,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.56,0.0,0.225,0.22,0.11,0.08,0.025,0.01,0.0,0.84,0.0,0.0,0.0,0.0,0.0,greedy,Train for 100x100 examples,100.0,10000.0,1475.1559581069996,EVALUATIONS/WEIGHTS_BACKUP/1617363528565888761#140224512816960,50-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 11:59:52,Moving all the complexity to embedding,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",16.500000732421874,-0.3000010986328124,46.79383065717201,1617364792667552292#140663798228800,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
=================================================================
Total params: 4,192
Trainable params: 4,192
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.055,0.0,0.015,0.0,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.325,0.0,0.085,0.04,0.015,0.015,0.0,0.0,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.625,0.0,0.33,0.31,0.16,0.165,0.095,0.1,0.075,0.86,0.065,0.05,0.015,0.005,0.005,0.12,0.025,0.06,0.105,0.07,0.08,0.09,0.07,0.045,0.0,0.065,0.05,0.055,0.02,0.01,greedy,Train for 200 iterations on each of 600 examples,600.0,600.0,5489.738457908,EVALUATIONS/WEIGHTS_BACKUP/1617364792667552292#140663798228800,"ER(10, 0.5151730583335019)",1.0
godot,2021/04/02 11:57:29,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.530616394042968,0.0051427307128906,0.0143061556039834,1617364649268863704#139747158427456,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.09,0.09,0.075,0.125,0.13,0.125,0.115,0.115,0.115,0.01,0.065,0.075,0.095,0.085,0.13,0.75,0.435,0.7,0.555,0.565,0.535,0.535,0.575,0.525,0.88,0.61,0.46,0.42,0.485,0.375,0.775,0.605,0.71,0.59,0.625,0.575,0.59,0.665,0.625,0.88,0.685,0.6,0.56,0.65,0.505,0.09,0.225,0.085,0.17,0.16,0.195,0.21,0.17,0.16,0.01,0.13,0.205,0.215,0.185,0.3,greedy,Train for 1000x100 examples,1000.0,100000.0,5021.296147992,EVALUATIONS/WEIGHTS_BACKUP/1617364649268863704#139747158427456,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 12:03:55,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",5.201901000976562,0.0961972656249999,0.1023438943121703,1617365035122965247#139776124294976,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.145,0.225,0.19,0.16,0.225,0.19,0.125,0.19,0.175,0.01,0.17,0.175,0.2,0.125,0.155,0.565,0.295,0.525,0.45,0.445,0.475,0.44,0.435,0.485,0.855,0.45,0.395,0.375,0.43,0.35,0.635,0.5,0.54,0.56,0.495,0.53,0.585,0.54,0.615,0.855,0.545,0.575,0.52,0.605,0.55,0.16,0.39,0.22,0.23,0.305,0.265,0.215,0.275,0.255,0.01,0.27,0.29,0.365,0.27,0.33,greedy,Train for 500x100 examples,500.0,50000.0,4765.773914927,EVALUATIONS/WEIGHTS_BACKUP/1617365035122965247#139776124294976,35-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 12:07:04,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.861196228027344,-0.0286765747070312,0.0146938050714275,1617365224196290375#139745525417792,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.64,0.01,0.345,0.155,0.08,0.05,0.145,0.17,0.14,0.585,0.095,0.04,0.035,0.02,0.04,0.02,0.0,0.32,0.38,0.145,0.065,0.035,0.025,0.015,0.305,0.02,0.005,0.0,0.0,0.0,0.02,0.015,0.345,0.485,0.36,0.255,0.07,0.1,0.03,0.305,0.03,0.015,0.015,0.04,0.015,0.76,0.255,0.375,0.26,0.16,0.17,0.295,0.42,0.385,0.585,0.345,0.32,0.24,0.24,0.29,greedy,Train for 1000x100 examples,1000.0,100000.0,5007.485994197,EVALUATIONS/WEIGHTS_BACKUP/1617365224196290375#139745525417792,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 12:12:15,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",6.742496582031251,0.0120328369140624,0.0241376770765455,1617365535545714440#139635501643584,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.115,0.125,0.15,0.195,0.22,0.205,0.21,0.25,0.17,0.01,0.22,0.175,0.14,0.13,0.1,0.535,0.075,0.395,0.3,0.285,0.3,0.24,0.2,0.255,0.855,0.235,0.13,0.135,0.1,0.07,0.645,0.225,0.475,0.42,0.34,0.45,0.375,0.4,0.43,0.855,0.345,0.29,0.295,0.24,0.245,0.16,0.5,0.175,0.295,0.315,0.31,0.34,0.44,0.355,0.01,0.425,0.55,0.495,0.515,0.46,greedy,Train for 500x100 examples,500.0,50000.0,4804.189464937001,EVALUATIONS/WEIGHTS_BACKUP/1617365535545714440#139635501643584,35-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 12:17:34,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",11.36335986328125,0.0218112792968749,0.0989767033962039,1617365854638800251#140595498923840,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.17,0.205,0.28,0.29,0.29,0.29,0.235,0.255,0.27,0.05,0.235,0.225,0.24,0.255,0.235,0.52,0.145,0.46,0.31,0.245,0.28,0.285,0.24,0.24,0.87,0.205,0.245,0.19,0.125,0.18,0.595,0.385,0.47,0.365,0.295,0.33,0.425,0.39,0.35,0.87,0.375,0.39,0.35,0.28,0.325,0.205,0.475,0.315,0.37,0.405,0.37,0.345,0.445,0.445,0.05,0.45,0.43,0.405,0.56,0.485,greedy,Train for 100x100 examples,100.0,10000.0,2335.0332646940005,EVALUATIONS/WEIGHTS_BACKUP/1617365854638800251#140595498923840,70-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 12:45:06,Moving all the complexity to embedding,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",17.02600439453125,0.78580126953125,6.53487546054987,1617367506030525377#140350364276544,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
=================================================================
Total params: 4,192
Trainable params: 4,192
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,0.0,0.065,0.015,0.005,0.005,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.045,0.01,0.01,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.45,0.01,0.205,0.145,0.06,0.065,0.02,0.02,0.035,0.84,0.015,0.0,0.01,0.0,0.005,0.29,0.015,0.175,0.24,0.13,0.225,0.13,0.12,0.075,0.02,0.095,0.05,0.05,0.02,0.02,greedy,Train for 200 iterations on each of 600 examples,600.0,600.0,8201.380369395001,EVALUATIONS/WEIGHTS_BACKUP/1617367506030525377#140350364276544,"ER(10, 0.5151730583335019)",1.0
godot,2021/04/02 12:43:58,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",18.0399033203125,-0.1988671875000001,0.0715094440867574,1617367438223731014#139747158427456,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.175,0.01,0.105,0.095,0.1,0.06,0.06,0.05,0.045,0.01,0.03,0.025,0.035,0.015,0.005,0.32,0.01,0.12,0.055,0.045,0.015,0.015,0.01,0.005,0.855,0.005,0.0,0.01,0.0,0.005,0.51,0.065,0.245,0.16,0.125,0.105,0.07,0.09,0.095,0.855,0.11,0.085,0.06,0.055,0.07,0.26,0.37,0.235,0.27,0.225,0.32,0.275,0.355,0.3,0.01,0.315,0.25,0.32,0.32,0.34,greedy,Train for 100x100 examples,100.0,10000.0,2354.557400394,EVALUATIONS/WEIGHTS_BACKUP/1617367438223731014#139747158427456,70-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 13:21:25,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",6.86501318359375,-0.0523531494140625,0.1239059009269709,1617369685259128405#140267211765568,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.2,0.17,0.2,0.225,0.25,0.165,0.225,0.2,0.22,0.015,0.17,0.19,0.16,0.19,0.135,0.46,0.17,0.45,0.4,0.28,0.335,0.245,0.3,0.295,0.88,0.3,0.225,0.195,0.195,0.195,0.54,0.425,0.49,0.455,0.345,0.455,0.315,0.435,0.405,0.88,0.42,0.375,0.39,0.4,0.385,0.235,0.46,0.26,0.3,0.375,0.325,0.38,0.345,0.345,0.015,0.385,0.41,0.415,0.4,0.435,greedy,Train for 500x100 examples,500.0,50000.0,7843.891322678,EVALUATIONS/WEIGHTS_BACKUP/1617369685259128405#140267211765568,50-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 13:28:10,Moving all the complexity to embedding,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",11.32419580078125,0.0387001953125,17.46475572021984,1617370090309075045#140676142569280,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
|    └─MaxMessageParsing: 2-11           --
|    |    └─Sequential: 3-21             2,112
|    |    └─Sequential: 3-22             2,080
|    └─MaxMessageParsing: 2-12           --
|    |    └─Sequential: 3-23             2,112
|    |    └─Sequential: 3-24             2,080
|    └─MaxMessageParsing: 2-13           --
|    |    └─Sequential: 3-25             2,112
|    |    └─Sequential: 3-26             2,080
|    └─MaxMessageParsing: 2-14           --
|    |    └─Sequential: 3-27             2,112
|    |    └─Sequential: 3-28             2,080
|    └─MaxMessageParsing: 2-15           --
|    |    └─Sequential: 3-29             2,112
|    |    └─Sequential: 3-30             2,080
=================================================================
Total params: 62,880
Trainable params: 62,880
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
=================================================================
Total params: 4,192
Trainable params: 4,192
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.055,0.0,0.015,0.0,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.325,0.0,0.085,0.04,0.015,0.015,0.0,0.0,0.0,0.86,0.0,0.0,0.0,0.0,0.0,0.635,0.0,0.33,0.31,0.16,0.165,0.095,0.1,0.075,0.86,0.065,0.05,0.015,0.005,0.005,0.11,0.025,0.06,0.105,0.07,0.08,0.09,0.07,0.045,0.0,0.065,0.05,0.055,0.02,0.01,greedy,Train for 200 iterations on each of 600 examples,600.0,600.0,10784.977256855,EVALUATIONS/WEIGHTS_BACKUP/1617370090309075045#140676142569280,"ER(10, 0.5151730583335019)",1.0
godot,2021/04/02 13:28:42,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.334771606445313,0.0561617431640625,0.0434690702213558,1617370122356472294#140397519025984,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.09,0.095,0.06,0.065,0.055,0.115,0.08,0.07,0.055,0.015,0.11,0.085,0.05,0.07,0.1,0.665,0.395,0.675,0.555,0.54,0.52,0.45,0.575,0.495,0.88,0.465,0.41,0.39,0.365,0.36,0.695,0.585,0.695,0.655,0.62,0.59,0.55,0.68,0.59,0.88,0.61,0.56,0.55,0.56,0.555,0.1,0.22,0.085,0.135,0.135,0.19,0.205,0.135,0.165,0.015,0.2,0.225,0.175,0.22,0.22,greedy,Train for 1000x100 examples,1000.0,100000.0,9846.069960897,EVALUATIONS/WEIGHTS_BACKUP/1617370122356472294#140397519025984,35-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 13:31:39,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",8.874966918945313,0.0771414794921874,0.0701107854561939,1617370299350293346#140348148295488,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.16,0.08,0.115,0.155,0.18,0.165,0.175,0.165,0.195,0.7,0.19,0.125,0.12,0.145,0.135,0.445,0.1,0.375,0.21,0.205,0.155,0.16,0.135,0.165,0.22,0.15,0.095,0.105,0.115,0.085,0.59,0.32,0.51,0.355,0.335,0.275,0.275,0.295,0.315,0.22,0.35,0.315,0.355,0.34,0.315,0.215,0.545,0.18,0.28,0.345,0.405,0.4,0.4,0.46,0.7,0.47,0.405,0.425,0.475,0.54,greedy,Train for 500x100 examples,500.0,50000.0,7794.406388547,EVALUATIONS/WEIGHTS_BACKUP/1617370299350293346#140348148295488,50-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 13:35:58,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",6.364834594726562,-0.0769949951171874,0.0912715678121216,1617370557908932964#139812070238016,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.145,0.045,0.16,0.155,0.145,0.15,0.165,0.17,0.09,0.01,0.155,0.1,0.065,0.07,0.035,0.47,0.03,0.385,0.26,0.215,0.235,0.17,0.145,0.13,0.88,0.125,0.085,0.08,0.085,0.08,0.595,0.275,0.46,0.39,0.355,0.375,0.315,0.295,0.33,0.88,0.285,0.305,0.345,0.25,0.26,0.205,0.455,0.21,0.27,0.28,0.305,0.425,0.455,0.34,0.01,0.465,0.425,0.41,0.515,0.445,greedy,Train for 1000x100 examples,1000.0,100000.0,9695.091954941,EVALUATIONS/WEIGHTS_BACKUP/1617370557908932964#139812070238016,35-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 14:15:55,Moving all the complexity to embedding,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",13.263099121093749,0.2627001953124995,20.7181727145956,1617372955238967694#139729741158208,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessageParsing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessageParsing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessageParsing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessageParsing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessageParsing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
|    └─MaxMessageParsing: 2-11           --
|    |    └─Sequential: 3-21             2,112
|    |    └─Sequential: 3-22             2,080
|    └─MaxMessageParsing: 2-12           --
|    |    └─Sequential: 3-23             2,112
|    |    └─Sequential: 3-24             2,080
|    └─MaxMessageParsing: 2-13           --
|    |    └─Sequential: 3-25             2,112
|    |    └─Sequential: 3-26             2,080
|    └─MaxMessageParsing: 2-14           --
|    |    └─Sequential: 3-27             2,112
|    |    └─Sequential: 3-28             2,080
|    └─MaxMessageParsing: 2-15           --
|    |    └─Sequential: 3-29             2,112
|    |    └─Sequential: 3-30             2,080
|    └─MaxMessageParsing: 2-16           --
|    |    └─Sequential: 3-31             2,112
|    |    └─Sequential: 3-32             2,080
|    └─MaxMessageParsing: 2-17           --
|    |    └─Sequential: 3-33             2,112
|    |    └─Sequential: 3-34             2,080
|    └─MaxMessageParsing: 2-18           --
|    |    └─Sequential: 3-35             2,112
|    |    └─Sequential: 3-36             2,080
|    └─MaxMessageParsing: 2-19           --
|    |    └─Sequential: 3-37             2,112
|    |    └─Sequential: 3-38             2,080
|    └─MaxMessageParsing: 2-20           --
|    |    └─Sequential: 3-39             2,112
|    |    └─Sequential: 3-40             2,080
=================================================================
Total params: 83,840
Trainable params: 83,840
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
=================================================================
Total params: 4,192
Trainable params: 4,192
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,0.0,0.065,0.015,0.005,0.005,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.18,0.0,0.045,0.01,0.01,0.0,0.0,0.0,0.0,0.84,0.0,0.0,0.0,0.0,0.0,0.45,0.01,0.205,0.145,0.06,0.065,0.02,0.02,0.035,0.84,0.015,0.0,0.01,0.0,0.005,0.29,0.015,0.175,0.24,0.13,0.225,0.13,0.12,0.075,0.02,0.095,0.05,0.05,0.02,0.02,greedy,Train for 200 iterations on each of 600 examples,600.0,600.0,13648.824241512,EVALUATIONS/WEIGHTS_BACKUP/1617372955238967694#139729741158208,"ER(10, 0.5151730583335019)",1.0
godot,2021/04/02 15:15:26,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",14.434948974609377,-0.4054228515624999,0.921398843418018,1617376526068305272#140224512816960,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.07,0.045,0.08,0.085,0.055,0.11,0.085,0.1,0.055,0.01,0.11,0.055,0.025,0.04,0.04,0.365,0.03,0.285,0.285,0.19,0.17,0.17,0.13,0.115,0.88,0.09,0.08,0.07,0.04,0.03,0.57,0.18,0.42,0.44,0.35,0.355,0.335,0.315,0.295,0.88,0.26,0.25,0.235,0.225,0.18,0.08,0.465,0.11,0.18,0.165,0.23,0.265,0.325,0.26,0.01,0.37,0.38,0.325,0.385,0.365,greedy,Train for 500x100 examples,500.0,50000.0,12741.038639504,EVALUATIONS/WEIGHTS_BACKUP/1617376526068305272#140224512816960,70-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 15:36:30,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",191.90822656250003,2.2432070312500003,20.750486814162286,1617377790881152142#140543355795264,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.38,0.0,0.305,0.175,0.07,0.04,0.03,0.005,0.005,0.025,0.0,0.0,0.0,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.68,0.0,0.0,0.0,0.0,0.0,0.015,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.68,0.0,0.0,0.0,0.0,0.0,0.6,0.05,0.515,0.47,0.325,0.28,0.26,0.24,0.145,0.025,0.205,0.14,0.095,0.095,0.07,greedy,Train for 1000x100 examples,1000.0,100000.0,15937.374266775,EVALUATIONS/WEIGHTS_BACKUP/1617377790881152142#140543355795264,50-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 15:42:19,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",11.72671484375,-0.048124755859375,0.1056458224138054,1617378139069564214#139776124294976,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.185,0.06,0.225,0.19,0.215,0.135,0.235,0.185,0.125,0.105,0.115,0.115,0.11,0.085,0.125,0.36,0.035,0.215,0.18,0.095,0.15,0.11,0.085,0.05,0.815,0.115,0.08,0.05,0.025,0.02,0.48,0.24,0.34,0.285,0.2,0.26,0.21,0.245,0.165,0.815,0.245,0.27,0.21,0.18,0.18,0.255,0.49,0.355,0.37,0.39,0.38,0.47,0.445,0.465,0.105,0.425,0.44,0.485,0.565,0.535,greedy,Train for 500x100 examples,500.0,50000.0,12647.796853192,EVALUATIONS/WEIGHTS_BACKUP/1617378139069564214#139776124294976,70-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 15:53:16,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",8.375028076171876,-0.0045942382812498,0.0629661325467907,1617378796040664237#140373548214080,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.545,0.195,0.255,0.285,0.38,0.21,0.27,0.31,0.23,0.8,0.255,0.23,0.22,0.18,0.165,0.105,0.06,0.32,0.175,0.04,0.115,0.105,0.08,0.12,0.095,0.11,0.095,0.085,0.085,0.04,0.105,0.2,0.405,0.24,0.065,0.23,0.24,0.135,0.175,0.095,0.235,0.19,0.22,0.245,0.195,0.7,0.575,0.305,0.445,0.62,0.485,0.47,0.635,0.49,0.8,0.555,0.585,0.56,0.535,0.54,greedy,Train for 1000x100 examples,1000.0,100000.0,15969.017270431,EVALUATIONS/WEIGHTS_BACKUP/1617378796040664237#140373548214080,50-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 19:13:06,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",9.74444104003906,-0.2287969970703123,0.2270479350634957,1617390786593321664#140663798228800,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.15,0.205,0.27,0.235,0.265,0.265,0.26,0.25,0.22,0.015,0.25,0.15,0.125,0.21,0.13,0.545,0.16,0.4,0.285,0.225,0.19,0.195,0.17,0.195,0.885,0.18,0.175,0.12,0.16,0.125,0.63,0.345,0.435,0.37,0.305,0.28,0.275,0.29,0.29,0.885,0.295,0.31,0.26,0.32,0.31,0.18,0.525,0.3,0.365,0.35,0.445,0.42,0.515,0.445,0.015,0.535,0.485,0.52,0.54,0.515,greedy,Train for 1000x100 examples,1000.0,100000.0,25877.397812834,EVALUATIONS/WEIGHTS_BACKUP/1617390786593321664#140663798228800,70-cycle with expected 1 extra edge per node,1.0
godot,2021/04/02 19:25:15,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",11.115415283203124,0.0553881835937499,0.0440807468535808,1617391515572455222#139745525417792,100.0,1e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 1e-05
    weight_decay: 0
)",0.145,0.115,0.175,0.12,0.125,0.095,0.17,0.175,0.145,0.015,0.13,0.1,0.12,0.19,0.115,0.425,0.125,0.29,0.225,0.215,0.22,0.195,0.17,0.22,0.905,0.155,0.19,0.15,0.145,0.135,0.54,0.41,0.365,0.365,0.325,0.395,0.335,0.345,0.36,0.905,0.355,0.445,0.365,0.37,0.425,0.2,0.385,0.27,0.25,0.3,0.25,0.33,0.37,0.345,0.015,0.38,0.375,0.395,0.44,0.4,greedy,Train for 1000x100 examples,1000.0,100000.0,25904.850590079,EVALUATIONS/WEIGHTS_BACKUP/1617391515572455222#139745525417792,70-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 11:17:04,Shallow Embed-Process net for determining best train size,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",20.60151538085937,0.2676008300781252,44.574056352754496,1617707824620791636#140102212593472,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.11,0.0,0.095,0.06,0.03,0.05,0.005,0.015,0.01,0.015,0.005,0.005,0.0,0.0,0.0,0.47,0.0,0.28,0.16,0.08,0.03,0.025,0.005,0.0,0.865,0.0,0.005,0.0,0.0,0.0,0.5,0.015,0.335,0.26,0.185,0.105,0.105,0.09,0.045,0.865,0.085,0.035,0.02,0.01,0.02,0.11,0.055,0.095,0.2,0.15,0.155,0.16,0.17,0.105,0.015,0.105,0.07,0.085,0.065,0.045,greedy,Train for 200 iterations on each of 100 examples,100.0,100.0,706.8139801210001,EVALUATIONS/WEIGHTS_BACKUP/1617707824620791636#140102212593472,"ER(10, 0.5151730583335019)",1.0
godot,2021/04/06 11:32:32,Shallow Embed-Process net for determining best train size,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",28.28400756835937,1.7206049804687495,37.594108345662335,1617708752696679250#139866659292992,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.155,0.0,0.13,0.125,0.08,0.06,0.045,0.025,0.005,0.015,0.02,0.0,0.0,0.0,0.0,0.45,0.0,0.3,0.175,0.085,0.055,0.01,0.015,0.01,0.875,0.0,0.0,0.0,0.0,0.0,0.69,0.02,0.54,0.37,0.285,0.3,0.19,0.17,0.195,0.875,0.125,0.12,0.12,0.07,0.085,0.205,0.2,0.21,0.37,0.335,0.36,0.385,0.44,0.255,0.015,0.42,0.225,0.29,0.32,0.215,greedy,Train for 200 iterations on each of 100 examples,100.0,100.0,1634.515748133,EVALUATIONS/WEIGHTS_BACKUP/1617708752696679250#139866659292992,"ER(20, 0.2943611031936029)",1.0
godot,2021/04/06 11:52:03,Shallow Embed-Process net for determining best train size,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",28.44179736328125,-0.2887001953125001,1.9928277396115843,1617709923036040856#139693860235072,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.545,0.0,0.46,0.325,0.195,0.135,0.105,0.065,0.025,0.905,0.035,0.01,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.015,0.02,0.005,0.0,0.0,0.005,0.025,0.0,0.005,0.005,0.0,0.005,0.0,0.865,0.48,0.78,0.82,0.725,0.765,0.72,0.705,0.615,0.905,0.7,0.61,0.54,0.51,0.55,greedy,Train for 200 iterations on each of 100 examples,100.0,100.0,2804.367193766,EVALUATIONS/WEIGHTS_BACKUP/1617709923036040856#139693860235072,"ER(30, 0.2112160313490763)",1.0
godot,2021/04/06 12:13:00,Shallow Embed-Process net for determining best train size,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",33.83154931640625,-1.0767993164062497,10.921295505122089,1617711180761702077#139712138442560,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.315,0.0,0.215,0.145,0.05,0.07,0.035,0.01,0.005,0.3,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.545,0.0,0.0,0.0,0.0,0.0,0.015,0.0,0.015,0.01,0.0,0.005,0.0,0.0,0.0,0.545,0.01,0.0,0.0,0.0,0.0,0.725,0.205,0.515,0.57,0.435,0.53,0.43,0.435,0.36,0.3,0.435,0.34,0.33,0.215,0.245,greedy,Train for 200 iterations on each of 100 examples,100.0,100.0,4061.053881961,EVALUATIONS/WEIGHTS_BACKUP/1617711180761702077#139712138442560,"ER(40, 0.16651646620094074)",1.0
godot,2021/04/06 12:13:33,Shallow Embed-Process net for determining best train size,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",20.159022949218752,-1.0709990234375,18.704851768632807,1617711213193855347#140576646727488,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.115,0.0,0.08,0.055,0.045,0.02,0.055,0.015,0.0,0.015,0.0,0.0,0.0,0.0,0.0,0.47,0.0,0.305,0.17,0.085,0.06,0.02,0.005,0.01,0.88,0.0,0.0,0.005,0.0,0.0,0.515,0.04,0.395,0.31,0.175,0.215,0.13,0.12,0.11,0.88,0.08,0.07,0.06,0.06,0.05,0.25,0.23,0.225,0.29,0.265,0.32,0.265,0.35,0.335,0.015,0.385,0.305,0.31,0.295,0.24,greedy,Train for 200 iterations on each of 500 examples,500.0,500.0,4093.274473768001,EVALUATIONS/WEIGHTS_BACKUP/1617711213193855347#140576646727488,"ER(10, 0.5151730583335019)",1.0
godot,2021/04/06 12:29:37,Shallow Embed-Process net for determining best train size,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",38.116,0.8760000000000006,4.5285265000002255,1617712177868643938#140664716494656,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.205,0.0,0.005,0.01,0.0,0.0,0.005,0.0,0.0,0.885,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.655,0.0,0.16,0.2,0.04,0.065,0.025,0.03,0.015,0.885,0.02,0.005,0.02,0.005,0.0,greedy,Train for 200 iterations on each of 100 examples,100.0,100.0,5057.686973252999,EVALUATIONS/WEIGHTS_BACKUP/1617712177868643938#140664716494656,"ER(50, 0.13828607398114504)",1.0
godot,2021/04/06 13:34:14,Shallow Embed-Process net for determining best train size,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",33.98560424804688,-0.63440087890625,98.83416362476692,1617716054769519059#140121043457856,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.115,0.0,0.14,0.135,0.115,0.125,0.105,0.09,0.06,0.015,0.06,0.03,0.025,0.005,0.0,0.54,0.0,0.425,0.25,0.19,0.15,0.09,0.085,0.07,0.88,0.005,0.025,0.0,0.0,0.005,0.56,0.07,0.48,0.4,0.305,0.325,0.21,0.21,0.225,0.88,0.115,0.135,0.095,0.105,0.04,0.36,0.585,0.39,0.52,0.515,0.58,0.575,0.62,0.58,0.015,0.695,0.665,0.625,0.59,0.6,greedy,Train for 200 iterations on each of 500 examples,500.0,500.0,8932.384387058,EVALUATIONS/WEIGHTS_BACKUP/1617716054769519059#140121043457856,"ER(20, 0.2943611031936029)",1.0
godot,2021/04/06 14:11:05,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.367505157470704,-0.0251487731933593,0.0391661904689879,1617718265495392850#140099107817280,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.08,0.05,0.06,0.06,0.07,0.07,0.065,0.075,0.03,0.01,0.05,0.05,0.04,0.05,0.065,0.72,0.495,0.745,0.685,0.625,0.595,0.54,0.585,0.6,0.94,0.615,0.53,0.55,0.5,0.49,0.75,0.755,0.765,0.735,0.695,0.715,0.69,0.75,0.74,0.94,0.765,0.655,0.705,0.735,0.68,0.09,0.15,0.065,0.08,0.09,0.095,0.125,0.115,0.085,0.01,0.115,0.145,0.145,0.13,0.17,greedy,Train for 2000x100 examples,2000.0,200000.0,11140.957463224,EVALUATIONS/WEIGHTS_BACKUP/1617718265495392850#140099107817280,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 14:14:43,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.5950176696777345,0.0763271179199218,0.0243465637438768,1617718483748762399#139887475713856,100.0,5e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    weight_decay: 0
)",0.07,0.135,0.065,0.07,0.09,0.08,0.075,0.075,0.05,0.015,0.135,0.085,0.11,0.07,0.115,0.71,0.365,0.69,0.595,0.525,0.555,0.51,0.505,0.525,0.915,0.5,0.48,0.41,0.485,0.335,0.735,0.47,0.73,0.66,0.555,0.58,0.565,0.6,0.615,0.915,0.575,0.56,0.49,0.555,0.45,0.085,0.305,0.085,0.095,0.12,0.125,0.11,0.12,0.125,0.015,0.22,0.16,0.19,0.16,0.26,greedy,Train for 2000x100 examples,2000.0,200000.0,11358.500468224998,EVALUATIONS/WEIGHTS_BACKUP/1617718483748762399#139887475713856,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 14:49:57,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.696757263183593,-0.0340963745117187,0.0057079613806472,1617720597192111849#140161139873600,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.055,0.02,0.015,0.045,0.015,0.06,0.035,0.04,0.04,0.01,0.05,0.01,0.05,0.045,0.065,0.73,0.545,0.78,0.68,0.635,0.625,0.625,0.66,0.62,0.88,0.615,0.555,0.49,0.62,0.485,0.765,0.715,0.795,0.72,0.65,0.67,0.685,0.72,0.715,0.88,0.7,0.69,0.645,0.715,0.625,0.07,0.125,0.03,0.09,0.065,0.09,0.075,0.075,0.09,0.01,0.11,0.065,0.125,0.095,0.17,greedy,Train for 2000x100 examples,2000.0,200000.0,13470.332625845,EVALUATIONS/WEIGHTS_BACKUP/1617720597192111849#140161139873600,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 15:05:05,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.2724036865234374,0.0241859741210937,0.0314358788223785,1617721505305047713#140102212593472,100.0,5e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    weight_decay: 0
)",0.075,0.05,0.025,0.045,0.06,0.09,0.075,0.055,0.06,0.01,0.055,0.07,0.05,0.035,0.06,0.685,0.34,0.74,0.63,0.555,0.54,0.55,0.535,0.555,0.895,0.505,0.455,0.455,0.425,0.3,0.735,0.58,0.76,0.69,0.635,0.625,0.64,0.685,0.66,0.895,0.725,0.63,0.61,0.635,0.525,0.085,0.31,0.04,0.08,0.115,0.19,0.15,0.145,0.16,0.01,0.15,0.205,0.205,0.195,0.31,greedy,Train for 2000x100 examples,2000.0,200000.0,13530.489916513,EVALUATIONS/WEIGHTS_BACKUP/1617721505305047713#140102212593472,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 15:25:19,Shallow Embed-Process net for determining best train size,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",38.945098632812496,-1.706598632812501,189.90838914770552,1617722719779194764#140112026187584,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.515,0.005,0.28,0.265,0.17,0.205,0.1,0.12,0.06,0.33,0.04,0.025,0.01,0.02,0.0,0.26,0.0,0.33,0.22,0.175,0.135,0.075,0.03,0.05,0.585,0.02,0.02,0.005,0.0,0.0,0.32,0.03,0.4,0.325,0.245,0.23,0.155,0.12,0.125,0.585,0.09,0.09,0.055,0.035,0.025,0.64,0.53,0.475,0.56,0.595,0.655,0.67,0.715,0.62,0.33,0.685,0.66,0.585,0.6,0.535,greedy,Train for 200 iterations on each of 500 examples,500.0,500.0,15594.065181994,EVALUATIONS/WEIGHTS_BACKUP/1617722719779194764#140112026187584,"ER(30, 0.2112160313490763)",1.0
godot,2021/04/06 15:31:10,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.430865539550781,-0.0136946105957031,0.01970924655628,1617723070001656037#140513064859456,100.0,5e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    weight_decay: 0
)",0.045,0.04,0.02,0.06,0.015,0.04,0.015,0.03,0.03,0.01,0.03,0.015,0.075,0.025,0.04,0.665,0.495,0.71,0.59,0.575,0.57,0.555,0.605,0.52,0.92,0.57,0.545,0.455,0.48,0.45,0.715,0.695,0.75,0.65,0.67,0.665,0.645,0.74,0.69,0.92,0.755,0.695,0.59,0.685,0.655,0.075,0.155,0.025,0.11,0.07,0.095,0.075,0.075,0.105,0.01,0.07,0.085,0.145,0.12,0.13,greedy,Train for 3000x100 examples,3000.0,300000.0,15941.243155853,EVALUATIONS/WEIGHTS_BACKUP/1617723070001656037#140513064859456,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 15:53:14,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.3020425415039063,-0.0157496643066406,0.0091291169867293,1617724393916395052#140189562459968,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.075,0.02,0.02,0.045,0.05,0.06,0.06,0.05,0.065,0.01,0.035,0.04,0.055,0.015,0.03,0.705,0.395,0.76,0.68,0.57,0.545,0.575,0.605,0.545,0.865,0.515,0.495,0.435,0.425,0.36,0.75,0.705,0.775,0.765,0.7,0.665,0.675,0.745,0.65,0.865,0.72,0.675,0.62,0.68,0.67,0.1,0.165,0.025,0.06,0.09,0.13,0.14,0.12,0.165,0.01,0.145,0.185,0.17,0.165,0.205,greedy,Train for 3000x100 examples,3000.0,300000.0,17265.21976371,EVALUATIONS/WEIGHTS_BACKUP/1617724393916395052#140189562459968,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 16:44:40,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.225945129394531,-0.1226707763671875,0.0155896532222641,1617727480269065895#139693860235072,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.11,0.06,0.03,0.065,0.075,0.095,0.09,0.055,0.06,0.01,0.08,0.09,0.075,0.06,0.08,0.68,0.44,0.79,0.67,0.605,0.54,0.515,0.585,0.58,0.885,0.6,0.515,0.535,0.555,0.46,0.725,0.64,0.795,0.695,0.625,0.595,0.57,0.665,0.635,0.885,0.675,0.595,0.605,0.66,0.595,0.12,0.145,0.035,0.1,0.11,0.155,0.155,0.12,0.13,0.01,0.14,0.15,0.145,0.14,0.15,greedy,Train for 2000x100 examples,2000.0,200000.0,17370.081568098,EVALUATIONS/WEIGHTS_BACKUP/1617727480269065895#139693860235072,30-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 16:51:01,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.861122131347656,-0.0113215332031249,0.0181302509439387,1617727861589971050#140327512172352,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.035,0.015,0.03,0.025,0.03,0.03,0.035,0.015,0.03,0.0,0.02,0.005,0.02,0.015,0.005,0.715,0.41,0.705,0.575,0.625,0.52,0.515,0.585,0.56,0.88,0.57,0.445,0.47,0.47,0.375,0.735,0.715,0.745,0.685,0.69,0.635,0.665,0.72,0.715,0.88,0.745,0.675,0.68,0.695,0.665,0.045,0.155,0.04,0.065,0.065,0.085,0.09,0.085,0.07,0.0,0.075,0.08,0.125,0.125,0.13,greedy,Train for 3000x100 examples,3000.0,300000.0,20729.647273885003,EVALUATIONS/WEIGHTS_BACKUP/1617727861589971050#140327512172352,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 17:07:51,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.6350431518554687,-0.0347481689453125,0.0278152116128662,1617728871804786797#140576646727488,100.0,5e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    weight_decay: 0
)",0.095,0.09,0.07,0.08,0.075,0.055,0.09,0.07,0.055,0.015,0.07,0.09,0.085,0.07,0.11,0.685,0.305,0.685,0.59,0.54,0.54,0.475,0.545,0.49,0.865,0.515,0.39,0.405,0.37,0.355,0.705,0.585,0.705,0.65,0.645,0.66,0.605,0.66,0.635,0.865,0.705,0.595,0.59,0.655,0.56,0.125,0.285,0.095,0.125,0.14,0.13,0.205,0.195,0.195,0.015,0.175,0.26,0.285,0.235,0.295,greedy,Train for 2000x100 examples,2000.0,200000.0,17475.783831891,EVALUATIONS/WEIGHTS_BACKUP/1617728871804786797#140576646727488,30-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 17:36:39,Shallow Embed-Process net for determining best train size,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",38.69436572265626,-0.8015981445312491,9.39987333773638,1617730599903230863#140621504546624,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.63,0.0,0.575,0.35,0.255,0.185,0.075,0.075,0.065,0.035,0.01,0.0,0.0,0.0,0.0,0.045,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.87,0.0,0.0,0.0,0.0,0.0,0.055,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.87,0.0,0.0,0.0,0.0,0.0,0.855,0.48,0.86,0.77,0.745,0.755,0.705,0.705,0.655,0.035,0.685,0.575,0.55,0.455,0.425,greedy,Train for 200 iterations on each of 500 examples,500.0,500.0,23470.790042202,EVALUATIONS/WEIGHTS_BACKUP/1617730599903230863#140621504546624,"ER(40, 0.16651646620094074)",1.0
godot,2021/04/06 17:34:19,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.03956591796875,0.0128244018554687,0.0404289456892978,1617730459657213068#139866659292992,100.0,5e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    weight_decay: 0
)",0.095,0.03,0.07,0.085,0.085,0.08,0.045,0.05,0.085,0.015,0.045,0.065,0.045,0.055,0.075,0.69,0.405,0.68,0.57,0.555,0.565,0.535,0.53,0.415,0.925,0.55,0.435,0.435,0.405,0.38,0.72,0.585,0.72,0.62,0.59,0.6,0.585,0.63,0.575,0.925,0.67,0.56,0.57,0.62,0.55,0.125,0.175,0.09,0.125,0.115,0.125,0.09,0.13,0.175,0.015,0.13,0.165,0.105,0.125,0.205,greedy,Train for 3000x100 examples,3000.0,300000.0,21523.204675518,EVALUATIONS/WEIGHTS_BACKUP/1617730459657213068#139866659292992,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 19:20:42,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.678393310546874,-0.0068197631835936,0.0217547098695298,1617736842740447393#140121043457856,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.075,0.08,0.045,0.07,0.06,0.085,0.09,0.05,0.04,0.01,0.07,0.04,0.055,0.045,0.07,0.67,0.39,0.715,0.585,0.57,0.55,0.54,0.535,0.555,0.86,0.525,0.52,0.395,0.445,0.445,0.7,0.595,0.755,0.665,0.635,0.69,0.65,0.66,0.68,0.86,0.66,0.685,0.6,0.655,0.66,0.09,0.25,0.055,0.105,0.095,0.135,0.145,0.145,0.13,0.01,0.175,0.115,0.16,0.15,0.165,greedy,Train for 2000x100 examples,2000.0,200000.0,20588.63499327,EVALUATIONS/WEIGHTS_BACKUP/1617736842740447393#140121043457856,35-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 19:36:13,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.095483459472656,-0.0097632446289062,0.0481804789968727,1617737773398944714#139712138442560,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.035,0.02,0.02,0.05,0.055,0.02,0.03,0.035,0.03,0.0,0.025,0.02,0.045,0.02,0.025,0.73,0.44,0.765,0.62,0.58,0.59,0.555,0.555,0.555,0.89,0.595,0.475,0.465,0.435,0.445,0.76,0.695,0.77,0.67,0.595,0.665,0.605,0.685,0.7,0.89,0.73,0.65,0.605,0.64,0.64,0.05,0.07,0.025,0.085,0.075,0.065,0.065,0.07,0.065,0.0,0.05,0.085,0.095,0.115,0.09,greedy,Train for 3000x100 examples,3000.0,300000.0,26401.38655893,EVALUATIONS/WEIGHTS_BACKUP/1617737773398944714#139712138442560,30-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 20:00:42,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.3776044921875,-0.0785715942382812,0.0681086764683804,1617739242230648954#140664716494656,100.0,5e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    weight_decay: 0
)",0.1,0.085,0.07,0.09,0.09,0.09,0.085,0.085,0.06,0.015,0.08,0.09,0.08,0.07,0.12,0.72,0.415,0.71,0.615,0.58,0.57,0.565,0.59,0.6,0.9,0.6,0.47,0.495,0.5,0.44,0.76,0.635,0.725,0.655,0.61,0.63,0.63,0.675,0.705,0.9,0.7,0.575,0.595,0.685,0.575,0.11,0.2,0.09,0.11,0.145,0.16,0.16,0.17,0.12,0.015,0.145,0.215,0.18,0.15,0.23,greedy,Train for 3000x100 examples,3000.0,300000.0,26892.938650376,EVALUATIONS/WEIGHTS_BACKUP/1617739242230648954#140664716494656,30-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 20:12:26,Shallow Embed-Process net for determining best train size,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",41.7995,-0.3754999999999995,7.331800999999814,1617739946423185651#140160899372864,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.125,0.0,0.055,0.045,0.02,0.005,0.015,0.0,0.005,0.015,0.01,0.01,0.0,0.0,0.0,0.345,0.0,0.24,0.13,0.075,0.05,0.025,0.02,0.03,0.895,0.0,0.005,0.005,0.005,0.0,0.37,0.025,0.29,0.19,0.13,0.135,0.095,0.09,0.06,0.895,0.095,0.065,0.035,0.035,0.01,0.475,0.5,0.41,0.535,0.47,0.49,0.485,0.625,0.48,0.015,0.55,0.525,0.515,0.545,0.47,greedy,Train for 200 iterations on each of 500 examples,500.0,500.0,32813.489756995,EVALUATIONS/WEIGHTS_BACKUP/1617739946423185651#140160899372864,"ER(50, 0.13828607398114504)",1.0
godot,2021/04/06 20:18:02,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.1639638671875,-0.089916259765625,0.0723457444598274,1617740282346212993#139887475713856,100.0,5e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    weight_decay: 0
)",0.09,0.075,0.09,0.12,0.165,0.135,0.105,0.14,0.08,0.015,0.12,0.125,0.09,0.12,0.135,0.7,0.475,0.7,0.585,0.545,0.56,0.545,0.54,0.525,0.895,0.565,0.425,0.46,0.365,0.37,0.75,0.66,0.73,0.655,0.585,0.64,0.625,0.66,0.625,0.895,0.655,0.57,0.585,0.565,0.585,0.105,0.18,0.1,0.17,0.235,0.21,0.18,0.19,0.2,0.015,0.18,0.225,0.23,0.225,0.265,greedy,Train for 2000x100 examples,2000.0,200000.0,21298.045098778,EVALUATIONS/WEIGHTS_BACKUP/1617740282346212993#139887475713856,35-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 23:13:19,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.754141357421875,0.0031753540039062,0.0120331108522631,1617750799655743761#140099107817280,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.04,0.05,0.035,0.035,0.045,0.045,0.075,0.025,0.06,0.01,0.06,0.07,0.075,0.04,0.035,0.695,0.415,0.76,0.62,0.605,0.555,0.57,0.595,0.5,0.88,0.57,0.41,0.4,0.425,0.365,0.735,0.68,0.785,0.71,0.655,0.665,0.645,0.735,0.645,0.88,0.675,0.58,0.58,0.71,0.655,0.05,0.125,0.04,0.07,0.075,0.09,0.095,0.05,0.13,0.01,0.11,0.155,0.15,0.12,0.14,greedy,Train for 3000x100 examples,3000.0,300000.0,31978.073398738,EVALUATIONS/WEIGHTS_BACKUP/1617750799655743761#140099107817280,35-cycle with expected 1 extra edge per node,1.0
godot,2021/04/06 23:35:39,"Encode-process-decode on different train sizes, learn rates, and train length","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.709689392089844,-0.0122694702148437,0.0368427241490572,1617752139825612830#140161139873600,100.0,5e-05,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 5e-05
    weight_decay: 0
)",0.09,0.03,0.02,0.035,0.075,0.065,0.065,0.03,0.055,0.01,0.06,0.01,0.025,0.02,0.055,0.69,0.47,0.71,0.6,0.535,0.525,0.51,0.575,0.505,0.89,0.59,0.465,0.46,0.455,0.415,0.72,0.72,0.74,0.73,0.61,0.665,0.68,0.74,0.69,0.89,0.75,0.72,0.675,0.69,0.64,0.09,0.16,0.025,0.075,0.12,0.12,0.12,0.11,0.1,0.01,0.12,0.115,0.13,0.12,0.195,greedy,Train for 3000x100 examples,3000.0,300000.0,31032.460601496,EVALUATIONS/WEIGHTS_BACKUP/1617752139825612830#140161139873600,35-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 09:45:52,Conv. speed for supervised embed-process on 200 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.790832580566407,0.0653146362304688,0.0185489903634916,1617788752193621340#139651526715200,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.02,0.005,0.07,0.1,0.05,0.07,0.065,0.035,0.035,0.0,0.04,0.02,0.02,0.015,0.005,0.645,0.035,0.545,0.455,0.435,0.33,0.33,0.345,0.235,0.875,0.23,0.135,0.115,0.105,0.05,0.69,0.345,0.57,0.53,0.53,0.54,0.5,0.58,0.48,0.875,0.545,0.47,0.39,0.465,0.365,0.09,0.305,0.105,0.15,0.105,0.14,0.165,0.14,0.15,0.0,0.195,0.21,0.235,0.255,0.245,greedy,Train for 200x100 examples,200.0,20000.0,1263.693436224,EVALUATIONS/WEIGHTS_BACKUP/1617788752193621340#139651526715200,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:18:03,Processor 5 layers deep in supervised embed-process,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.109053833007812,-0.0463826293945312,0.0258890853333753,1617790683830462674#139861682378560,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.11,0.045,0.08,0.115,0.115,0.125,0.125,0.115,0.115,0.01,0.09,0.095,0.06,0.06,0.05,0.7,0.04,0.625,0.45,0.38,0.335,0.29,0.3,0.23,0.93,0.21,0.125,0.07,0.1,0.05,0.765,0.31,0.76,0.58,0.525,0.48,0.465,0.53,0.46,0.93,0.48,0.34,0.28,0.4,0.325,0.13,0.425,0.11,0.185,0.205,0.27,0.28,0.295,0.3,0.01,0.31,0.41,0.46,0.335,0.405,greedy,Train for 500x100 examples,500.0,50000.0,3193.855791672,EVALUATIONS/WEIGHTS_BACKUP/1617790683830462674#139861682378560,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:18:21,Hidden dimension of 128 in supervised embed-process,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.404022155761719,0.0639553833007813,0.0178465628155883,1617790701449760111#140151626487616,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,0.07,0.09,0.115,0.115,0.095,0.155,0.12,0.105,0.01,0.105,0.065,0.035,0.06,0.01,0.705,0.07,0.66,0.51,0.43,0.36,0.26,0.305,0.235,0.925,0.24,0.18,0.135,0.095,0.095,0.735,0.355,0.69,0.575,0.5,0.46,0.4,0.445,0.435,0.925,0.395,0.4,0.315,0.31,0.355,0.12,0.39,0.105,0.19,0.175,0.255,0.325,0.34,0.33,0.01,0.33,0.385,0.345,0.415,0.33,greedy,Train for 500x100 examples,500.0,50000.0,3211.432230174,EVALUATIONS/WEIGHTS_BACKUP/1617790701449760111#140151626487616,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:19:11,Processor 3 layers deep in supervised embed-process,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.230263122558594,0.0380300903320312,0.0040298528296638,1617790751160388239#140124190746432,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.005,0.005,0.02,0.02,0.03,0.01,0.0,0.005,0.005,0.0,0.0,0.0,0.005,0.005,0.0,0.66,0.015,0.52,0.335,0.32,0.205,0.175,0.15,0.14,0.905,0.115,0.07,0.09,0.025,0.03,0.75,0.41,0.595,0.545,0.445,0.475,0.455,0.475,0.415,0.905,0.5,0.48,0.475,0.425,0.39,0.04,0.11,0.035,0.07,0.08,0.07,0.06,0.06,0.065,0.0,0.11,0.095,0.075,0.12,0.095,greedy,Train for 500x100 examples,500.0,50000.0,3260.899528129,EVALUATIONS/WEIGHTS_BACKUP/1617790751160388239#140124190746432,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:19:06,Hidden dimension of 32 in supervised embed-process,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.587197875976562,0.0364262084960936,0.0134076360714843,1617790746258465664#140705141282624,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.11,0.07,0.115,0.16,0.135,0.165,0.1,0.125,0.12,0.015,0.12,0.09,0.08,0.06,0.035,0.7,0.035,0.56,0.485,0.355,0.225,0.275,0.295,0.205,0.87,0.165,0.125,0.095,0.07,0.07,0.74,0.075,0.575,0.51,0.395,0.275,0.31,0.33,0.22,0.87,0.225,0.205,0.125,0.125,0.095,0.12,0.615,0.155,0.275,0.255,0.445,0.355,0.43,0.455,0.015,0.47,0.495,0.56,0.5,0.57,greedy,Train for 500x100 examples,500.0,50000.0,3256.2109680060003,EVALUATIONS/WEIGHTS_BACKUP/1617790746258465664#140705141282624,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:19:00,Processor 15 layers deep in supervised embed-process,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.36214892578125,0.0655657348632813,0.0420448199751071,1617790740022159202#139949993473856,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.015,0.04,0.03,0.06,0.045,0.08,0.09,0.11,0.085,0.0,0.08,0.085,0.1,0.105,0.075,0.685,0.105,0.63,0.535,0.45,0.42,0.36,0.345,0.295,0.875,0.37,0.175,0.19,0.145,0.135,0.7,0.415,0.63,0.575,0.515,0.525,0.485,0.47,0.475,0.875,0.53,0.385,0.32,0.38,0.355,0.09,0.38,0.105,0.215,0.17,0.22,0.22,0.285,0.275,0.0,0.24,0.335,0.36,0.395,0.395,greedy,Train for 500x100 examples,500.0,50000.0,3249.874784351,EVALUATIONS/WEIGHTS_BACKUP/1617790740022159202#139949993473856,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:19:16,Processor 7 layers deep in supervised embed-process,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.396211608886719,-0.0140592651367187,0.0563615310272709,1617790756613022400#140480927770432,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.015,0.03,0.025,0.065,0.04,0.02,0.025,0.075,0.045,0.0,0.035,0.04,0.035,0.035,0.04,0.67,0.065,0.605,0.45,0.365,0.335,0.25,0.31,0.28,0.86,0.195,0.125,0.145,0.065,0.04,0.725,0.415,0.655,0.58,0.545,0.575,0.485,0.515,0.57,0.86,0.54,0.455,0.45,0.455,0.36,0.075,0.345,0.055,0.085,0.11,0.09,0.13,0.165,0.15,0.0,0.185,0.235,0.22,0.265,0.39,greedy,Train for 500x100 examples,500.0,50000.0,3266.41548421,EVALUATIONS/WEIGHTS_BACKUP/1617790756613022400#140480927770432,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:20:15,Processor 2 layers deep in supervised embed-process,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.990021911621093,0.0625518798828125,0.0067658409302122,1617790814945959554#139777587369792,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.085,0.12,0.095,0.15,0.165,0.13,0.14,0.185,0.135,0.0,0.15,0.165,0.17,0.115,0.12,0.735,0.175,0.72,0.53,0.515,0.41,0.41,0.405,0.37,0.895,0.345,0.285,0.215,0.24,0.165,0.74,0.375,0.725,0.595,0.565,0.515,0.49,0.53,0.515,0.895,0.52,0.41,0.41,0.45,0.325,0.105,0.505,0.115,0.215,0.22,0.215,0.28,0.32,0.27,0.0,0.33,0.36,0.385,0.38,0.485,greedy,Train for 500x100 examples,500.0,50000.0,3325.145784766,EVALUATIONS/WEIGHTS_BACKUP/1617790814945959554#139777587369792,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:20:40,Hidden dimension of 16 in supervised embed-process,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.254826599121094,0.047910888671875,0.0669603967431697,1617790839882009540#139815377442624,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.03,0.065,0.05,0.07,0.07,0.045,0.065,0.065,0.095,0.0,0.075,0.06,0.065,0.045,0.035,0.705,0.13,0.69,0.545,0.51,0.38,0.4,0.34,0.285,0.865,0.26,0.215,0.19,0.155,0.105,0.735,0.445,0.69,0.62,0.565,0.535,0.52,0.605,0.515,0.865,0.52,0.505,0.47,0.48,0.48,0.08,0.355,0.09,0.16,0.16,0.205,0.205,0.17,0.24,0.0,0.255,0.28,0.28,0.26,0.29,greedy,Train for 500x100 examples,500.0,50000.0,3349.92112774,EVALUATIONS/WEIGHTS_BACKUP/1617790839882009540#139815377442624,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:20:57,Hidden dimension of 8 in supervised embed-process,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.235852355957031,0.0264427490234375,0.0841955318882341,1617790856974125136#140349595322176,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.015,0.005,0.015,0.055,0.025,0.025,0.02,0.02,0.01,0.0,0.025,0.035,0.01,0.02,0.02,0.675,0.035,0.635,0.49,0.455,0.335,0.32,0.315,0.275,0.88,0.23,0.115,0.08,0.07,0.05,0.7,0.375,0.665,0.6,0.56,0.525,0.535,0.605,0.495,0.88,0.56,0.435,0.425,0.475,0.375,0.04,0.28,0.05,0.105,0.06,0.14,0.125,0.15,0.145,0.0,0.165,0.23,0.22,0.23,0.27,greedy,Train for 500x100 examples,500.0,50000.0,3367.153209458,EVALUATIONS/WEIGHTS_BACKUP/1617790856974125136#140349595322176,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:21:14,Conv. speed for supervised embed-process on 500 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.29838525390625,0.0280507202148436,0.0326781108436442,1617790874075689209#140713418405696,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.02,0.1,0.055,0.05,0.05,0.055,0.055,0.08,0.1,0.0,0.085,0.095,0.085,0.11,0.115,0.69,0.14,0.665,0.52,0.425,0.435,0.395,0.39,0.285,0.885,0.335,0.225,0.17,0.215,0.155,0.69,0.34,0.665,0.56,0.48,0.535,0.53,0.545,0.415,0.885,0.5,0.415,0.385,0.395,0.33,0.1,0.46,0.095,0.18,0.175,0.22,0.21,0.26,0.315,0.0,0.28,0.33,0.365,0.38,0.455,greedy,Train for 500x100 examples,500.0,50000.0,3384.344209797,EVALUATIONS/WEIGHTS_BACKUP/1617790874075689209#140713418405696,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:21:39,Processor 10 layers deep in supervised embed-process,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.969213989257813,-0.0589195556640624,0.0159567199553034,1617790898986689629#139911387297600,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.04,0.1,0.07,0.09,0.07,0.145,0.145,0.115,0.16,0.0,0.16,0.165,0.115,0.12,0.11,0.68,0.12,0.625,0.505,0.5,0.37,0.35,0.36,0.305,0.9,0.24,0.225,0.21,0.17,0.125,0.715,0.285,0.67,0.595,0.56,0.495,0.485,0.495,0.43,0.9,0.38,0.365,0.405,0.33,0.305,0.11,0.52,0.095,0.235,0.19,0.28,0.325,0.345,0.385,0.0,0.43,0.425,0.4,0.495,0.47,greedy,Train for 500x100 examples,500.0,50000.0,3409.2381799520003,EVALUATIONS/WEIGHTS_BACKUP/1617790898986689629#139911387297600,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:21:55,Hidden dimension of 64 in supervised embed-process,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.997034362792968,3.106689453122868e-05,0.0633833761490709,1617790914974609588#139877495912256,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.09,0.075,0.1,0.16,0.095,0.125,0.13,0.145,0.095,0.005,0.1,0.095,0.1,0.075,0.06,0.73,0.07,0.675,0.535,0.485,0.42,0.355,0.36,0.32,0.885,0.29,0.265,0.13,0.16,0.12,0.745,0.29,0.68,0.565,0.535,0.505,0.44,0.54,0.505,0.885,0.465,0.415,0.35,0.385,0.325,0.11,0.455,0.13,0.265,0.19,0.235,0.255,0.28,0.24,0.005,0.255,0.295,0.355,0.355,0.33,greedy,Train for 500x100 examples,500.0,50000.0,3425.347074889,EVALUATIONS/WEIGHTS_BACKUP/1617790914974609588#139877495912256,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:49:21,Hidden dimension of 256 in supervised embed-process,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.09651171875,0.0418462524414062,0.0104704529956975,1617792561456056661#139651526715200,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.07,0.02,0.065,0.08,0.07,0.075,0.095,0.05,0.075,0.0,0.045,0.03,0.045,0.02,0.04,0.675,0.07,0.59,0.445,0.44,0.375,0.275,0.265,0.225,0.91,0.235,0.155,0.12,0.07,0.06,0.725,0.45,0.645,0.565,0.54,0.515,0.5,0.565,0.525,0.91,0.54,0.505,0.465,0.485,0.435,0.08,0.245,0.08,0.145,0.13,0.185,0.165,0.14,0.165,0.0,0.17,0.185,0.205,0.205,0.275,greedy,Train for 500x100 examples,500.0,50000.0,3381.2974505500006,EVALUATIONS/WEIGHTS_BACKUP/1617792561456056661#139651526715200,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 10:50:59,Supervised Embed-Process on noisy 10-cycles over 500 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",1.6505919189453124,0.0080230407714843,0.0195386272567925,1617792659345901159#140349595322176,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,0.035,0.065,0.135,0.145,0.11,0.1,0.1,0.075,0.005,0.05,0.08,0.045,0.04,0.055,0.675,0.05,0.665,0.52,0.435,0.345,0.315,0.29,0.27,0.88,0.235,0.175,0.105,0.115,0.075,0.675,0.075,0.67,0.525,0.465,0.385,0.36,0.31,0.295,0.88,0.275,0.19,0.14,0.16,0.11,0.125,0.52,0.07,0.215,0.215,0.27,0.255,0.38,0.29,0.005,0.4,0.46,0.465,0.505,0.55,greedy,Train for 500x100 examples,500.0,50000.0,1345.978090441,EVALUATIONS/WEIGHTS_BACKUP/1617792659345901159#140349595322176,10-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 11:14:14,Supervised Embed-Process on noisy 10-cycles over 1000 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",1.5776793823242188,-0.0023918762207031,0.0008763438081516,1617794054868603966#140713418405696,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.095,0.035,0.065,0.055,0.07,0.105,0.06,0.045,0.06,0.0,0.06,0.05,0.02,0.025,0.02,0.715,0.03,0.665,0.465,0.465,0.295,0.255,0.285,0.21,0.92,0.175,0.115,0.075,0.045,0.05,0.715,0.06,0.665,0.47,0.495,0.32,0.305,0.29,0.235,0.92,0.195,0.135,0.095,0.05,0.075,0.095,0.51,0.065,0.245,0.18,0.355,0.26,0.355,0.365,0.0,0.42,0.475,0.415,0.43,0.425,greedy,Train for 1000x100 examples,1000.0,100000.0,2715.515541212,EVALUATIONS/WEIGHTS_BACKUP/1617794054868603966#140713418405696,10-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 11:14:35,Supervised Embed-Process on noisy 20-cycles over 500 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.25756640625,-0.0894166870117188,0.0119406950639824,1617794075578959736#139911387297600,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.01,0.03,0.035,0.075,0.065,0.045,0.045,0.06,0.075,0.0,0.055,0.03,0.055,0.04,0.01,0.685,0.015,0.61,0.445,0.35,0.31,0.27,0.25,0.2,0.915,0.21,0.105,0.1,0.07,0.065,0.74,0.345,0.67,0.595,0.455,0.535,0.475,0.515,0.445,0.915,0.455,0.33,0.345,0.36,0.33,0.075,0.34,0.06,0.13,0.165,0.17,0.185,0.195,0.225,0.0,0.23,0.355,0.345,0.325,0.285,greedy,Train for 500x100 examples,500.0,50000.0,2695.4575570630004,EVALUATIONS/WEIGHTS_BACKUP/1617794075578959736#139911387297600,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 11:20:20,Conv. speed for supervised embed-process on 1000 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.003624816894531,-0.0370079956054688,0.0515185754471794,1617794420032603231#140714783745856,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.06,0.12,0.115,0.2,0.165,0.16,0.18,0.185,0.17,0.0,0.15,0.155,0.12,0.14,0.125,0.69,0.16,0.655,0.49,0.44,0.375,0.355,0.385,0.38,0.89,0.345,0.29,0.175,0.21,0.13,0.77,0.285,0.725,0.55,0.52,0.505,0.46,0.505,0.49,0.89,0.435,0.44,0.325,0.335,0.295,0.09,0.545,0.145,0.29,0.26,0.27,0.34,0.355,0.345,0.0,0.395,0.39,0.435,0.5,0.48,greedy,Train for 1000x100 examples,1000.0,100000.0,6928.127166624,EVALUATIONS/WEIGHTS_BACKUP/1617794420032603231#140714783745856,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 11:36:58,"Neighbors masked for training on 30-graphs, supervised encode-process-decode","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.342175598144531,-0.05594580078125,0.056301558084673,1617795417919885100#139861682378560,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.105,0.115,0.115,0.135,0.16,0.165,0.155,0.155,0.15,0.015,0.15,0.125,0.14,0.17,0.135,0.605,0.345,0.62,0.55,0.445,0.47,0.425,0.435,0.45,0.885,0.42,0.39,0.355,0.355,0.32,0.67,0.555,0.67,0.625,0.53,0.575,0.52,0.56,0.57,0.885,0.575,0.585,0.55,0.51,0.495,0.115,0.3,0.125,0.21,0.235,0.235,0.255,0.245,0.215,0.015,0.305,0.285,0.285,0.325,0.335,greedy,Masking neighbors! Train for 500x100 examples,500.0,50000.0,4298.395178646,EVALUATIONS/WEIGHTS_BACKUP/1617795417919885100#139861682378560,30-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 11:55:32,"Shallow Embed-Process net trained on ER(10, 0.8) for 200","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",25.982,4.388000000000002,77.01144600000009,1617796532151911457#140714783745856,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,0.0,0.145,0.145,0.065,0.08,0.04,0.04,0.015,0.015,0.01,0.0,0.005,0.0,0.0,0.575,0.0,0.33,0.215,0.13,0.065,0.045,0.025,0.025,0.885,0.01,0.0,0.0,0.0,0.0,0.62,0.005,0.405,0.345,0.22,0.145,0.095,0.09,0.06,0.885,0.065,0.015,0.015,0.03,0.0,0.15,0.01,0.18,0.205,0.155,0.205,0.105,0.135,0.1,0.015,0.095,0.055,0.055,0.02,0.025,greedy,Train for 200 iterations on each of 200 examples. Masking to select only neighbors at each step.,200.0,200.0,1624.3998214509993,EVALUATIONS/WEIGHTS_BACKUP/1617796532151911457#140714783745856,"ER(10, 0.5151730583335019)",1.0
godot,2021/04/07 12:07:19,Supervised Embed-Process on noisy 30-cycles over 500 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",5.34894287109375,0.0857715454101562,0.0262823499594588,1617797239702464723#139651526715200,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.025,0.01,0.04,0.055,0.06,0.025,0.035,0.03,0.03,0.0,0.06,0.055,0.015,0.015,0.005,0.68,0.04,0.655,0.46,0.355,0.36,0.295,0.255,0.23,0.905,0.165,0.165,0.105,0.095,0.06,0.68,0.35,0.665,0.53,0.47,0.55,0.475,0.475,0.445,0.905,0.405,0.41,0.35,0.41,0.395,0.085,0.3,0.07,0.19,0.2,0.18,0.17,0.215,0.265,0.0,0.3,0.295,0.28,0.29,0.275,greedy,Train for 500x100 examples,500.0,50000.0,4219.149772604,EVALUATIONS/WEIGHTS_BACKUP/1617797239702464723#139651526715200,30-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 12:07:26,Supervised Embed-Process on noisy 20-cycles over 1000 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.103018737792969,0.0068889770507812,0.0120208796871992,1617797246649008426#139877495912256,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.0,0.005,0.005,0.005,0.0,0.0,0.005,0.005,0.01,0.0,0.005,0.01,0.0,0.0,0.0,0.675,0.07,0.67,0.49,0.42,0.37,0.26,0.285,0.235,0.86,0.235,0.165,0.18,0.13,0.11,0.71,0.42,0.705,0.635,0.59,0.595,0.52,0.6,0.53,0.86,0.625,0.515,0.535,0.52,0.49,0.06,0.33,0.03,0.075,0.1,0.11,0.14,0.16,0.17,0.0,0.145,0.18,0.18,0.235,0.27,greedy,Train for 1000x100 examples,1000.0,100000.0,5824.219195862999,EVALUATIONS/WEIGHTS_BACKUP/1617797246649008426#139877495912256,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 12:14:00,Conv. speed for supervised embed-process on 1500 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.441495971679688,-0.0292297973632811,0.0357305144422497,1617797640544832554#139927593969472,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.04,0.035,0.09,0.13,0.1,0.095,0.095,0.11,0.105,0.0,0.095,0.05,0.055,0.065,0.035,0.665,0.02,0.595,0.445,0.36,0.34,0.26,0.25,0.14,0.875,0.18,0.1,0.08,0.04,0.045,0.735,0.215,0.64,0.54,0.455,0.475,0.415,0.42,0.33,0.875,0.405,0.315,0.265,0.325,0.21,0.045,0.355,0.13,0.22,0.175,0.19,0.225,0.285,0.295,0.0,0.265,0.29,0.31,0.295,0.33,greedy,Train for 1500x100 examples,1500.0,150000.0,10146.27411814,EVALUATIONS/WEIGHTS_BACKUP/1617797640544832554#139927593969472,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 12:46:03,"Neighbors masked for training on 50-graphs, supervised encode-process-decode","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",6.534480834960936,-0.0496075439453125,0.0496269502843418,1617799563204573289#140124190746432,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.09,0.09,0.175,0.095,0.13,0.145,0.13,0.15,0.11,0.015,0.15,0.105,0.14,0.12,0.11,0.505,0.34,0.515,0.48,0.41,0.405,0.365,0.395,0.335,0.915,0.435,0.39,0.34,0.35,0.315,0.585,0.63,0.565,0.535,0.485,0.53,0.495,0.54,0.56,0.915,0.545,0.565,0.545,0.555,0.51,0.105,0.27,0.195,0.175,0.225,0.23,0.26,0.29,0.225,0.015,0.245,0.245,0.3,0.29,0.325,greedy,Masking neighbors! Train for 500x100 examples,500.0,50000.0,8396.444695911,EVALUATIONS/WEIGHTS_BACKUP/1617799563204573289#140124190746432,50-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 12:52:39,"Shallow Embed-Process net trained on ER(10, 0.8) for 500","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",21.807,-5.367,148.64813599999997,1617799959858166254#139861682378560,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.05,0.0,0.115,0.1,0.055,0.035,0.025,0.005,0.055,0.01,0.045,0.015,0.01,0.01,0.005,0.565,0.0,0.43,0.245,0.14,0.12,0.095,0.06,0.05,0.925,0.01,0.0,0.005,0.0,0.0,0.67,0.02,0.53,0.375,0.31,0.25,0.24,0.185,0.12,0.925,0.085,0.05,0.015,0.015,0.01,0.13,0.31,0.165,0.29,0.235,0.34,0.305,0.35,0.35,0.01,0.45,0.37,0.34,0.33,0.32,greedy,Train for 200 iterations on each of 500 examples. Masking to select only neighbors at each step.,500.0,500.0,4086.51441734,EVALUATIONS/WEIGHTS_BACKUP/1617799959858166254#139861682378560,"ER(10, 0.5151730583335019)",1.0
godot,2021/04/07 12:48:52,"Neighbors masked for training on 30-graphs, supervised encode-process-decode","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.595166564941407,0.0195067749023436,0.0220895701589913,1617799732218837952#140151626487616,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.05,0.04,0.035,0.07,0.06,0.03,0.035,0.03,0.055,0.0,0.08,0.08,0.06,0.08,0.08,0.675,0.39,0.71,0.615,0.525,0.545,0.52,0.55,0.48,0.92,0.495,0.45,0.41,0.44,0.38,0.735,0.54,0.745,0.66,0.6,0.605,0.575,0.625,0.54,0.92,0.61,0.55,0.515,0.56,0.565,0.095,0.215,0.065,0.145,0.125,0.095,0.12,0.11,0.175,0.0,0.155,0.145,0.17,0.185,0.17,greedy,Masking neighbors! Train for 1000x100 examples,1000.0,100000.0,8595.991359488002,EVALUATIONS/WEIGHTS_BACKUP/1617799732218837952#140151626487616,30-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 12:57:40,"Shallow Embed-Process net trained on ER(20, 0.8) for 200","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",22.003000000000004,1.1369999999999998,7.216065999999898,1617800260474057431#140714783745856,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.155,0.0,0.095,0.115,0.045,0.02,0.005,0.02,0.0,0.14,0.0,0.005,0.0,0.0,0.0,0.34,0.0,0.125,0.09,0.02,0.02,0.015,0.0,0.0,0.8,0.0,0.0,0.0,0.0,0.0,0.61,0.045,0.34,0.285,0.235,0.255,0.135,0.17,0.15,0.8,0.13,0.12,0.075,0.1,0.055,0.29,0.225,0.285,0.44,0.29,0.375,0.345,0.35,0.285,0.14,0.36,0.36,0.29,0.245,0.245,greedy,Train for 200 iterations on each of 200 examples. Masking to select only neighbors at each step.,200.0,200.0,3584.919349740001,EVALUATIONS/WEIGHTS_BACKUP/1617800260474057431#140714783745856,"ER(20, 0.2943611031936029)",1.0
godot,2021/04/07 13:17:03,Conv. speed for supervised embed-process on 2000 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.920739074707032,0.0805923461914062,0.0071530609379468,1617801423128432537#139917653378880,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.01,0.025,0.0,0.03,0.015,0.025,0.035,0.015,0.015,0.0,0.01,0.01,0.01,0.04,0.02,0.68,0.17,0.735,0.57,0.51,0.465,0.41,0.4,0.43,0.87,0.365,0.315,0.235,0.25,0.145,0.735,0.54,0.78,0.67,0.625,0.615,0.605,0.625,0.615,0.87,0.68,0.58,0.535,0.525,0.54,0.07,0.325,0.015,0.12,0.11,0.16,0.19,0.155,0.18,0.0,0.155,0.235,0.25,0.29,0.275,greedy,Train for 2000x100 examples,2000.0,200000.0,13926.323145968,EVALUATIONS/WEIGHTS_BACKUP/1617801423128432537#139917653378880,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 13:32:42,Supervised Embed-Process on noisy 30-cycles over 1000 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.504454650878907,-0.0482848510742186,0.0038405327695905,1617802361844182853#140349595322176,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.06,0.095,0.09,0.15,0.175,0.125,0.17,0.11,0.125,0.005,0.175,0.235,0.125,0.155,0.115,0.7,0.065,0.69,0.595,0.46,0.455,0.38,0.435,0.33,0.885,0.32,0.155,0.165,0.14,0.095,0.73,0.145,0.7,0.6,0.48,0.485,0.41,0.505,0.405,0.885,0.38,0.24,0.27,0.23,0.19,0.11,0.54,0.13,0.22,0.27,0.28,0.335,0.295,0.315,0.005,0.39,0.5,0.435,0.505,0.515,greedy,Train for 1000x100 examples,1000.0,100000.0,9237.717171416,EVALUATIONS/WEIGHTS_BACKUP/1617802361844182853#140349595322176,30-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 14:01:07,"Shallow Embed-Process net trained on ER(30, 0.8) for 200","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",26.885,-0.0560000000000002,1.0008399999999256,1617804067931891671#139877495912256,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.72,0.0,0.495,0.27,0.14,0.11,0.055,0.045,0.025,0.93,0.01,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.005,0.05,0.085,0.045,0.045,0.015,0.03,0.01,0.0,0.015,0.01,0.025,0.005,0.005,0.895,0.26,0.795,0.76,0.61,0.66,0.625,0.62,0.485,0.93,0.475,0.43,0.33,0.335,0.225,greedy,Train for 200 iterations on each of 200 examples. Masking to select only neighbors at each step.,200.0,200.0,6279.369090911998,EVALUATIONS/WEIGHTS_BACKUP/1617804067931891671#139877495912256,"ER(30, 0.2112160313490763)",1.0
godot,2021/04/07 14:15:35,"Neighbors masked for training on 70-graphs, supervised encode-process-decode","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",9.1055751953125,0.2418630371093748,0.2680422522953876,1617804935867261308#139949993473856,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.105,0.02,0.085,0.125,0.15,0.165,0.095,0.14,0.085,0.015,0.09,0.06,0.015,0.045,0.015,0.565,0.03,0.535,0.335,0.225,0.175,0.165,0.125,0.1,0.92,0.14,0.08,0.06,0.03,0.035,0.67,0.05,0.58,0.435,0.29,0.275,0.27,0.26,0.22,0.92,0.27,0.165,0.16,0.105,0.07,0.13,0.085,0.115,0.2,0.21,0.26,0.175,0.255,0.16,0.015,0.2,0.21,0.095,0.19,0.095,greedy,Masking neighbors! Train for 500x100 examples,500.0,50000.0,13719.11439082,EVALUATIONS/WEIGHTS_BACKUP/1617804935867261308#139949993473856,70-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 14:48:41,"Shallow Embed-Process net trained on ER(20, 0.8) for 500","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",21.712,-1.092,28.262066000000004,1617806921961213418#139651526715200,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.0,0.075,0.105,0.045,0.065,0.035,0.02,0.02,0.015,0.0,0.005,0.0,0.005,0.0,0.475,0.0,0.35,0.16,0.125,0.075,0.035,0.035,0.015,0.9,0.005,0.0,0.0,0.0,0.0,0.715,0.16,0.605,0.525,0.405,0.38,0.33,0.38,0.245,0.9,0.25,0.17,0.21,0.16,0.16,0.185,0.395,0.215,0.28,0.325,0.415,0.36,0.4,0.47,0.015,0.42,0.41,0.435,0.385,0.32,greedy,Train for 200 iterations on each of 500 examples. Masking to select only neighbors at each step.,500.0,500.0,9202.934600055998,EVALUATIONS/WEIGHTS_BACKUP/1617806921961213418#139651526715200,"ER(20, 0.2943611031936029)",1.0
godot,2021/04/07 14:54:57,Supervised Embed-Process on noisy 70-cycles over 500 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",11.665164306640625,-0.03595703125,0.1621553379861211,1617807296914168794#140713418405696,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.035,0.055,0.02,0.055,0.115,0.085,0.07,0.13,0.09,0.0,0.09,0.085,0.065,0.06,0.06,0.69,0.07,0.63,0.495,0.4,0.395,0.35,0.365,0.31,0.92,0.35,0.22,0.2,0.16,0.115,0.725,0.395,0.69,0.59,0.52,0.58,0.5,0.525,0.535,0.92,0.56,0.505,0.51,0.5,0.46,0.09,0.45,0.055,0.16,0.185,0.205,0.24,0.295,0.27,0.0,0.255,0.325,0.315,0.325,0.39,greedy,Train for 500x100 examples,500.0,50000.0,12797.411213514,EVALUATIONS/WEIGHTS_BACKUP/1617807296914168794#140713418405696,70-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 15:09:45,"Neighbors masked for training on 50-graphs, supervised encode-process-decode","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",5.613539428710938,0.0008199462890624,0.0232730325452941,1617808185669195824#140705141282624,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.02,0.11,0.045,0.07,0.06,0.085,0.07,0.09,0.08,0.0,0.09,0.09,0.055,0.12,0.045,0.65,0.28,0.685,0.575,0.49,0.46,0.47,0.42,0.445,0.85,0.465,0.41,0.425,0.4,0.35,0.695,0.46,0.7,0.605,0.535,0.505,0.53,0.515,0.55,0.85,0.535,0.505,0.52,0.535,0.49,0.12,0.385,0.09,0.185,0.165,0.26,0.22,0.29,0.225,0.0,0.275,0.275,0.24,0.295,0.275,greedy,Masking neighbors! Train for 1000x100 examples,1000.0,100000.0,16979.726394908997,EVALUATIONS/WEIGHTS_BACKUP/1617808185669195824#140705141282624,50-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 16:34:57,"Shallow Embed-Process net trained on ER(50, 0.8) for 200","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",38.82,-0.3029999999999987,11.717310000000223,1617813297657316088#140124190746432,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.0,0.085,0.08,0.025,0.005,0.0,0.015,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.42,0.0,0.24,0.1,0.05,0.025,0.025,0.01,0.005,0.85,0.005,0.0,0.0,0.0,0.0,0.63,0.0,0.455,0.275,0.215,0.18,0.125,0.075,0.04,0.85,0.065,0.01,0.01,0.005,0.0,0.22,0.125,0.305,0.395,0.31,0.5,0.335,0.38,0.26,0.005,0.315,0.255,0.235,0.14,0.18,greedy,Train for 200 iterations on each of 200 examples. Masking to select only neighbors at each step.,200.0,200.0,13245.110163446,EVALUATIONS/WEIGHTS_BACKUP/1617813297657316088#140124190746432,"ER(50, 0.13828607398114504)",1.0
godot,2021/04/07 16:49:32,"Shallow Embed-Process net trained on ER(30, 0.8) for 500","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",36.901,3.645000000000001,113.76343400000042,1617814172807991324#139927593969472,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,0.0,0.165,0.165,0.135,0.105,0.075,0.065,0.06,0.01,0.035,0.01,0.01,0.005,0.005,0.64,0.0,0.47,0.28,0.17,0.135,0.11,0.09,0.06,0.905,0.01,0.01,0.005,0.005,0.0,0.745,0.045,0.6,0.485,0.375,0.36,0.305,0.295,0.215,0.905,0.14,0.165,0.065,0.11,0.085,0.18,0.565,0.255,0.34,0.42,0.455,0.445,0.485,0.5,0.01,0.575,0.47,0.575,0.55,0.485,greedy,Train for 200 iterations on each of 500 examples. Masking to select only neighbors at each step.,500.0,500.0,16070.828135439002,EVALUATIONS/WEIGHTS_BACKUP/1617814172807991324#139927593969472,"ER(30, 0.2112160313490763)",1.0
godot,2021/04/07 16:45:20,"Neighbors masked for training on 100-graphs, supervised encode-process-decode","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",19.550084716796874,-2.0929277343749995,1.1396254075453385,1617813920511253336#139777587369792,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,0.095,0.175,0.25,0.155,0.22,0.195,0.205,0.2,0.0,0.22,0.16,0.2,0.15,0.165,0.445,0.035,0.35,0.25,0.27,0.21,0.165,0.16,0.16,0.89,0.14,0.12,0.09,0.12,0.07,0.585,0.195,0.42,0.305,0.345,0.29,0.25,0.255,0.24,0.89,0.245,0.22,0.23,0.305,0.235,0.195,0.575,0.23,0.36,0.29,0.39,0.44,0.485,0.445,0.0,0.515,0.49,0.53,0.495,0.55,greedy,Masking neighbors! Train for 500x100 examples,500.0,50000.0,22627.168348045,EVALUATIONS/WEIGHTS_BACKUP/1617813920511253336#139777587369792,100-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 18:06:12,"Neighbors masked for training on 70-graphs, supervised encode-process-decode","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",10.089299194335938,-0.0553510742187501,0.0808568255657462,1617818772569652231#140480927770432,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.115,0.155,0.175,0.245,0.255,0.26,0.22,0.21,0.19,0.065,0.145,0.17,0.17,0.18,0.16,0.64,0.15,0.525,0.385,0.33,0.285,0.25,0.245,0.235,0.89,0.315,0.18,0.185,0.15,0.155,0.685,0.385,0.57,0.445,0.385,0.415,0.36,0.365,0.36,0.89,0.47,0.395,0.335,0.36,0.345,0.135,0.47,0.21,0.3,0.32,0.36,0.37,0.435,0.43,0.065,0.34,0.455,0.42,0.44,0.48,greedy,Masking neighbors! Train for 1000x100 examples,1000.0,100000.0,27517.828492362,EVALUATIONS/WEIGHTS_BACKUP/1617818772569652231#140480927770432,70-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 19:03:08,Supervised Embed-Process on noisy 70-cycles over 1000 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",13.111634765625002,-0.2838903808593752,0.4115096920569385,1617822188535460018#139911387297600,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.115,0.125,0.125,0.15,0.145,0.165,0.18,0.15,0.165,0.0,0.16,0.185,0.17,0.15,0.14,0.65,0.045,0.595,0.445,0.365,0.28,0.265,0.285,0.2,0.915,0.21,0.12,0.105,0.075,0.065,0.68,0.105,0.615,0.505,0.38,0.365,0.325,0.345,0.28,0.915,0.29,0.21,0.195,0.17,0.125,0.13,0.64,0.165,0.265,0.315,0.335,0.38,0.395,0.425,0.0,0.45,0.575,0.53,0.565,0.57,greedy,Train for 1000x100 examples,1000.0,100000.0,27664.236573779,EVALUATIONS/WEIGHTS_BACKUP/1617822188535460018#139911387297600,70-cycle with expected 1 extra edge per node,1.0
godot,2021/04/07 19:26:19,"Shallow Embed-Process net trained on ER(70, 0.8) for 200","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",58.773,0.386999999999999,3.357285999999476,1617823579161504212#140151626487616,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.385,0.0,0.12,0.045,0.005,0.015,0.01,0.0,0.0,0.045,0.0,0.0,0.0,0.0,0.0,0.015,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.825,0.0,0.0,0.0,0.0,0.0,0.025,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.825,0.0,0.0,0.0,0.0,0.0,0.65,0.225,0.39,0.45,0.26,0.39,0.29,0.29,0.255,0.045,0.26,0.28,0.245,0.255,0.185,greedy,Train for 200 iterations on each of 200 examples. Masking to select only neighbors at each step.,200.0,200.0,23419.90114487,EVALUATIONS/WEIGHTS_BACKUP/1617823579161504212#140151626487616,"ER(70, 0.10427536359937167)",1.0
godot,2021/04/07 22:37:58,"Shallow Embed-Process net trained on ER(50, 0.8) for 500","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",42.09599999999999,0.1560000000000002,0.8431340000008731,1617835078192748760#139861682378560,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.335,0.0,0.175,0.1,0.025,0.01,0.005,0.01,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.01,0.005,0.0,0.0,0.0,0.0,0.0,0.845,0.0,0.0,0.0,0.0,0.0,0.155,0.0,0.01,0.005,0.0,0.0,0.005,0.0,0.0,0.845,0.0,0.0,0.0,0.0,0.0,0.64,0.245,0.54,0.6,0.41,0.455,0.355,0.465,0.37,0.01,0.41,0.38,0.325,0.295,0.235,greedy,Train for 200 iterations on each of 500 examples. Masking to select only neighbors at each step.,500.0,500.0,34938.419253509,EVALUATIONS/WEIGHTS_BACKUP/1617835078192748760#139861682378560,"ER(50, 0.13828607398114504)",1.0
godot,2021/04/07 23:30:03,"Neighbors masked for training on 100-graphs, supervised encode-process-decode","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",16.791263916015623,-0.101646484375,0.4548529777996464,1617838203385884666#139815377442624,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessageParsing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessageParsing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessageParsing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,0.085,0.18,0.17,0.1,0.15,0.155,0.2,0.155,0.015,0.155,0.135,0.11,0.075,0.07,0.39,0.015,0.3,0.205,0.17,0.12,0.14,0.105,0.07,0.88,0.075,0.06,0.035,0.045,0.04,0.525,0.2,0.385,0.315,0.295,0.275,0.27,0.24,0.195,0.88,0.21,0.19,0.185,0.215,0.22,0.19,0.545,0.26,0.3,0.265,0.325,0.305,0.46,0.45,0.015,0.455,0.495,0.465,0.455,0.55,greedy,Masking neighbors! Train for 1000x100 examples,1000.0,100000.0,46880.53842867,EVALUATIONS/WEIGHTS_BACKUP/1617838203385884666#139815377442624,100-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 05:29:18,"Shallow Embed-Process net trained on ER(70, 0.8) for 500","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",56.834,-0.1980000000000004,1.120473999999831,1617859758789457332#140714783745856,200.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessageParsing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessageParsing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.165,0.0,0.105,0.055,0.04,0.01,0.0,0.0,0.0,0.22,0.005,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.025,0.0,0.005,0.015,0.0,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.465,0.09,0.385,0.45,0.235,0.415,0.27,0.305,0.21,0.22,0.28,0.2,0.205,0.14,0.095,greedy,Train for 200 iterations on each of 500 examples. Masking to select only neighbors at each step.,500.0,500.0,59278.314496570005,EVALUATIONS/WEIGHTS_BACKUP/1617859758789457332#140714783745856,"ER(70, 0.10427536359937167)",1.0
godot,2021/04/08 11:19:16,Processor 2 layers deep in supervised encode-process-decode,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.375755615234374,-0.0507112426757812,0.0303898813191487,1617880756266252798#139846383318848,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
=================================================================
Total params: 8,384
Trainable params: 8,384
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.11,0.16,0.13,0.125,0.165,0.19,0.14,0.135,0.155,0.015,0.155,0.13,0.135,0.13,0.105,0.585,0.31,0.52,0.48,0.4,0.375,0.395,0.405,0.315,0.905,0.35,0.325,0.29,0.335,0.275,0.635,0.535,0.55,0.575,0.515,0.505,0.505,0.62,0.465,0.905,0.575,0.575,0.48,0.6,0.555,0.13,0.395,0.15,0.19,0.255,0.3,0.255,0.245,0.315,0.015,0.295,0.28,0.38,0.295,0.325,greedy,Train for 500x100 examples,500.0,50000.0,1525.873199344,EVALUATIONS/WEIGHTS_BACKUP/1617880756266252798#139846383318848,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 11:15:10,Conv. speed for supervised encode-process-decode on 200 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.360587097167969,-0.0220004272460936,0.0184204969123449,1617880510678605156#140635889071936,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.165,0.165,0.15,0.27,0.24,0.22,0.225,0.185,0.195,0.02,0.275,0.195,0.185,0.21,0.215,0.45,0.135,0.46,0.275,0.23,0.24,0.195,0.24,0.19,0.9,0.2,0.195,0.14,0.2,0.16,0.525,0.355,0.505,0.34,0.33,0.35,0.3,0.38,0.375,0.9,0.345,0.335,0.335,0.325,0.31,0.215,0.47,0.21,0.395,0.375,0.42,0.4,0.385,0.405,0.02,0.46,0.46,0.495,0.49,0.545,greedy,Train for 200x100 examples,200.0,20000.0,1280.570531028,EVALUATIONS/WEIGHTS_BACKUP/1617880510678605156#140635889071936,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 11:29:03,Processor 3 layers deep in supervised encode-process-decode,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.768562744140625,0.0429482421875,0.007771952389616,1617881343179680436#140483880470336,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
=================================================================
Total params: 12,576
Trainable params: 12,576
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.085,0.05,0.065,0.035,0.035,0.065,0.03,0.055,0.045,0.045,0.055,0.045,0.03,0.015,0.06,0.605,0.245,0.585,0.49,0.48,0.455,0.385,0.385,0.41,0.875,0.38,0.34,0.315,0.36,0.275,0.68,0.61,0.63,0.625,0.625,0.585,0.58,0.64,0.665,0.875,0.635,0.61,0.595,0.675,0.63,0.1,0.255,0.08,0.1,0.11,0.145,0.13,0.16,0.13,0.045,0.135,0.17,0.17,0.17,0.215,greedy,Train for 500x100 examples,500.0,50000.0,2112.339832604,EVALUATIONS/WEIGHTS_BACKUP/1617881343179680436#140483880470336,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 11:47:06,Hidden dimension of 64 in supervised encode-process-decode,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.974363952636719,0.0709216918945313,0.0141906637390452,1617882426213958586#140531610277696,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.09,0.19,0.185,0.14,0.145,0.14,0.12,0.175,0.15,0.045,0.115,0.165,0.14,0.15,0.17,0.635,0.37,0.59,0.545,0.525,0.565,0.475,0.5,0.485,0.875,0.515,0.44,0.38,0.415,0.305,0.67,0.535,0.6,0.605,0.59,0.6,0.54,0.59,0.57,0.875,0.64,0.545,0.49,0.565,0.42,0.115,0.375,0.205,0.2,0.26,0.25,0.29,0.285,0.245,0.045,0.25,0.325,0.34,0.31,0.43,greedy,Train for 500x100 examples,500.0,50000.0,3194.697610024,EVALUATIONS/WEIGHTS_BACKUP/1617882426213958586#140531610277696,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 11:47:40,Hidden dimension of 128 in supervised encode-process-decode,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.537065551757813,0.0172476806640625,0.025505560305211,1617882460623138447#140067215947584,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.13,0.19,0.16,0.155,0.16,0.23,0.155,0.18,0.175,0.005,0.16,0.19,0.185,0.175,0.16,0.585,0.32,0.61,0.565,0.465,0.445,0.475,0.52,0.465,0.905,0.475,0.425,0.355,0.4,0.41,0.645,0.55,0.64,0.615,0.54,0.535,0.58,0.63,0.555,0.905,0.61,0.57,0.46,0.58,0.55,0.16,0.385,0.175,0.21,0.265,0.325,0.26,0.24,0.275,0.005,0.275,0.315,0.36,0.305,0.33,greedy,Train for 500x100 examples,500.0,50000.0,3228.659637623,EVALUATIONS/WEIGHTS_BACKUP/1617882460623138447#140067215947584,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 11:48:15,Conv. speed for supervised encode-process-decode on 500 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.597178955078125,0.0467616577148437,0.0813506427641179,1617882495881179754#140688436934464,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,0.145,0.185,0.165,0.21,0.155,0.185,0.175,0.145,0.015,0.165,0.21,0.17,0.185,0.155,0.62,0.37,0.57,0.55,0.45,0.5,0.465,0.535,0.49,0.9,0.49,0.365,0.405,0.365,0.39,0.66,0.585,0.59,0.6,0.5,0.58,0.575,0.62,0.605,0.9,0.63,0.52,0.535,0.515,0.55,0.15,0.31,0.21,0.23,0.26,0.255,0.25,0.25,0.22,0.015,0.245,0.335,0.295,0.305,0.29,greedy,Train for 500x100 examples,500.0,50000.0,3264.25475036,EVALUATIONS/WEIGHTS_BACKUP/1617882495881179754#140688436934464,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 11:48:34,Processor 5 layers deep in supervised encode-process-decode,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.445580566406249,-0.149109375,0.0133671598793441,1617882514417708062#139741144889152,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.125,0.16,0.135,0.14,0.23,0.215,0.175,0.16,0.17,0.0,0.225,0.16,0.155,0.18,0.24,0.54,0.435,0.585,0.515,0.415,0.45,0.425,0.515,0.485,0.92,0.44,0.445,0.38,0.405,0.33,0.615,0.58,0.62,0.58,0.465,0.53,0.54,0.585,0.58,0.92,0.54,0.585,0.525,0.54,0.475,0.14,0.345,0.14,0.19,0.31,0.285,0.275,0.275,0.245,0.0,0.31,0.275,0.325,0.315,0.385,greedy,Train for 500x100 examples,500.0,50000.0,3282.728353909,EVALUATIONS/WEIGHTS_BACKUP/1617882514417708062#139741144889152,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 11:48:33,Hidden dimension of 16 in supervised encode-process-decode,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.735405944824219,-0.0198200683593749,0.0049909971471624,1617882513177809654#140389934511936,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.135,0.045,0.155,0.165,0.13,0.125,0.115,0.095,0.1,0.01,0.11,0.1,0.075,0.075,0.095,0.495,0.41,0.445,0.395,0.4,0.39,0.375,0.46,0.365,0.88,0.43,0.35,0.355,0.38,0.325,0.565,0.695,0.505,0.505,0.575,0.505,0.55,0.6,0.605,0.88,0.61,0.585,0.585,0.64,0.635,0.155,0.18,0.2,0.28,0.2,0.23,0.19,0.235,0.21,0.01,0.22,0.24,0.275,0.225,0.27,greedy,Train for 500x100 examples,500.0,50000.0,3280.932135632,EVALUATIONS/WEIGHTS_BACKUP/1617882513177809654#140389934511936,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 11:48:45,Hidden dimension of 8 in supervised encode-process-decode,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.885704345703125,-0.0209473876953125,0.0115997450197102,1617882525363090384#140620527241024,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.145,0.25,0.145,0.235,0.195,0.215,0.26,0.275,0.195,0.01,0.235,0.26,0.195,0.21,0.235,0.4,0.28,0.475,0.39,0.34,0.39,0.33,0.335,0.34,0.875,0.395,0.295,0.315,0.325,0.29,0.535,0.445,0.53,0.455,0.445,0.48,0.395,0.44,0.43,0.875,0.52,0.42,0.45,0.435,0.395,0.19,0.485,0.2,0.34,0.305,0.315,0.39,0.41,0.36,0.01,0.345,0.45,0.38,0.44,0.46,greedy,Train for 500x100 examples,500.0,50000.0,3293.319803119,EVALUATIONS/WEIGHTS_BACKUP/1617882525363090384#140620527241024,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 11:48:41,Hidden dimension of 32 in supervised encode-process-decode,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.406828918457032,0.0240006103515624,0.0160905731625859,1617882521120951783#140669008701248,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,0.17,0.14,0.16,0.145,0.15,0.145,0.185,0.155,0.06,0.16,0.105,0.13,0.12,0.235,0.665,0.365,0.7,0.6,0.56,0.545,0.52,0.555,0.515,0.865,0.475,0.465,0.41,0.42,0.345,0.71,0.56,0.73,0.66,0.635,0.635,0.61,0.625,0.645,0.865,0.61,0.62,0.555,0.585,0.52,0.125,0.325,0.145,0.19,0.215,0.24,0.235,0.245,0.23,0.06,0.265,0.26,0.27,0.265,0.395,greedy,Train for 500x100 examples,500.0,50000.0,3289.100657132,EVALUATIONS/WEIGHTS_BACKUP/1617882521120951783#140669008701248,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 12:05:29,Processor 7 layers deep in supervised encode-process-decode,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.6754219970703126,0.02773046875,0.0287781558488831,1617883529126972479#140155666908992,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
=================================================================
Total params: 29,344
Trainable params: 29,344
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.125,0.21,0.125,0.14,0.195,0.23,0.21,0.225,0.195,0.015,0.2,0.22,0.23,0.225,0.22,0.65,0.315,0.705,0.6,0.49,0.46,0.43,0.45,0.45,0.93,0.44,0.365,0.315,0.305,0.275,0.69,0.435,0.72,0.66,0.53,0.525,0.53,0.525,0.515,0.93,0.565,0.485,0.435,0.475,0.42,0.135,0.41,0.125,0.185,0.235,0.3,0.27,0.315,0.295,0.015,0.285,0.365,0.39,0.345,0.405,greedy,Train for 500x100 examples,500.0,50000.0,4297.084515756,EVALUATIONS/WEIGHTS_BACKUP/1617883529126972479#140155666908992,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 12:16:57,Hidden dimension of 256 in supervised encode-process-decode,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.601562744140625,-0.0288428344726562,0.0235142179875591,1617884217253356128#139846383318848,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.11,0.06,0.1,0.11,0.125,0.115,0.115,0.15,0.09,0.045,0.08,0.045,0.1,0.095,0.075,0.63,0.395,0.625,0.59,0.475,0.525,0.51,0.485,0.49,0.88,0.5,0.385,0.38,0.405,0.36,0.69,0.635,0.665,0.655,0.59,0.635,0.62,0.605,0.635,0.88,0.69,0.615,0.62,0.615,0.62,0.115,0.21,0.115,0.135,0.205,0.23,0.245,0.23,0.165,0.045,0.15,0.19,0.24,0.225,0.235,greedy,Train for 500x100 examples,500.0,50000.0,3246.0980418889994,EVALUATIONS/WEIGHTS_BACKUP/1617884217253356128#139846383318848,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 12:34:35,Processor 10 layers deep in supervised encode-process-decode,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.879304077148437,0.0984131469726562,0.0067707782049701,1617885275798089734#140252003276608,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessagePassing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessagePassing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessagePassing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
=================================================================
Total params: 41,920
Trainable params: 41,920
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.085,0.14,0.08,0.155,0.125,0.105,0.175,0.17,0.175,0.015,0.11,0.115,0.14,0.14,0.165,0.64,0.33,0.7,0.52,0.525,0.51,0.485,0.51,0.415,0.89,0.495,0.4,0.345,0.36,0.38,0.695,0.55,0.735,0.6,0.62,0.615,0.61,0.63,0.55,0.89,0.67,0.585,0.525,0.585,0.58,0.095,0.32,0.1,0.21,0.21,0.19,0.25,0.23,0.25,0.015,0.2,0.265,0.34,0.29,0.29,greedy,Train for 500x100 examples,500.0,50000.0,6042.635805812,EVALUATIONS/WEIGHTS_BACKUP/1617885275798089734#140252003276608,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 12:43:40,Conv. speed for supervised encode-process-decode on 1000 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.1147876586914065,0.0962255249023437,0.023121603508418,1617885820576526436#139660663887680,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.045,0.065,0.05,0.085,0.095,0.125,0.11,0.115,0.075,0.01,0.1,0.07,0.1,0.1,0.1,0.7,0.445,0.71,0.575,0.5,0.535,0.49,0.52,0.51,0.905,0.53,0.48,0.47,0.485,0.41,0.72,0.675,0.72,0.675,0.585,0.585,0.585,0.64,0.66,0.905,0.66,0.645,0.625,0.645,0.6,0.05,0.19,0.055,0.125,0.155,0.16,0.15,0.2,0.15,0.01,0.17,0.18,0.21,0.21,0.24,greedy,Train for 1000x100 examples,1000.0,100000.0,6586.679920197999,EVALUATIONS/WEIGHTS_BACKUP/1617885820576526436#139660663887680,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 13:26:40,Processor 15 layers deep in supervised encode-process-decode,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",6.658210815429688,-0.1589219970703125,0.1064699012734777,1617888400544804942#139811543091008,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             2,112
|    |    └─Sequential: 3-12             2,080
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             2,112
|    |    └─Sequential: 3-14             2,080
|    └─MaxMessagePassing: 2-8            --
|    |    └─Sequential: 3-15             2,112
|    |    └─Sequential: 3-16             2,080
|    └─MaxMessagePassing: 2-9            --
|    |    └─Sequential: 3-17             2,112
|    |    └─Sequential: 3-18             2,080
|    └─MaxMessagePassing: 2-10           --
|    |    └─Sequential: 3-19             2,112
|    |    └─Sequential: 3-20             2,080
|    └─MaxMessagePassing: 2-11           --
|    |    └─Sequential: 3-21             2,112
|    |    └─Sequential: 3-22             2,080
|    └─MaxMessagePassing: 2-12           --
|    |    └─Sequential: 3-23             2,112
|    |    └─Sequential: 3-24             2,080
|    └─MaxMessagePassing: 2-13           --
|    |    └─Sequential: 3-25             2,112
|    |    └─Sequential: 3-26             2,080
|    └─MaxMessagePassing: 2-14           --
|    |    └─Sequential: 3-27             2,112
|    |    └─Sequential: 3-28             2,080
|    └─MaxMessagePassing: 2-15           --
|    |    └─Sequential: 3-29             2,112
|    |    └─Sequential: 3-30             2,080
=================================================================
Total params: 62,880
Trainable params: 62,880
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.25,0.0,0.19,0.16,0.125,0.115,0.11,0.085,0.08,0.065,0.09,0.055,0.03,0.035,0.02,0.305,0.0,0.2,0.11,0.05,0.04,0.045,0.03,0.015,0.87,0.03,0.005,0.015,0.01,0.0,0.48,0.02,0.26,0.205,0.13,0.135,0.115,0.105,0.09,0.87,0.085,0.085,0.08,0.085,0.065,0.36,0.285,0.305,0.37,0.275,0.345,0.315,0.35,0.335,0.065,0.375,0.27,0.26,0.36,0.33,greedy,Train for 500x100 examples,500.0,50000.0,9164.414722974,EVALUATIONS/WEIGHTS_BACKUP/1617888400544804942#139811543091008,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 13:38:57,Conv. speed for supervised encode-process-decode on 1500 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.0360577392578127,-0.0231457519531249,0.0329540972388144,1617889137251008266#139705758848832,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.05,0.1,0.035,0.055,0.06,0.045,0.08,0.065,0.11,0.015,0.07,0.09,0.07,0.07,0.12,0.735,0.525,0.76,0.645,0.595,0.595,0.605,0.635,0.595,0.895,0.635,0.555,0.5,0.595,0.515,0.77,0.69,0.785,0.735,0.67,0.725,0.685,0.715,0.685,0.895,0.73,0.725,0.66,0.7,0.66,0.065,0.215,0.05,0.09,0.1,0.095,0.115,0.115,0.15,0.015,0.145,0.155,0.15,0.115,0.205,greedy,Train for 1500x100 examples,1500.0,150000.0,9901.211953736,EVALUATIONS/WEIGHTS_BACKUP/1617889137251008266#139705758848832,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 14:33:18,Conv. speed for supervised encode-process-decode on 2000 epochs,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.7392285766601563,-0.0232100219726562,0.0175286770229927,1617892398077150462#139875822155584,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.03,0.1,0.035,0.075,0.04,0.08,0.025,0.055,0.055,0.01,0.06,0.04,0.03,0.04,0.085,0.685,0.455,0.71,0.625,0.63,0.58,0.575,0.575,0.575,0.9,0.585,0.55,0.54,0.535,0.445,0.71,0.63,0.74,0.715,0.7,0.66,0.71,0.72,0.68,0.9,0.72,0.72,0.695,0.69,0.64,0.07,0.245,0.045,0.14,0.07,0.17,0.08,0.13,0.13,0.01,0.17,0.14,0.13,0.2,0.215,greedy,Train for 2000x100 examples,2000.0,200000.0,13161.347980859,EVALUATIONS/WEIGHTS_BACKUP/1617892398077150462#139875822155584,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 15:17:59,"Hq neighbors masked on 25-graphs, supervised encode-process-decode","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.0493764038085933,0.0143289184570313,0.0092419672456252,1617895077004993505#140635889071936,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.01,0.0,0.02,0.015,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.295,0.705,0.605,0.59,0.535,0.575,0.6,0.5,0.95,0.555,0.485,0.445,0.44,0.36,0.735,0.595,0.735,0.67,0.71,0.655,0.71,0.765,0.695,0.95,0.73,0.695,0.675,0.685,0.615,0.06,0.255,0.03,0.07,0.08,0.105,0.085,0.08,0.08,0.0,0.11,0.17,0.135,0.16,0.23,greedy,Masking neighbors! Train for 2000x100 examples,2000.0,200000.0,14084.254680088,EVALUATIONS/WEIGHTS_BACKUP/1617895077004993505#140635889071936,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 16:23:35,"Hq neighbors masked on 30-graphs, supervised encode-process-decode","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.436705505371094,0.0875587158203124,0.0223444188983563,1617899014033057427#140483880470336,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.695,0.465,0.71,0.64,0.565,0.505,0.525,0.595,0.54,0.865,0.505,0.455,0.43,0.48,0.45,0.695,0.685,0.71,0.715,0.625,0.67,0.665,0.725,0.67,0.865,0.68,0.665,0.66,0.73,0.66,0.05,0.22,0.035,0.035,0.045,0.115,0.08,0.11,0.11,0.0,0.115,0.15,0.16,0.135,0.19,greedy,Masking neighbors! Train for 2000x100 examples,2000.0,200000.0,17366.322665505,EVALUATIONS/WEIGHTS_BACKUP/1617899014033057427#140483880470336,30-cycle with expected 1 extra edge per node,1.0
godot,2021/04/08 21:18:59,"Hq neighbors masked on 50-graphs, supervised encode-process-decode","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",5.351095886230469,-0.0856613159179687,0.0463018082480779,1617916738776364804#140531610277696,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              2,112
|    |    └─Sequential: 3-2              2,080
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              2,112
|    |    └─Sequential: 3-4              2,080
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              2,112
|    |    └─Sequential: 3-6              2,080
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              2,112
|    |    └─Sequential: 3-8              2,080
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              2,112
|    |    └─Sequential: 3-10             2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.69,0.365,0.705,0.59,0.51,0.465,0.465,0.535,0.525,0.89,0.52,0.465,0.45,0.455,0.395,0.715,0.62,0.71,0.665,0.545,0.55,0.54,0.665,0.625,0.89,0.67,0.59,0.62,0.585,0.63,0.065,0.19,0.015,0.065,0.08,0.155,0.125,0.1,0.125,0.0,0.1,0.195,0.17,0.195,0.145,greedy,Masking neighbors! Train for 2000x100 examples,2000.0,200000.0,33823.668389004,EVALUATIONS/WEIGHTS_BACKUP/1617916738776364804#140531610277696,50-cycle with expected 1 extra edge per node,1.0
godot,2021/04/09 09:18:47,"GatedGCN e-p-d on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.288515930175781,0.0464138793945312,0.0552317319728441,1617959927558312188#140546538112832,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,0.04,0.19,0.28,0.225,0.19,0.19,0.185,0.135,0.015,0.11,0.11,0.08,0.035,0.05,0.76,0.045,0.66,0.47,0.375,0.335,0.185,0.195,0.19,0.985,0.175,0.07,0.065,0.07,0.03,0.78,0.245,0.68,0.495,0.445,0.455,0.315,0.37,0.315,0.985,0.385,0.26,0.2,0.23,0.2,0.13,0.35,0.205,0.33,0.31,0.305,0.315,0.34,0.28,0.015,0.255,0.34,0.355,0.27,0.345,greedy,Train for 100x100 examples,100.0,10000.0,820.6368773390001,EVALUATIONS/WEIGHTS_BACKUP/1617959927558312188#140546538112832,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/09 09:22:04,"GatedGCN e-p-d on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.7929832153320313,0.029326416015625,0.0430653950433281,1617960124045436838#140014449153856,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.235,0.21,0.185,0.165,0.225,0.245,0.215,0.26,0.19,0.015,0.21,0.29,0.22,0.245,0.245,0.645,0.085,0.57,0.475,0.35,0.285,0.29,0.265,0.255,0.985,0.19,0.12,0.125,0.14,0.1,0.68,0.255,0.645,0.575,0.445,0.39,0.395,0.385,0.375,0.985,0.365,0.28,0.255,0.285,0.255,0.24,0.525,0.205,0.26,0.27,0.34,0.35,0.43,0.345,0.015,0.375,0.5,0.425,0.51,0.5,greedy,Train for 100x100 examples,100.0,10000.0,1016.909998491,EVALUATIONS/WEIGHTS_BACKUP/1617960124045436838#140014449153856,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/09 09:29:16,"GatedGCN e-p-d on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",113.67773046875,-0.0667187499999983,0.0111914239751058,1617960556799278652#140681700276032,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.015,0.0,0.0,0.005,0.005,0.005,0.0,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.255,0.0,0.0,0.0,0.0,0.0,0.005,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.255,0.0,0.0,0.0,0.0,0.0,0.03,0.005,0.015,0.01,0.015,0.02,0.0,0.0,0.01,0.005,0.0,0.0,0.0,0.0,0.0,greedy,Train for 100x100 examples,100.0,10000.0,1449.30459351,EVALUATIONS/WEIGHTS_BACKUP/1617960556799278652#140681700276032,35-cycle with expected 1 extra edge per node,1.0
godot,2021/04/09 09:31:37,"GatedGCN emb-proc on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",48.538,-1.397999999999999,582.7760960000005,1617960697961257356#140659467728704,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,0.435,0.125,0.205,0.185,0.205,0.205,0.21,0.205,0.015,0.28,0.32,0.25,0.38,0.375,0.75,0.29,0.745,0.625,0.565,0.54,0.51,0.55,0.565,0.985,0.485,0.405,0.445,0.35,0.33,0.755,0.32,0.745,0.625,0.58,0.54,0.53,0.57,0.575,0.985,0.495,0.43,0.465,0.38,0.365,0.145,0.49,0.125,0.21,0.205,0.215,0.235,0.24,0.23,0.015,0.33,0.37,0.285,0.43,0.41,greedy,Train for 100 iterations on each of 100 examples. Masking to select only neighbors at each step.,100.0,100.0,1590.635640211,EVALUATIONS/WEIGHTS_BACKUP/1617960697961257356#140659467728704,"ER(20, 0.2943611031936029)",1.0
godot,2021/04/09 09:42:06,"GatedGCN e-p-d on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",7.208755737304687,0.0895718994140624,0.1094075220415859,1617961326043250434#140343920854848,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.34,0.24,0.29,0.33,0.275,0.225,0.33,0.255,0.25,0.015,0.295,0.265,0.23,0.24,0.29,0.535,0.185,0.485,0.315,0.29,0.255,0.18,0.235,0.205,0.985,0.21,0.17,0.145,0.16,0.125,0.54,0.28,0.53,0.405,0.345,0.35,0.295,0.355,0.31,0.985,0.305,0.315,0.245,0.245,0.265,0.375,0.51,0.33,0.47,0.375,0.405,0.51,0.49,0.44,0.015,0.455,0.51,0.495,0.51,0.52,greedy,Train for 100x100 examples,100.0,10000.0,2217.707435136,EVALUATIONS/WEIGHTS_BACKUP/1617961326043250434#140343920854848,50-cycle with expected 1 extra edge per node,1.0
godot,2021/04/09 09:59:16,"GatedGCN emb-proc on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",47.846,0.1340000000000003,586.2581040000005,1617962356151393680#140546538112832,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.23,0.56,0.315,0.32,0.35,0.42,0.435,0.52,0.55,0.085,0.495,0.46,0.49,0.52,0.54,0.645,0.185,0.58,0.485,0.365,0.31,0.29,0.255,0.205,0.91,0.27,0.25,0.19,0.195,0.185,0.65,0.19,0.585,0.495,0.38,0.33,0.3,0.26,0.215,0.91,0.275,0.255,0.19,0.2,0.195,0.23,0.6,0.325,0.33,0.38,0.46,0.47,0.555,0.6,0.085,0.545,0.525,0.57,0.585,0.61,greedy,Train for 100 iterations on each of 100 examples. Masking to select only neighbors at each step.,100.0,100.0,2013.490747057,EVALUATIONS/WEIGHTS_BACKUP/1617962356151393680#140546538112832,"ER(25, 0.24532699947978218)",1.0
godot,2021/04/09 10:16:09,"GatedGCN e-p-d on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.641127502441406,-0.0442339477539063,0.0086142484182358,1617963369185829602#139716592785216,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.105,0.1,0.055,0.13,0.105,0.135,0.155,0.12,0.19,0.01,0.17,0.125,0.105,0.15,0.145,0.76,0.235,0.73,0.605,0.575,0.565,0.5,0.52,0.43,0.9,0.5,0.405,0.38,0.355,0.29,0.775,0.33,0.755,0.64,0.605,0.61,0.525,0.57,0.48,0.9,0.54,0.46,0.435,0.42,0.365,0.125,0.295,0.1,0.19,0.2,0.205,0.255,0.26,0.3,0.01,0.295,0.255,0.265,0.295,0.29,greedy,Train for 500x100 examples,500.0,50000.0,4260.334921747,EVALUATIONS/WEIGHTS_BACKUP/1617963369185829602#139716592785216,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/09 10:26:48,"GatedGCN emb-proc on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",51.79200000000001,0.4180000000000021,597.7642959999998,1617964008962056416#140659467728704,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.62,0.67,0.67,0.725,0.67,0.66,0.665,0.725,0.725,0.14,0.72,0.68,0.64,0.67,0.67,0.285,0.03,0.22,0.08,0.05,0.065,0.04,0.045,0.035,0.86,0.05,0.04,0.045,0.045,0.02,0.285,0.03,0.225,0.085,0.055,0.08,0.04,0.045,0.04,0.86,0.05,0.045,0.045,0.045,0.03,0.635,0.765,0.69,0.755,0.735,0.735,0.74,0.79,0.79,0.14,0.77,0.765,0.72,0.78,0.79,greedy,Train for 100 iterations on each of 100 examples. Masking to select only neighbors at each step.,100.0,100.0,3052.133087422,EVALUATIONS/WEIGHTS_BACKUP/1617964008962056416#140659467728704,"ER(35, 0.18599238277363112)",1.0
godot,2021/04/09 10:36:09,"GatedGCN e-p-d on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.13905078125,-0.0588236083984375,0.0556058855611301,1617964569213715550#139757890869056,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.075,0.195,0.105,0.215,0.175,0.185,0.195,0.205,0.205,0.015,0.195,0.22,0.23,0.145,0.17,0.77,0.285,0.75,0.59,0.52,0.525,0.485,0.425,0.455,0.985,0.42,0.36,0.295,0.33,0.25,0.775,0.445,0.78,0.655,0.575,0.615,0.555,0.545,0.535,0.985,0.545,0.455,0.42,0.5,0.415,0.085,0.395,0.105,0.225,0.23,0.225,0.28,0.3,0.285,0.015,0.305,0.37,0.36,0.36,0.385,greedy,Train for 500x100 examples,500.0,50000.0,5459.32730594,EVALUATIONS/WEIGHTS_BACKUP/1617964569213715550#139757890869056,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/09 11:14:37,"GatedGCN e-p-d on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.185329223632813,-0.0577302856445312,0.0134668504714703,1617966877842377798#140526598387520,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.155,0.27,0.22,0.27,0.225,0.29,0.295,0.295,0.35,0.01,0.335,0.315,0.3,0.31,0.29,0.665,0.3,0.59,0.435,0.455,0.385,0.345,0.41,0.355,0.985,0.36,0.31,0.255,0.3,0.315,0.7,0.465,0.615,0.5,0.5,0.45,0.42,0.49,0.41,0.985,0.43,0.415,0.385,0.41,0.425,0.165,0.43,0.265,0.34,0.275,0.355,0.4,0.385,0.465,0.01,0.44,0.445,0.425,0.485,0.42,greedy,Train for 500x100 examples,500.0,50000.0,7765.435987806,EVALUATIONS/WEIGHTS_BACKUP/1617966877842377798#140526598387520,35-cycle with expected 1 extra edge per node,1.0
godot,2021/04/09 11:22:59,"GatedGCN emb-proc on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",51.69200000000001,-1.978,347.69349599999987,1617967379427618754#139802784470848,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.08,0.02,0.06,0.045,0.04,0.035,0.02,0.03,0.02,0.015,0.03,0.04,0.025,0.04,0.03,0.795,0.19,0.78,0.66,0.555,0.485,0.45,0.445,0.48,0.985,0.38,0.26,0.245,0.295,0.21,0.805,0.35,0.81,0.7,0.665,0.61,0.63,0.615,0.565,0.985,0.56,0.47,0.39,0.45,0.37,0.085,0.405,0.06,0.105,0.07,0.11,0.085,0.155,0.145,0.015,0.205,0.235,0.3,0.31,0.36,greedy,Train for 100 iterations on each of 500 examples. Masking to select only neighbors at each step.,500.0,500.0,8268.759445648,EVALUATIONS/WEIGHTS_BACKUP/1617967379427618754#139802784470848,"ER(20, 0.2943611031936029)",1.0
godot,2021/04/09 11:30:57,"GatedGCN e-p-d on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.3647178649902343,0.0627152099609374,0.0401085398381395,1617967857663362689#140138081015616,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.15,0.035,0.11,0.075,0.075,0.07,0.1,0.135,0.01,0.14,0.12,0.14,0.15,0.14,0.76,0.18,0.75,0.66,0.55,0.51,0.425,0.485,0.41,0.985,0.35,0.3,0.26,0.22,0.155,0.795,0.36,0.805,0.71,0.635,0.655,0.57,0.63,0.6,0.985,0.555,0.55,0.48,0.475,0.425,0.12,0.485,0.08,0.165,0.135,0.185,0.21,0.205,0.225,0.01,0.27,0.32,0.33,0.405,0.425,greedy,Train for 1000x100 examples,1000.0,100000.0,8746.272118291,EVALUATIONS/WEIGHTS_BACKUP/1617967857663362689#140138081015616,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/09 11:44:36,"GatedGCN emb-proc on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",74.53200000000001,14.214000000000002,622.2209359999988,1617968676891749754#139716592785216,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.885,0.745,0.885,0.8,0.735,0.76,0.775,0.79,0.775,0.99,0.79,0.755,0.705,0.795,0.685,0.005,0.0,0.01,0.01,0.0,0.0,0.005,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.005,0.0,0.01,0.01,0.0,0.0,0.005,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.895,0.825,0.9,0.85,0.79,0.835,0.83,0.84,0.835,0.99,0.84,0.835,0.795,0.87,0.815,greedy,Train for 100 iterations on each of 100 examples. Masking to select only neighbors at each step.,100.0,100.0,4869.575543259,EVALUATIONS/WEIGHTS_BACKUP/1617968676891749754#139716592785216,"ER(50, 0.13828607398114504)",1.0
godot,2021/04/09 12:10:52,"GatedGCN e-p-d on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.968649780273437,-0.0162584838867188,0.0127531481520684,1617970252220664310#140221621987136,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.055,0.13,0.08,0.095,0.11,0.16,0.115,0.095,0.14,0.01,0.165,0.155,0.145,0.12,0.095,0.745,0.295,0.735,0.6,0.535,0.505,0.445,0.445,0.47,0.985,0.445,0.375,0.325,0.415,0.34,0.755,0.545,0.745,0.65,0.6,0.585,0.55,0.6,0.565,0.985,0.59,0.53,0.505,0.605,0.565,0.09,0.255,0.09,0.15,0.165,0.225,0.19,0.195,0.2,0.01,0.235,0.27,0.275,0.235,0.21,greedy,Train for 1000x100 examples,1000.0,100000.0,11139.533753143,EVALUATIONS/WEIGHTS_BACKUP/1617970252220664310#140221621987136,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/09 12:26:57,"GatedGCN emb-proc on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",74.668,-16.94,1040.4846959999977,1617971217098663141#140014449153856,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.09,0.175,0.08,0.065,0.065,0.1,0.145,0.14,0.135,0.015,0.125,0.16,0.15,0.195,0.16,0.785,0.31,0.775,0.67,0.565,0.55,0.495,0.55,0.51,0.985,0.54,0.425,0.335,0.365,0.36,0.785,0.32,0.785,0.68,0.59,0.58,0.51,0.555,0.52,0.985,0.55,0.44,0.35,0.375,0.38,0.095,0.5,0.085,0.07,0.1,0.17,0.205,0.26,0.265,0.015,0.25,0.355,0.395,0.435,0.425,greedy,Train for 100 iterations on each of 500 examples. Masking to select only neighbors at each step.,500.0,500.0,10644.884590164,EVALUATIONS/WEIGHTS_BACKUP/1617971217098663141#140014449153856,"ER(25, 0.24532699947978218)",1.0
godot,2021/04/09 13:32:57,"GatedGCN e-p-d on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.9895184936523433,-0.0500937499999999,0.0506154250093722,1617975177714602076#139723358263104,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.065,0.17,0.055,0.1,0.1,0.16,0.12,0.115,0.105,0.01,0.12,0.15,0.14,0.185,0.215,0.735,0.435,0.755,0.65,0.59,0.515,0.56,0.56,0.535,0.985,0.54,0.455,0.445,0.41,0.375,0.755,0.595,0.76,0.69,0.645,0.58,0.61,0.66,0.62,0.985,0.66,0.625,0.58,0.55,0.5,0.065,0.285,0.06,0.145,0.12,0.24,0.165,0.175,0.175,0.01,0.195,0.26,0.265,0.305,0.33,greedy,Train for 1000x100 examples,1000.0,100000.0,16059.875315521002,EVALUATIONS/WEIGHTS_BACKUP/1617975177714602076#139723358263104,35-cycle with expected 1 extra edge per node,1.0
godot,2021/04/09 13:41:52,"GatedGCN emb-proc on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",54.032,-8.338,614.8510959999999,1617975712422709787#139932782204736,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.105,0.1,0.08,0.115,0.09,0.04,0.07,0.115,0.085,0.005,0.09,0.085,0.105,0.125,0.125,0.815,0.485,0.8,0.675,0.63,0.655,0.62,0.615,0.62,0.985,0.605,0.565,0.5,0.54,0.465,0.815,0.495,0.81,0.69,0.655,0.695,0.645,0.635,0.64,0.985,0.61,0.575,0.52,0.57,0.495,0.105,0.31,0.08,0.13,0.125,0.08,0.12,0.18,0.155,0.005,0.21,0.235,0.235,0.28,0.29,greedy,Train for 100 iterations on each of 1000 examples. Masking to select only neighbors at each step.,1000.0,1000.0,16598.069979777,EVALUATIONS/WEIGHTS_BACKUP/1617975712422709787#139932782204736,"ER(20, 0.2943611031936029)",1.0
godot,2021/04/09 14:30:19,"GatedGCN emb-proc on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",44.848,0.7019999999999996,293.5733760000005,1617978619847758168#140343920854848,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.095,0.41,0.095,0.125,0.125,0.145,0.175,0.27,0.26,0.01,0.265,0.31,0.245,0.37,0.37,0.72,0.26,0.685,0.5,0.495,0.46,0.425,0.41,0.415,0.985,0.39,0.305,0.335,0.26,0.3,0.755,0.295,0.695,0.52,0.535,0.5,0.455,0.425,0.455,0.985,0.425,0.355,0.37,0.295,0.34,0.1,0.5,0.095,0.315,0.22,0.275,0.295,0.36,0.365,0.01,0.37,0.44,0.37,0.53,0.445,greedy,Train for 100 iterations on each of 500 examples. Masking to select only neighbors at each step.,500.0,500.0,16805.375708359,EVALUATIONS/WEIGHTS_BACKUP/1617978619847758168#140343920854848,"ER(35, 0.18599238277363112)",1.0
godot,2021/04/09 15:25:34,"GatedGCN emb-proc on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",56.584,-14.688000000000002,1302.6217439999991,1617981934895527196#140681700276032,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.045,0.21,0.025,0.035,0.045,0.06,0.07,0.14,0.17,0.005,0.185,0.175,0.12,0.205,0.205,0.755,0.31,0.765,0.655,0.55,0.53,0.525,0.54,0.44,0.985,0.41,0.37,0.405,0.35,0.325,0.775,0.435,0.765,0.705,0.6,0.63,0.61,0.6,0.53,0.985,0.525,0.49,0.515,0.45,0.43,0.09,0.36,0.045,0.07,0.1,0.1,0.095,0.19,0.225,0.005,0.26,0.255,0.21,0.33,0.33,greedy,Train for 100 iterations on each of 1000 examples. Masking to select only neighbors at each step.,1000.0,1000.0,21279.097046401,EVALUATIONS/WEIGHTS_BACKUP/1617981934895527196#140681700276032,"ER(25, 0.24532699947978218)",1.0
godot,2021/04/09 15:51:56,"GatedGCN e-p-d on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",5.1513388671875004,-0.04160546875,0.0313997679183373,1617983516021269928#140602943448896,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.055,0.05,0.045,0.045,0.045,0.035,0.04,0.005,0.05,0.01,0.045,0.07,0.035,0.04,0.04,0.76,0.455,0.715,0.565,0.535,0.5,0.495,0.555,0.48,0.985,0.57,0.435,0.485,0.395,0.415,0.795,0.665,0.75,0.635,0.62,0.61,0.585,0.69,0.6,0.985,0.72,0.645,0.65,0.655,0.65,0.075,0.13,0.095,0.1,0.06,0.105,0.115,0.095,0.13,0.01,0.105,0.15,0.11,0.14,0.16,greedy,Train for 1000x100 examples,1000.0,100000.0,24392.046720265,EVALUATIONS/WEIGHTS_BACKUP/1617983516021269928#140602943448896,50-cycle with expected 1 extra edge per node,1.0
godot,2021/04/09 16:34:31,"Long train of currently best e-p-d model, 4000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.8080892333984377,0.0002026977539062,0.0195392437087873,1617986071123712822#140009362286400,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.065,0.04,0.05,0.025,0.065,0.05,0.095,0.065,0.03,0.01,0.04,0.06,0.055,0.05,0.065,0.7,0.465,0.765,0.665,0.57,0.605,0.495,0.575,0.56,0.885,0.6,0.46,0.455,0.52,0.46,0.735,0.67,0.77,0.745,0.625,0.66,0.62,0.715,0.69,0.885,0.745,0.66,0.645,0.67,0.595,0.07,0.22,0.06,0.065,0.11,0.115,0.165,0.115,0.075,0.01,0.1,0.165,0.17,0.16,0.225,greedy,Train for 4000x100 examples,4000.0,400000.0,26950.88685803,EVALUATIONS/WEIGHTS_BACKUP/1617986071123712822#140009362286400,25-cycle with expected 1 extra edge per node,1.0
godot,2021/04/09 17:55:32,"GatedGCN emb-proc on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",88.848,-4.8320000000000025,593.1856560000015,1617990932407781374#140659467728704,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.59,0.74,0.725,0.69,0.65,0.715,0.71,0.74,0.715,0.2,0.735,0.715,0.685,0.755,0.69,0.3,0.0,0.105,0.06,0.03,0.025,0.015,0.015,0.01,0.795,0.01,0.0,0.005,0.0,0.005,0.3,0.0,0.105,0.06,0.03,0.025,0.015,0.015,0.01,0.795,0.01,0.0,0.005,0.0,0.005,0.59,0.84,0.74,0.73,0.715,0.775,0.79,0.775,0.805,0.2,0.825,0.815,0.78,0.835,0.825,greedy,Train for 100 iterations on each of 500 examples. Masking to select only neighbors at each step.,500.0,500.0,26644.911317456,EVALUATIONS/WEIGHTS_BACKUP/1617990932407781374#140659467728704,"ER(50, 0.13828607398114504)",1.0
godot,2021/04/09 19:08:31,"GatedGCN emb-proc on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",102.032,-1.0840000000000003,1912.8740559999987,1617995311461100209#140546538112832,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.025,0.015,0.005,0.02,0.02,0.025,0.015,0.03,0.005,0.0,0.04,0.015,0.01,0.015,0.01,0.8,0.445,0.795,0.665,0.59,0.595,0.56,0.565,0.595,0.985,0.585,0.505,0.46,0.51,0.45,0.8,0.495,0.805,0.685,0.625,0.645,0.615,0.62,0.645,0.985,0.615,0.58,0.5,0.55,0.48,0.085,0.305,0.06,0.1,0.12,0.125,0.135,0.19,0.15,0.0,0.185,0.185,0.23,0.225,0.29,greedy,Train for 100 iterations on each of 1000 examples. Masking to select only neighbors at each step.,1000.0,1000.0,32681.956306437,EVALUATIONS/WEIGHTS_BACKUP/1617995311461100209#140546538112832,"ER(35, 0.18599238277363112)",1.0
godot,2021/04/10 01:41:40,"GatedGCN emb-proc on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",71.32,-17.814,721.436920000001,1618018900379531527#139757890869056,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.01,0.44,0.04,0.065,0.14,0.125,0.18,0.22,0.185,0.0,0.245,0.325,0.345,0.39,0.45,0.425,0.235,0.505,0.415,0.305,0.37,0.27,0.35,0.355,0.785,0.345,0.25,0.195,0.215,0.185,0.66,0.31,0.69,0.675,0.53,0.595,0.45,0.53,0.475,0.785,0.495,0.33,0.345,0.325,0.275,0.035,0.585,0.06,0.095,0.215,0.21,0.26,0.315,0.32,0.0,0.335,0.5,0.445,0.53,0.595,greedy,Train for 100 iterations on each of 1000 examples. Masking to select only neighbors at each step.,1000.0,1000.0,53792.00594315,EVALUATIONS/WEIGHTS_BACKUP/1618018900379531527#139757890869056,"ER(50, 0.13828607398114504)",1.0
godot,2021/04/12 11:21:57,"GatedGCN emb-proc on diff. train sizes, learn rates, train lengths","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",52.54599999999999,6.779999999999999,337.396224000001,1618226517777899103#139686529406784,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.15,0.505,0.185,0.19,0.26,0.265,0.3,0.415,0.345,0.005,0.41,0.38,0.39,0.485,0.445,0.725,0.175,0.68,0.575,0.435,0.4,0.355,0.35,0.34,0.985,0.28,0.28,0.24,0.175,0.19,0.725,0.175,0.68,0.575,0.435,0.405,0.36,0.355,0.345,0.985,0.285,0.29,0.24,0.18,0.215,0.15,0.55,0.185,0.195,0.28,0.28,0.31,0.425,0.38,0.005,0.45,0.43,0.42,0.53,0.49,greedy,Train for 100 iterations on each of 100 examples. Masking to select only neighbors at each step.,100.0,100.0,4199.078087398,EVALUATIONS/WEIGHTS_BACKUP/1618226517777899103#139686529406784,"ER(20, 0.2943611031936029)",1.0
godot,2021/04/12 18:35:50,Attention e-p-d test,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.803978576660156,0.007644287109375,0.0073559805878264,1618252550852326351#140225377412928,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (node hidden dim=32, edge attributes dim=16): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─AttentionMPNNWithEdgeFeatures: 2-1     --
|    |    └─Sequential: 3-1                   --
|    |    |    └─Linear: 4-1                  2,592
|    |    |    └─ReLU: 4-2                    --
|    |    |    └─Dropout: 4-3                 --
|    |    |    └─Linear: 4-4                  1,056
|    |    |    └─ReLU: 4-5                    --
|    |    |    └─Dropout: 4-6                 --
|    |    |    └─Linear: 4-7                  1,056
|    |    └─Sequential: 3-2                   --
|    |    |    └─Linear: 4-8                  1,296
|    |    |    └─ReLU: 4-9                    --
|    |    |    └─Dropout: 4-10                --
|    |    |    └─Linear: 4-11                 272
|    |    |    └─ReLU: 4-12                   --
|    |    |    └─Dropout: 4-13                --
|    |    |    └─Linear: 4-14                 272
|    |    └─Linear: 3-3                       81
|    └─AttentionMPNNWithEdgeFeatures: 2-2     --
|    |    └─Sequential: 3-4                   --
|    |    |    └─Linear: 4-15                 2,592
|    |    |    └─ReLU: 4-16                   --
|    |    |    └─Dropout: 4-17                --
|    |    |    └─Linear: 4-18                 1,056
|    |    |    └─ReLU: 4-19                   --
|    |    |    └─Dropout: 4-20                --
|    |    |    └─Linear: 4-21                 1,056
|    |    └─Sequential: 3-5                   --
|    |    |    └─Linear: 4-22                 1,296
|    |    |    └─ReLU: 4-23                   --
|    |    |    └─Dropout: 4-24                --
|    |    |    └─Linear: 4-25                 272
|    |    |    └─ReLU: 4-26                   --
|    |    |    └─Dropout: 4-27                --
|    |    |    └─Linear: 4-28                 272
|    |    └─Linear: 3-6                       81
======================================================================
Total params: 13,250
Trainable params: 13,250
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.045,0.0,0.035,0.06,0.02,0.035,0.015,0.015,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.795,0.0,0.71,0.6,0.55,0.49,0.25,0.115,0.035,0.985,0.005,0.0,0.0,0.0,0.0,0.805,0.0,0.775,0.685,0.62,0.595,0.485,0.39,0.135,0.985,0.04,0.0,0.0,0.0,0.0,0.06,0.0,0.055,0.12,0.055,0.08,0.06,0.095,0.02,0.0,0.06,0.0,0.0,0.0,0.0,greedy,Train for 500x100 examples,500.0,50000.0,10896.319275082,EVALUATIONS/WEIGHTS_BACKUP/1618252550852326351#140225377412928,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/13 23:31:27,GatedGCN emb-proc on diff. that broke down,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",113.37,-19.432,2188.69384,1618356687281375581#139674940213056,100.0,0.0001,"Reinforcement loss used in AlphaZero paper. loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(b(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and b(S_t) is the estimated value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.175,0.4,0.125,0.14,0.155,0.165,0.185,0.2,0.195,0.015,0.25,0.265,0.265,0.305,0.38,0.75,0.275,0.705,0.6,0.51,0.52,0.5,0.52,0.49,0.985,0.46,0.4,0.385,0.355,0.27,0.755,0.31,0.72,0.63,0.555,0.55,0.535,0.535,0.52,0.985,0.485,0.435,0.405,0.385,0.295,0.175,0.545,0.13,0.2,0.19,0.235,0.27,0.255,0.29,0.015,0.33,0.395,0.38,0.435,0.555,greedy,Train for 100 iterations on each of 1000 examples. Masking to select only neighbors at each step.,1000.0,1000.0,123064.643812608,EVALUATIONS/WEIGHTS_BACKUP/1618356687281375581#139674940213056,"ER(50, 0.13828607398114504)",1.0
godot,2021/04/15 22:01:29,Attention e-p-d test,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.9084302978515626,0.064609375,0.0262674558121922,1618524089599564538#140565001799488,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (node hidden dim=32, edge attributes dim=16): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─AttentionMPNNWithEdgeFeatures: 2-1     --
|    |    └─Sequential: 3-1                   --
|    |    |    └─Linear: 4-1                  2,592
|    |    |    └─ReLU: 4-2                    --
|    |    |    └─Dropout: 4-3                 --
|    |    |    └─Linear: 4-4                  1,056
|    |    |    └─ReLU: 4-5                    --
|    |    |    └─Dropout: 4-6                 --
|    |    |    └─Linear: 4-7                  1,056
|    |    └─Sequential: 3-2                   --
|    |    |    └─Linear: 4-8                  1,296
|    |    |    └─ReLU: 4-9                    --
|    |    |    └─Dropout: 4-10                --
|    |    |    └─Linear: 4-11                 272
|    |    |    └─ReLU: 4-12                   --
|    |    |    └─Dropout: 4-13                --
|    |    |    └─Linear: 4-14                 272
|    |    └─Linear: 3-3                       81
|    └─AttentionMPNNWithEdgeFeatures: 2-2     --
|    |    └─Sequential: 3-4                   --
|    |    |    └─Linear: 4-15                 2,592
|    |    |    └─ReLU: 4-16                   --
|    |    |    └─Dropout: 4-17                --
|    |    |    └─Linear: 4-18                 1,056
|    |    |    └─ReLU: 4-19                   --
|    |    |    └─Dropout: 4-20                --
|    |    |    └─Linear: 4-21                 1,056
|    |    └─Sequential: 3-5                   --
|    |    |    └─Linear: 4-22                 1,296
|    |    |    └─ReLU: 4-23                   --
|    |    |    └─Dropout: 4-24                --
|    |    |    └─Linear: 4-25                 272
|    |    |    └─ReLU: 4-26                   --
|    |    |    └─Dropout: 4-27                --
|    |    |    └─Linear: 4-28                 272
|    |    └─Linear: 3-6                       81
======================================================================
Total params: 13,250
Trainable params: 13,250
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.15,0.0,0.1,0.16,0.205,0.225,0.26,0.27,0.145,0.015,0.065,0.0,0.0,0.0,0.0,0.755,0.0,0.74,0.605,0.5,0.355,0.2,0.125,0.03,0.98,0.01,0.0,0.0,0.0,0.0,0.785,0.0,0.77,0.62,0.52,0.42,0.305,0.22,0.07,0.98,0.04,0.0,0.0,0.0,0.0,0.155,0.0,0.125,0.2,0.22,0.26,0.35,0.415,0.32,0.015,0.275,0.105,0.01,0.0,0.0,greedy,Train for 500x100 examples,500.0,50000.0,11945.417777658,EVALUATIONS/WEIGHTS_BACKUP/1618524089599564538#140565001799488,20-cycle with expected 1 extra edge per node,1.0
godot,2021/04/16 12:59:09,Attention e-p-d test,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",1.6216714477539065,0.0224956665039062,0.0045524620035375,1618577949473785994#140391620368192,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (node hidden dim=32, edge attributes dim=16): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─AttentionMPNNWithEdgeFeatures: 2-1     --
|    |    └─Sequential: 3-1                   --
|    |    |    └─Linear: 4-1                  2,592
|    |    |    └─ReLU: 4-2                    --
|    |    |    └─Dropout: 4-3                 --
|    |    |    └─Linear: 4-4                  1,056
|    |    |    └─ReLU: 4-5                    --
|    |    |    └─Dropout: 4-6                 --
|    |    |    └─Linear: 4-7                  1,056
|    |    └─Sequential: 3-2                   --
|    |    |    └─Linear: 4-8                  1,296
|    |    |    └─ReLU: 4-9                    --
|    |    |    └─Dropout: 4-10                --
|    |    |    └─Linear: 4-11                 272
|    |    |    └─ReLU: 4-12                   --
|    |    |    └─Dropout: 4-13                --
|    |    |    └─Linear: 4-14                 272
|    |    └─Linear: 3-3                       81
|    └─AttentionMPNNWithEdgeFeatures: 2-2     --
|    |    └─Sequential: 3-4                   --
|    |    |    └─Linear: 4-15                 2,592
|    |    |    └─ReLU: 4-16                   --
|    |    |    └─Dropout: 4-17                --
|    |    |    └─Linear: 4-18                 1,056
|    |    |    └─ReLU: 4-19                   --
|    |    |    └─Dropout: 4-20                --
|    |    |    └─Linear: 4-21                 1,056
|    |    └─Sequential: 3-5                   --
|    |    |    └─Linear: 4-22                 1,296
|    |    |    └─ReLU: 4-23                   --
|    |    |    └─Dropout: 4-24                --
|    |    |    └─Linear: 4-25                 272
|    |    |    └─ReLU: 4-26                   --
|    |    |    └─Dropout: 4-27                --
|    |    |    └─Linear: 4-28                 272
|    |    └─Linear: 3-6                       81
======================================================================
Total params: 13,250
Trainable params: 13,250
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.095,0.0,0.09,0.09,0.04,0.005,0.005,0.015,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.775,0.0,0.705,0.38,0.205,0.075,0.015,0.01,0.005,0.98,0.0,0.0,0.0,0.0,0.0,0.8,0.0,0.725,0.49,0.295,0.215,0.08,0.07,0.02,0.98,0.02,0.0,0.0,0.0,0.0,0.115,0.0,0.115,0.095,0.065,0.015,0.035,0.04,0.015,0.02,0.02,0.0,0.0,0.0,0.0,greedy,Train for 500x100 examples,500.0,50000.0,1150.922482749,EVALUATIONS/WEIGHTS_BACKUP/1618577949473785994#140391620368192,10-cycle with expected 1 extra edge per node,
godot,2021/04/16 13:18:49,Attention e-p-d test,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",1.460364776611328,-0.0208403015136718,0.0081766355171759,1618579129670602002#140023334385472,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (node hidden dim=32, edge attributes dim=16): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─AttentionMPNNWithEdgeFeatures: 2-1     --
|    |    └─Sequential: 3-1                   --
|    |    |    └─Linear: 4-1                  2,592
|    |    |    └─ReLU: 4-2                    --
|    |    |    └─Dropout: 4-3                 --
|    |    |    └─Linear: 4-4                  1,056
|    |    |    └─ReLU: 4-5                    --
|    |    |    └─Dropout: 4-6                 --
|    |    |    └─Linear: 4-7                  1,056
|    |    └─Sequential: 3-2                   --
|    |    |    └─Linear: 4-8                  1,296
|    |    |    └─ReLU: 4-9                    --
|    |    |    └─Dropout: 4-10                --
|    |    |    └─Linear: 4-11                 272
|    |    |    └─ReLU: 4-12                   --
|    |    |    └─Dropout: 4-13                --
|    |    |    └─Linear: 4-14                 272
|    |    └─Linear: 3-3                       81
|    └─AttentionMPNNWithEdgeFeatures: 2-2     --
|    |    └─Sequential: 3-4                   --
|    |    |    └─Linear: 4-15                 2,592
|    |    |    └─ReLU: 4-16                   --
|    |    |    └─Dropout: 4-17                --
|    |    |    └─Linear: 4-18                 1,056
|    |    |    └─ReLU: 4-19                   --
|    |    |    └─Dropout: 4-20                --
|    |    |    └─Linear: 4-21                 1,056
|    |    └─Sequential: 3-5                   --
|    |    |    └─Linear: 4-22                 1,296
|    |    |    └─ReLU: 4-23                   --
|    |    |    └─Dropout: 4-24                --
|    |    |    └─Linear: 4-25                 272
|    |    |    └─ReLU: 4-26                   --
|    |    |    └─Dropout: 4-27                --
|    |    |    └─Linear: 4-28                 272
|    |    └─Linear: 3-6                       81
======================================================================
Total params: 13,250
Trainable params: 13,250
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.085,0.0,0.085,0.09,0.025,0.01,0.0,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.795,0.0,0.49,0.08,0.005,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.81,0.0,0.76,0.315,0.055,0.0,0.0,0.0,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.095,0.0,0.11,0.13,0.09,0.06,0.005,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,greedy,Train for 1000x100 examples,1000.0,100000.0,2330.502204585,EVALUATIONS/WEIGHTS_BACKUP/1618579129670602002#140023334385472,10-cycle with expected 1 extra edge per node,
godot,2021/04/16 13:19:56,Attention e-p-d test,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.965548767089844,0.03319091796875,0.0175324812222044,1618579196634938926#139970957576000,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (node hidden dim=32, edge attributes dim=16): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─AttentionMPNNWithEdgeFeatures: 2-1     --
|    |    └─Sequential: 3-1                   --
|    |    |    └─Linear: 4-1                  2,592
|    |    |    └─ReLU: 4-2                    --
|    |    |    └─Dropout: 4-3                 --
|    |    |    └─Linear: 4-4                  1,056
|    |    |    └─ReLU: 4-5                    --
|    |    |    └─Dropout: 4-6                 --
|    |    |    └─Linear: 4-7                  1,056
|    |    └─Sequential: 3-2                   --
|    |    |    └─Linear: 4-8                  1,296
|    |    |    └─ReLU: 4-9                    --
|    |    |    └─Dropout: 4-10                --
|    |    |    └─Linear: 4-11                 272
|    |    |    └─ReLU: 4-12                   --
|    |    |    └─Dropout: 4-13                --
|    |    |    └─Linear: 4-14                 272
|    |    └─Linear: 3-3                       81
|    └─AttentionMPNNWithEdgeFeatures: 2-2     --
|    |    └─Sequential: 3-4                   --
|    |    |    └─Linear: 4-15                 2,592
|    |    |    └─ReLU: 4-16                   --
|    |    |    └─Dropout: 4-17                --
|    |    |    └─Linear: 4-18                 1,056
|    |    |    └─ReLU: 4-19                   --
|    |    |    └─Dropout: 4-20                --
|    |    |    └─Linear: 4-21                 1,056
|    |    └─Sequential: 3-5                   --
|    |    |    └─Linear: 4-22                 1,296
|    |    |    └─ReLU: 4-23                   --
|    |    |    └─Dropout: 4-24                --
|    |    |    └─Linear: 4-25                 272
|    |    |    └─ReLU: 4-26                   --
|    |    |    └─Dropout: 4-27                --
|    |    |    └─Linear: 4-28                 272
|    |    └─Linear: 3-6                       81
======================================================================
Total params: 13,250
Trainable params: 13,250
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.05,0.0,0.07,0.075,0.085,0.13,0.15,0.24,0.425,0.015,0.285,0.0,0.0,0.0,0.0,0.705,0.0,0.595,0.505,0.52,0.44,0.36,0.225,0.12,0.975,0.02,0.0,0.0,0.0,0.0,0.805,0.0,0.78,0.655,0.63,0.555,0.46,0.35,0.175,0.975,0.07,0.005,0.0,0.0,0.0,0.08,0.0,0.08,0.105,0.1,0.17,0.195,0.29,0.51,0.015,0.615,0.325,0.0,0.0,0.0,greedy,Train for 500x100 examples,500.0,50000.0,2396.604463338,EVALUATIONS/WEIGHTS_BACKUP/1618579196634938926#139970957576000,20-cycle with expected 1 extra edge per node,
godot,2021/04/16 14:00:42,Attention e-p-d test,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.691194641113281,0.0214672241210937,0.0095305517356676,1618581642017149314#139712832030528,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (node hidden dim=32, edge attributes dim=16): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─AttentionMPNNWithEdgeFeatures: 2-1     --
|    |    └─Sequential: 3-1                   --
|    |    |    └─Linear: 4-1                  2,592
|    |    |    └─ReLU: 4-2                    --
|    |    |    └─Dropout: 4-3                 --
|    |    |    └─Linear: 4-4                  1,056
|    |    |    └─ReLU: 4-5                    --
|    |    |    └─Dropout: 4-6                 --
|    |    |    └─Linear: 4-7                  1,056
|    |    └─Sequential: 3-2                   --
|    |    |    └─Linear: 4-8                  1,296
|    |    |    └─ReLU: 4-9                    --
|    |    |    └─Dropout: 4-10                --
|    |    |    └─Linear: 4-11                 272
|    |    |    └─ReLU: 4-12                   --
|    |    |    └─Dropout: 4-13                --
|    |    |    └─Linear: 4-14                 272
|    |    └─Linear: 3-3                       81
|    └─AttentionMPNNWithEdgeFeatures: 2-2     --
|    |    └─Sequential: 3-4                   --
|    |    |    └─Linear: 4-15                 2,592
|    |    |    └─ReLU: 4-16                   --
|    |    |    └─Dropout: 4-17                --
|    |    |    └─Linear: 4-18                 1,056
|    |    |    └─ReLU: 4-19                   --
|    |    |    └─Dropout: 4-20                --
|    |    |    └─Linear: 4-21                 1,056
|    |    └─Sequential: 3-5                   --
|    |    |    └─Linear: 4-22                 1,296
|    |    |    └─ReLU: 4-23                   --
|    |    |    └─Dropout: 4-24                --
|    |    |    └─Linear: 4-25                 272
|    |    |    └─ReLU: 4-26                   --
|    |    |    └─Dropout: 4-27                --
|    |    |    └─Linear: 4-28                 272
|    |    └─Linear: 3-6                       81
======================================================================
Total params: 13,250
Trainable params: 13,250
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.095,0.0,0.06,0.065,0.05,0.065,0.08,0.075,0.08,0.01,0.095,0.19,0.025,0.0,0.0,0.745,0.0,0.73,0.585,0.545,0.475,0.415,0.4,0.365,0.98,0.3,0.05,0.0,0.0,0.0,0.78,0.0,0.79,0.675,0.625,0.63,0.56,0.59,0.565,0.98,0.46,0.165,0.025,0.0,0.0,0.135,0.0,0.07,0.1,0.07,0.12,0.125,0.13,0.115,0.01,0.13,0.39,0.255,0.02,0.0,greedy,Train for 1000x100 examples,1000.0,100000.0,4840.280036127,EVALUATIONS/WEIGHTS_BACKUP/1618581642017149314#139712832030528,20-cycle with expected 1 extra edge per node,
godot,2021/04/16 17:30:28,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",5.026504150390625,0.0715066528320312,0.0022862715595337,1618594228913710030#140417594304320,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.19,0.17,0.225,0.235,0.315,0.22,0.215,0.24,0.245,0.02,0.19,0.19,0.165,0.165,0.135,0.455,0.075,0.4,0.28,0.155,0.22,0.175,0.19,0.17,0.9,0.16,0.105,0.1,0.115,0.175,0.555,0.355,0.465,0.43,0.315,0.445,0.295,0.41,0.365,0.9,0.41,0.31,0.35,0.405,0.435,0.24,0.51,0.275,0.32,0.43,0.375,0.415,0.395,0.445,0.02,0.4,0.48,0.505,0.42,0.4,greedy,Train for 100x100 batches of 2 examples,100.0,10000.0,343.01254999900004,EVALUATIONS/WEIGHTS_BACKUP/1618594228913710030#140417594304320,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 17:32:23,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.813699096679688,0.0216821289062501,0.0092159079156175,1618594343641087716#140538529281856,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.125,0.15,0.23,0.325,0.295,0.3,0.335,0.265,0.275,0.05,0.32,0.235,0.205,0.195,0.185,0.62,0.025,0.525,0.375,0.355,0.31,0.255,0.325,0.22,0.865,0.2,0.14,0.105,0.08,0.055,0.675,0.23,0.585,0.435,0.42,0.455,0.4,0.48,0.395,0.865,0.34,0.34,0.285,0.28,0.295,0.15,0.575,0.25,0.37,0.38,0.39,0.43,0.405,0.44,0.05,0.515,0.465,0.54,0.525,0.495,greedy,Train for 100x100 batches of 4 examples,100.0,10000.0,457.631534767,EVALUATIONS/WEIGHTS_BACKUP/1618594343641087716#140538529281856,Batch of 4 25-cycles with expected 1,
godot,2021/04/16 17:35:59,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.751229248046876,-0.0092700805664062,0.0090121363092485,1618594559753012676#139947251357504,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.26,0.065,0.17,0.22,0.185,0.165,0.155,0.245,0.225,0.665,0.135,0.135,0.125,0.125,0.13,0.425,0.03,0.455,0.345,0.29,0.28,0.26,0.165,0.155,0.28,0.125,0.07,0.07,0.06,0.05,0.485,0.235,0.505,0.445,0.385,0.455,0.4,0.32,0.3,0.28,0.32,0.285,0.235,0.26,0.205,0.33,0.44,0.225,0.28,0.285,0.275,0.3,0.405,0.46,0.665,0.37,0.41,0.435,0.44,0.465,greedy,Train for 100x100 batches of 8 examples,100.0,10000.0,673.335762828,EVALUATIONS/WEIGHTS_BACKUP/1618594559753012676#139947251357504,Batch of 8 25-cycles with expected 1,
godot,2021/04/16 17:38:16,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.5435246582031255,-0.0623978271484373,0.0412479549070639,1618594696485325247#140694845663040,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.15,0.26,0.205,0.205,0.2,0.235,0.27,0.215,0.27,0.01,0.25,0.275,0.18,0.255,0.225,0.475,0.155,0.405,0.295,0.265,0.275,0.21,0.24,0.245,0.9,0.23,0.145,0.175,0.17,0.175,0.615,0.36,0.46,0.43,0.39,0.41,0.36,0.415,0.365,0.9,0.43,0.37,0.455,0.375,0.355,0.18,0.53,0.25,0.29,0.28,0.36,0.395,0.395,0.405,0.01,0.39,0.435,0.4,0.475,0.485,greedy,Train for 100x100 batches of 2 examples,100.0,10000.0,810.30541607,EVALUATIONS/WEIGHTS_BACKUP/1618594696485325247#140694845663040,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 17:43:37,Attention e-p-d test,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",1.5949496459960937,0.0212958984375,0.0060830736880093,1618595017338081077#140124883535680,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (node hidden dim=32, edge attributes dim=16): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─AttentionMPNNWithEdgeFeatures: 2-1     --
|    |    └─Sequential: 3-1                   --
|    |    |    └─Linear: 4-1                  2,592
|    |    |    └─ReLU: 4-2                    --
|    |    |    └─Dropout: 4-3                 --
|    |    |    └─Linear: 4-4                  1,056
|    |    |    └─ReLU: 4-5                    --
|    |    |    └─Dropout: 4-6                 --
|    |    |    └─Linear: 4-7                  1,056
|    |    └─Sequential: 3-2                   --
|    |    |    └─Linear: 4-8                  1,296
|    |    |    └─ReLU: 4-9                    --
|    |    |    └─Dropout: 4-10                --
|    |    |    └─Linear: 4-11                 272
|    |    |    └─ReLU: 4-12                   --
|    |    |    └─Dropout: 4-13                --
|    |    |    └─Linear: 4-14                 272
|    |    └─Linear: 3-3                       81
|    └─AttentionMPNNWithEdgeFeatures: 2-2     --
|    |    └─Sequential: 3-4                   --
|    |    |    └─Linear: 4-15                 2,592
|    |    |    └─ReLU: 4-16                   --
|    |    |    └─Dropout: 4-17                --
|    |    |    └─Linear: 4-18                 1,056
|    |    |    └─ReLU: 4-19                   --
|    |    |    └─Dropout: 4-20                --
|    |    |    └─Linear: 4-21                 1,056
|    |    └─Sequential: 3-5                   --
|    |    |    └─Linear: 4-22                 1,296
|    |    |    └─ReLU: 4-23                   --
|    |    |    └─Dropout: 4-24                --
|    |    |    └─Linear: 4-25                 272
|    |    |    └─ReLU: 4-26                   --
|    |    |    └─Dropout: 4-27                --
|    |    |    └─Linear: 4-28                 272
|    |    └─Linear: 3-6                       81
======================================================================
Total params: 13,250
Trainable params: 13,250
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.0,0.065,0.12,0.14,0.06,0.005,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.76,0.0,0.71,0.27,0.02,0.005,0.0,0.0,0.0,0.965,0.0,0.0,0.0,0.0,0.0,0.785,0.0,0.775,0.595,0.125,0.045,0.005,0.0,0.0,0.965,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.075,0.145,0.32,0.25,0.055,0.03,0.005,0.02,0.0,0.0,0.0,0.0,0.0,greedy,Train for 500x100 examples,500.0,50000.0,1130.978942159,EVALUATIONS/WEIGHTS_BACKUP/1618595017338081077#140124883535680,10-cycle with expected 1 extra edge per node,
godot,2021/04/16 17:42:48,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.346335571289063,0.0157399902343749,0.0103914101977125,1618594968914143430#139822042982208,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.16,0.22,0.24,0.235,0.28,0.215,0.28,0.235,0.225,0.015,0.205,0.225,0.16,0.155,0.17,0.565,0.15,0.52,0.38,0.335,0.27,0.21,0.22,0.225,0.915,0.255,0.135,0.18,0.2,0.165,0.635,0.32,0.545,0.49,0.455,0.46,0.39,0.38,0.4,0.915,0.44,0.35,0.365,0.43,0.37,0.18,0.495,0.255,0.315,0.33,0.335,0.39,0.42,0.36,0.015,0.37,0.475,0.385,0.38,0.38,greedy,Train for 100x100 batches of 4 examples,100.0,10000.0,1082.310601109,EVALUATIONS/WEIGHTS_BACKUP/1618594968914143430#139822042982208,Batch of 4 25-cycles with expected 1,
godot,2021/04/16 17:43:14,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.530913940429687,-0.0157031249999999,0.015526226578153,1618594994531016621#140228623398720,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-45            2,080
|    |    |    └─ReLU: 4-46              --
=================================================================
Total params: 42,016
Trainable params: 42,016
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.215,0.09,0.185,0.135,0.12,0.145,0.125,0.08,0.11,0.02,0.065,0.115,0.1,0.035,0.045,0.5,0.1,0.485,0.405,0.35,0.265,0.22,0.26,0.23,0.95,0.22,0.155,0.16,0.15,0.12,0.55,0.46,0.51,0.55,0.52,0.5,0.405,0.525,0.51,0.95,0.52,0.415,0.46,0.5,0.485,0.25,0.32,0.205,0.2,0.15,0.2,0.235,0.25,0.22,0.02,0.22,0.3,0.295,0.25,0.275,greedy,Train for 100x100 batches of 2 examples,100.0,10000.0,1108.127299347,EVALUATIONS/WEIGHTS_BACKUP/1618594994531016621#140228623398720,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 17:53:17,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.8315159301757817,0.0066840820312499,0.0049777337617999,1618595597267053004#139995215787840,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,0.105,0.11,0.165,0.175,0.135,0.12,0.13,0.135,0.015,0.1,0.105,0.08,0.085,0.07,0.575,0.25,0.625,0.435,0.44,0.38,0.37,0.37,0.375,0.925,0.335,0.315,0.295,0.29,0.285,0.64,0.585,0.685,0.565,0.52,0.52,0.545,0.585,0.575,0.925,0.55,0.565,0.58,0.54,0.57,0.14,0.225,0.13,0.23,0.235,0.22,0.195,0.185,0.2,0.015,0.205,0.2,0.22,0.21,0.195,greedy,Train for 500x100 batches of 2 examples,500.0,50000.0,1710.565534153,EVALUATIONS/WEIGHTS_BACKUP/1618595597267053004#139995215787840,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 17:52:08,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.030528747558593,-0.0402319946289061,0.004311970379824,1618595528983653149#140167541208896,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,0.19,0.155,0.255,0.21,0.17,0.215,0.215,0.215,0.015,0.185,0.215,0.215,0.21,0.19,0.54,0.345,0.54,0.415,0.42,0.39,0.35,0.35,0.34,0.875,0.44,0.275,0.26,0.335,0.275,0.625,0.59,0.585,0.55,0.55,0.55,0.485,0.465,0.49,0.875,0.585,0.52,0.53,0.51,0.49,0.13,0.335,0.195,0.285,0.24,0.25,0.305,0.37,0.32,0.015,0.28,0.365,0.38,0.385,0.42,greedy,Train for 100x100 batches of 8 examples,100.0,10000.0,1641.864872825,EVALUATIONS/WEIGHTS_BACKUP/1618595528983653149#140167541208896,Batch of 8 25-cycles with expected 1,
godot,2021/04/16 17:49:44,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.344310729980469,-0.0131020507812499,0.0128960008341252,1618595384271343227#140094671243072,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-45            2,080
|    |    |    └─ReLU: 4-46              --
=================================================================
Total params: 42,016
Trainable params: 42,016
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.17,0.2,0.175,0.25,0.275,0.185,0.21,0.235,0.195,0.015,0.305,0.18,0.175,0.165,0.16,0.585,0.175,0.495,0.405,0.32,0.37,0.28,0.29,0.3,0.91,0.25,0.205,0.2,0.235,0.145,0.645,0.42,0.54,0.49,0.42,0.555,0.455,0.445,0.405,0.91,0.385,0.49,0.415,0.5,0.4,0.21,0.405,0.195,0.295,0.33,0.26,0.3,0.335,0.325,0.015,0.425,0.3,0.34,0.335,0.375,greedy,Train for 100x100 batches of 4 examples,100.0,10000.0,1497.324323758,EVALUATIONS/WEIGHTS_BACKUP/1618595384271343227#140094671243072,Batch of 4 25-cycles with expected 1,
godot,2021/04/16 18:22:28,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.8349219360351565,0.0198528442382812,0.0209212035329038,1618597348865922810#140502190397248,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.13,0.23,0.21,0.3,0.25,0.235,0.285,0.29,0.285,0.01,0.235,0.3,0.245,0.29,0.225,0.505,0.1,0.42,0.31,0.27,0.245,0.225,0.2,0.24,0.9,0.2,0.17,0.175,0.105,0.075,0.615,0.25,0.475,0.405,0.365,0.365,0.31,0.35,0.385,0.9,0.4,0.305,0.335,0.23,0.23,0.155,0.62,0.285,0.395,0.37,0.365,0.445,0.5,0.465,0.01,0.445,0.535,0.485,0.62,0.59,greedy,Train for 100x100 batches of 2 examples,100.0,10000.0,333.53125556500004,EVALUATIONS/WEIGHTS_BACKUP/1618597348865922810#140502190397248,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 18:24:26,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.691363464355469,0.0279376831054687,0.0064776893359592,1618597466196339081#140157421860672,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.185,0.195,0.23,0.255,0.235,0.195,0.225,0.255,0.225,0.005,0.195,0.195,0.175,0.14,0.125,0.515,0.16,0.385,0.295,0.295,0.26,0.285,0.325,0.3,0.935,0.27,0.24,0.21,0.2,0.18,0.625,0.47,0.505,0.445,0.425,0.47,0.435,0.48,0.485,0.935,0.51,0.48,0.395,0.57,0.54,0.205,0.41,0.28,0.315,0.325,0.315,0.33,0.365,0.315,0.005,0.315,0.37,0.455,0.275,0.32,greedy,Train for 100x100 batches of 4 examples,100.0,10000.0,450.757035826,EVALUATIONS/WEIGHTS_BACKUP/1618597466196339081#140157421860672,Batch of 4 25-cycles with expected 1,
godot,2021/04/16 18:28:09,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.522826232910155,-0.020212646484375,0.0037456568756049,1618597689110676656#140539853567808,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.385,0.14,0.235,0.335,0.255,0.225,0.245,0.21,0.22,0.13,0.275,0.265,0.29,0.27,0.21,0.31,0.03,0.375,0.215,0.2,0.195,0.225,0.255,0.28,0.785,0.22,0.165,0.085,0.045,0.035,0.405,0.085,0.495,0.38,0.335,0.335,0.345,0.385,0.39,0.785,0.345,0.26,0.21,0.115,0.105,0.445,0.515,0.295,0.405,0.37,0.38,0.405,0.365,0.335,0.13,0.38,0.43,0.42,0.565,0.5,greedy,Train for 100x100 batches of 8 examples,100.0,10000.0,673.334444362,EVALUATIONS/WEIGHTS_BACKUP/1618597689110676656#140539853567808,Batch of 8 25-cycles with expected 1,
godot,2021/04/16 19:27:37,"2-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.3745108642578123,0.0487336425781249,0.0071115851160286,1618601257849372637#140224752113472,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.115,0.195,0.15,0.18,0.21,0.18,0.19,0.205,0.235,0.01,0.175,0.19,0.21,0.195,0.25,0.63,0.32,0.605,0.53,0.485,0.47,0.415,0.455,0.415,0.925,0.51,0.375,0.32,0.39,0.3,0.675,0.525,0.64,0.585,0.52,0.545,0.5,0.555,0.515,0.925,0.625,0.52,0.485,0.575,0.435,0.125,0.295,0.155,0.205,0.255,0.22,0.22,0.245,0.285,0.01,0.235,0.26,0.315,0.25,0.365,greedy,Train for 500x100 batches of 2 examples,500.0,50000.0,3234.675418146,EVALUATIONS/WEIGHTS_BACKUP/1618601257849372637#140224752113472,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 19:27:39,"2-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.500250915527344,0.0559901123046874,0.0117294956121742,1618601259173182361#139846054152000,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.125,0.165,0.14,0.19,0.225,0.22,0.19,0.23,0.24,0.01,0.2,0.195,0.21,0.23,0.205,0.61,0.385,0.675,0.54,0.425,0.4,0.445,0.385,0.365,0.92,0.375,0.37,0.365,0.295,0.305,0.675,0.565,0.695,0.61,0.525,0.515,0.555,0.515,0.515,0.92,0.485,0.505,0.54,0.47,0.455,0.15,0.34,0.165,0.225,0.285,0.28,0.26,0.345,0.35,0.01,0.305,0.335,0.325,0.395,0.39,greedy,Train for 500x100 batches of 2 examples,500.0,50000.0,3236.243579463,EVALUATIONS/WEIGHTS_BACKUP/1618601259173182361#139846054152000,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 19:27:39,"2-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.3564062500000005,0.0476273193359375,0.0032054773776941,1618601259977391238#139810447120192,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.135,0.19,0.135,0.16,0.16,0.235,0.165,0.125,0.18,0.045,0.175,0.155,0.205,0.15,0.165,0.71,0.345,0.645,0.57,0.485,0.435,0.51,0.525,0.5,0.84,0.495,0.47,0.365,0.42,0.375,0.745,0.535,0.675,0.6,0.525,0.495,0.585,0.62,0.565,0.84,0.6,0.605,0.49,0.56,0.53,0.155,0.31,0.155,0.215,0.205,0.285,0.2,0.205,0.24,0.045,0.235,0.21,0.315,0.27,0.285,greedy,Train for 500x100 batches of 2 examples,500.0,50000.0,3236.648059288,EVALUATIONS/WEIGHTS_BACKUP/1618601259977391238#139810447120192,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 19:27:44,"2-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.4258216552734373,-0.0146820678710938,0.0322475269045909,1618601264282715718#139972351563584,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.055,0.08,0.085,0.08,0.12,0.065,0.08,0.07,0.01,0.09,0.105,0.09,0.065,0.075,0.645,0.375,0.68,0.49,0.5,0.495,0.555,0.505,0.445,0.865,0.475,0.43,0.4,0.42,0.35,0.71,0.66,0.74,0.64,0.645,0.7,0.685,0.685,0.65,0.865,0.655,0.63,0.63,0.695,0.645,0.11,0.24,0.105,0.15,0.11,0.19,0.14,0.17,0.165,0.01,0.215,0.24,0.245,0.195,0.23,greedy,Train for 500x100 batches of 2 examples,500.0,50000.0,3241.2058881830003,EVALUATIONS/WEIGHTS_BACKUP/1618601264282715718#139972351563584,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 19:41:07,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",5.078235046386718,-0.0249244995117187,0.0261339939433256,1618602067933304920#140224752113472,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.24,0.015,0.225,0.17,0.17,0.135,0.145,0.11,0.12,0.215,0.135,0.08,0.06,0.065,0.03,0.44,0.01,0.34,0.255,0.23,0.18,0.125,0.125,0.165,0.7,0.1,0.075,0.055,0.06,0.04,0.54,0.15,0.455,0.44,0.395,0.405,0.375,0.4,0.38,0.7,0.38,0.355,0.315,0.3,0.22,0.28,0.29,0.27,0.255,0.25,0.305,0.36,0.315,0.27,0.215,0.4,0.37,0.35,0.35,0.345,greedy,Train for 100x100 batches of 2 examples,100.0,10000.0,342.910075968,EVALUATIONS/WEIGHTS_BACKUP/1618602067933304920#140224752113472,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 19:43:12,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.650898376464844,-0.0147052001953124,0.0158779373325188,1618602192008895181#139972351563584,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.15,0.055,0.245,0.24,0.23,0.245,0.245,0.215,0.225,0.02,0.235,0.165,0.16,0.13,0.105,0.54,0.03,0.4,0.255,0.265,0.2,0.195,0.18,0.165,0.9,0.11,0.075,0.05,0.03,0.015,0.595,0.205,0.45,0.39,0.405,0.315,0.3,0.295,0.325,0.9,0.29,0.295,0.22,0.17,0.175,0.2,0.475,0.275,0.355,0.295,0.375,0.395,0.39,0.395,0.02,0.44,0.43,0.435,0.525,0.42,greedy,Train for 100x100 batches of 4 examples,100.0,10000.0,458.03648781899983,EVALUATIONS/WEIGHTS_BACKUP/1618602192008895181#139972351563584,Batch of 4 25-cycles with expected 1,
godot,2021/04/16 19:46:56,"4-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.2523489990234373,0.0332836303710937,0.0174561766473733,1618602416732888217#140219512997696,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.11,0.215,0.125,0.18,0.19,0.205,0.13,0.18,0.16,0.005,0.195,0.155,0.18,0.14,0.185,0.67,0.365,0.65,0.58,0.5,0.48,0.48,0.485,0.49,0.96,0.52,0.445,0.39,0.415,0.385,0.71,0.54,0.69,0.635,0.56,0.56,0.56,0.575,0.57,0.96,0.6,0.595,0.485,0.57,0.54,0.135,0.32,0.155,0.205,0.25,0.245,0.215,0.29,0.245,0.005,0.26,0.26,0.32,0.26,0.315,greedy,Train for 500x100 batches of 4 examples,500.0,50000.0,4392.646670637,EVALUATIONS/WEIGHTS_BACKUP/1618602416732888217#140219512997696,Batch of 4 25-cycles with expected 1,
godot,2021/04/16 19:47:22,"4-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.09118408203125,0.0059974975585936,0.0014171654348444,1618602442048737683#140471833462592,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.145,0.205,0.16,0.225,0.18,0.14,0.155,0.19,0.195,0.02,0.16,0.175,0.165,0.18,0.18,0.62,0.26,0.59,0.455,0.465,0.49,0.435,0.43,0.405,0.92,0.415,0.32,0.285,0.325,0.275,0.66,0.43,0.61,0.54,0.55,0.605,0.54,0.525,0.545,0.92,0.545,0.435,0.435,0.49,0.405,0.165,0.375,0.18,0.265,0.26,0.23,0.24,0.32,0.305,0.02,0.32,0.355,0.34,0.28,0.33,greedy,Train for 500x100 batches of 4 examples,500.0,50000.0,4418.149808272,EVALUATIONS/WEIGHTS_BACKUP/1618602442048737683#140471833462592,Batch of 4 25-cycles with expected 1,
godot,2021/04/16 19:48:52,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.552091857910156,0.0209125976562498,0.0247894409920874,1618602532300907002#139846054152000,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.15,0.265,0.2,0.24,0.255,0.225,0.245,0.295,0.285,0.0,0.295,0.27,0.265,0.24,0.23,0.555,0.13,0.53,0.405,0.365,0.355,0.29,0.36,0.255,0.93,0.25,0.2,0.195,0.23,0.175,0.65,0.35,0.555,0.515,0.49,0.54,0.42,0.5,0.375,0.93,0.47,0.445,0.385,0.425,0.415,0.18,0.51,0.245,0.3,0.315,0.345,0.34,0.38,0.455,0.0,0.425,0.455,0.465,0.445,0.475,greedy,Train for 100x100 batches of 2 examples,100.0,10000.0,804.8306800739997,EVALUATIONS/WEIGHTS_BACKUP/1618602532300907002#139846054152000,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 19:53:55,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.4926669921875,-0.0535851440429686,0.0195548533116074,1618602835024503822#139810447120192,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-45            2,080
|    |    |    └─ReLU: 4-46              --
=================================================================
Total params: 42,016
Trainable params: 42,016
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.085,0.17,0.165,0.195,0.155,0.15,0.195,0.18,0.19,0.01,0.235,0.12,0.14,0.135,0.115,0.485,0.115,0.37,0.305,0.265,0.19,0.17,0.21,0.21,0.905,0.145,0.175,0.175,0.13,0.125,0.625,0.37,0.455,0.455,0.425,0.425,0.39,0.44,0.435,0.905,0.375,0.41,0.385,0.445,0.43,0.095,0.435,0.215,0.27,0.23,0.26,0.305,0.32,0.36,0.01,0.39,0.34,0.415,0.38,0.39,greedy,Train for 100x100 batches of 2 examples,100.0,10000.0,1106.339854916,EVALUATIONS/WEIGHTS_BACKUP/1618602835024503822#139810447120192,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 20:05:56,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.598055908203124,-0.0276161499023437,0.0021575181515096,1618603556380683733#140219512997696,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.22,0.145,0.3,0.29,0.225,0.33,0.3,0.315,0.215,0.045,0.215,0.23,0.2,0.13,0.12,0.615,0.075,0.515,0.35,0.415,0.295,0.28,0.255,0.275,0.895,0.32,0.175,0.08,0.11,0.075,0.66,0.245,0.54,0.475,0.52,0.4,0.415,0.435,0.45,0.895,0.47,0.425,0.345,0.36,0.345,0.24,0.56,0.325,0.365,0.285,0.425,0.395,0.47,0.355,0.045,0.38,0.475,0.5,0.42,0.475,greedy,Train for 100x100 batches of 8 examples,100.0,10000.0,676.536713939,EVALUATIONS/WEIGHTS_BACKUP/1618603556380683733#140219512997696,Batch of 8 25-cycles with expected 1,
godot,2021/04/16 20:02:43,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.4681481323242185,0.0218434448242186,0.0051502084088106,1618603363861626593#140224752113472,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.115,0.165,0.145,0.175,0.155,0.165,0.225,0.165,0.14,0.01,0.165,0.2,0.16,0.165,0.195,0.64,0.205,0.49,0.45,0.345,0.315,0.295,0.315,0.335,0.91,0.3,0.275,0.26,0.205,0.195,0.68,0.55,0.565,0.545,0.52,0.555,0.44,0.51,0.525,0.91,0.54,0.535,0.5,0.52,0.525,0.135,0.315,0.18,0.23,0.195,0.215,0.32,0.28,0.23,0.01,0.3,0.31,0.315,0.34,0.375,greedy,Train for 100x100 batches of 4 examples,100.0,10000.0,1081.0374566190003,EVALUATIONS/WEIGHTS_BACKUP/1618603363861626593#140224752113472,Batch of 4 25-cycles with expected 1,
godot,2021/04/16 20:11:53,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.273241333007813,0.0028145141601562,0.0019001268616776,1618603913095698647#139972351563584,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-45            2,080
|    |    |    └─ReLU: 4-46              --
=================================================================
Total params: 42,016
Trainable params: 42,016
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,0.325,0.195,0.32,0.26,0.27,0.28,0.28,0.305,0.01,0.29,0.28,0.28,0.325,0.29,0.675,0.26,0.54,0.44,0.415,0.415,0.355,0.36,0.31,0.905,0.365,0.36,0.29,0.245,0.265,0.695,0.39,0.58,0.5,0.515,0.545,0.46,0.475,0.465,0.905,0.505,0.46,0.435,0.41,0.445,0.155,0.45,0.215,0.375,0.295,0.31,0.34,0.37,0.41,0.01,0.365,0.405,0.385,0.445,0.41,greedy,Train for 100x100 batches of 4 examples,100.0,10000.0,1504.4106128169997,EVALUATIONS/WEIGHTS_BACKUP/1618603913095698647#139972351563584,Batch of 4 25-cycles with expected 1,
godot,2021/04/16 20:22:17,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.405497619628907,-0.0020374145507812,0.009385376974663,1618604537161666575#140471833462592,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.195,0.07,0.2,0.13,0.185,0.12,0.125,0.15,0.13,0.06,0.065,0.09,0.075,0.06,0.055,0.6,0.165,0.52,0.49,0.37,0.325,0.28,0.295,0.24,0.885,0.26,0.205,0.165,0.205,0.125,0.635,0.585,0.565,0.615,0.52,0.565,0.46,0.525,0.52,0.885,0.585,0.51,0.565,0.625,0.545,0.215,0.26,0.22,0.2,0.24,0.225,0.215,0.3,0.21,0.06,0.21,0.26,0.265,0.225,0.225,greedy,Train for 100x100 batches of 8 examples,100.0,10000.0,1618.1057008950002,EVALUATIONS/WEIGHTS_BACKUP/1618604537161666575#140471833462592,Batch of 8 25-cycles with expected 1,
godot,2021/04/16 20:25:10,"8-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.940015380859375,0.01575634765625,0.0012518624251569,1618604710517394931#140253567027008,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.11,0.15,0.13,0.13,0.125,0.125,0.135,0.125,0.08,0.03,0.105,0.13,0.115,0.195,0.1,0.695,0.445,0.7,0.635,0.61,0.565,0.51,0.625,0.595,0.87,0.57,0.505,0.48,0.45,0.52,0.73,0.625,0.745,0.685,0.685,0.665,0.595,0.74,0.685,0.87,0.715,0.675,0.66,0.635,0.675,0.125,0.285,0.145,0.17,0.15,0.165,0.195,0.155,0.135,0.03,0.16,0.175,0.18,0.25,0.215,greedy,Train for 500x100 batches of 8 examples,500.0,50000.0,6684.539242511,EVALUATIONS/WEIGHTS_BACKUP/1618604710517394931#140253567027008,Batch of 8 25-cycles with expected 1,
godot,2021/04/16 20:25:46,"8-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.0577907104492184,0.0005390014648437,0.0045926077657441,1618604746104827785#139867008038720,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.08,0.12,0.07,0.11,0.12,0.13,0.115,0.16,0.145,0.01,0.11,0.155,0.12,0.15,0.11,0.69,0.345,0.7,0.56,0.535,0.515,0.54,0.525,0.46,0.905,0.575,0.505,0.44,0.45,0.41,0.745,0.59,0.715,0.665,0.605,0.585,0.62,0.62,0.6,0.905,0.68,0.6,0.575,0.585,0.595,0.105,0.27,0.08,0.155,0.18,0.185,0.175,0.23,0.19,0.01,0.165,0.2,0.21,0.23,0.225,greedy,Train for 500x100 batches of 8 examples,500.0,50000.0,6718.568300115,EVALUATIONS/WEIGHTS_BACKUP/1618604746104827785#139867008038720,Batch of 8 25-cycles with expected 1,
godot,2021/04/16 20:36:02,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.751661071777344,-0.0104749755859375,0.0051368444267279,1618605362150275081#139810447120192,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.155,0.195,0.175,0.22,0.18,0.15,0.12,0.195,0.175,0.01,0.185,0.2,0.205,0.17,0.16,0.63,0.305,0.65,0.485,0.425,0.435,0.505,0.43,0.415,0.94,0.43,0.36,0.305,0.33,0.285,0.66,0.47,0.675,0.49,0.47,0.485,0.555,0.5,0.505,0.94,0.51,0.475,0.42,0.505,0.405,0.185,0.26,0.18,0.265,0.195,0.21,0.16,0.245,0.22,0.01,0.265,0.245,0.245,0.22,0.24,greedy,Train for 500x100 batches of 2 examples,500.0,50000.0,1737.9565506090005,EVALUATIONS/WEIGHTS_BACKUP/1618605362150275081#139810447120192,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 20:35:20,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.654182250976563,0.0058404541015626,0.00151931164282,1618605320959218474#139846054152000,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-45            2,080
|    |    |    └─ReLU: 4-46              --
=================================================================
Total params: 42,016
Trainable params: 42,016
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.22,0.06,0.3,0.35,0.31,0.29,0.245,0.29,0.225,0.025,0.305,0.15,0.13,0.08,0.07,0.475,0.005,0.39,0.28,0.2,0.21,0.115,0.09,0.125,0.905,0.1,0.03,0.02,0.02,0.005,0.57,0.105,0.43,0.39,0.285,0.305,0.235,0.225,0.255,0.905,0.195,0.205,0.15,0.15,0.09,0.25,0.51,0.34,0.425,0.41,0.445,0.485,0.58,0.425,0.025,0.545,0.43,0.475,0.4,0.44,greedy,Train for 100x100 batches of 8 examples,100.0,10000.0,2234.154856233,EVALUATIONS/WEIGHTS_BACKUP/1618605320959218474#139846054152000,Batch of 8 25-cycles with expected 1,
godot,2021/04/16 21:03:42,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.771974914550781,-0.0274277343749999,0.0026533964099222,1618607022546923126#139972351563584,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,0.145,0.15,0.165,0.125,0.155,0.215,0.17,0.18,0.02,0.135,0.17,0.17,0.185,0.175,0.57,0.3,0.66,0.58,0.525,0.495,0.435,0.435,0.435,0.925,0.44,0.37,0.375,0.36,0.27,0.63,0.435,0.695,0.645,0.585,0.605,0.5,0.58,0.52,0.925,0.565,0.49,0.48,0.49,0.455,0.17,0.285,0.165,0.2,0.15,0.205,0.245,0.235,0.24,0.02,0.195,0.245,0.25,0.285,0.285,greedy,Train for 500x100 batches of 4 examples,500.0,50000.0,2312.5920042820007,EVALUATIONS/WEIGHTS_BACKUP/1618607022546923126#139972351563584,Batch of 4 25-cycles with expected 1,
godot,2021/04/16 21:17:04,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.113855529785156,-0.0257476806640625,0.003421546182313,1618607824523919957#140219512997696,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.125,0.19,0.13,0.14,0.14,0.17,0.1,0.13,0.17,0.06,0.16,0.185,0.135,0.205,0.19,0.68,0.325,0.705,0.575,0.475,0.465,0.505,0.475,0.435,0.865,0.42,0.39,0.36,0.31,0.265,0.705,0.49,0.73,0.615,0.53,0.515,0.56,0.585,0.5,0.865,0.555,0.5,0.485,0.455,0.41,0.135,0.3,0.145,0.18,0.165,0.215,0.125,0.145,0.21,0.06,0.2,0.25,0.22,0.3,0.285,greedy,Train for 500x100 batches of 2 examples,500.0,50000.0,4048.07166027,EVALUATIONS/WEIGHTS_BACKUP/1618607824523919957#140219512997696,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 21:30:43,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.494731018066406,-0.0226534423828124,0.003430735163306,1618608643007258030#139867008038720,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.13,0.26,0.14,0.15,0.14,0.125,0.12,0.145,0.16,0.015,0.23,0.165,0.21,0.19,0.185,0.62,0.295,0.66,0.54,0.495,0.51,0.46,0.48,0.48,0.915,0.405,0.42,0.35,0.385,0.275,0.7,0.405,0.67,0.565,0.52,0.545,0.53,0.545,0.55,0.915,0.49,0.52,0.425,0.525,0.44,0.155,0.36,0.165,0.205,0.175,0.17,0.145,0.185,0.18,0.015,0.275,0.19,0.27,0.205,0.275,greedy,Train for 500x100 batches of 8 examples,500.0,50000.0,3434.880969973,EVALUATIONS/WEIGHTS_BACKUP/1618608643007258030#139867008038720,Batch of 8 25-cycles with expected 1,
godot,2021/04/16 21:44:32,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.448241027832032,0.0802929687499999,0.0524385846787467,1618609472952643745#140224752113472,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-45            2,080
|    |    |    └─ReLU: 4-46              --
=================================================================
Total params: 42,016
Trainable params: 42,016
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.185,0.265,0.18,0.225,0.275,0.215,0.25,0.285,0.24,0.005,0.3,0.305,0.285,0.265,0.225,0.665,0.125,0.58,0.495,0.355,0.44,0.325,0.33,0.315,0.91,0.24,0.23,0.165,0.18,0.14,0.69,0.28,0.605,0.55,0.445,0.47,0.39,0.465,0.425,0.91,0.375,0.345,0.265,0.275,0.29,0.205,0.525,0.185,0.265,0.335,0.31,0.375,0.38,0.345,0.005,0.425,0.485,0.505,0.55,0.48,greedy,Train for 500x100 batches of 2 examples,500.0,50000.0,5549.584580557,EVALUATIONS/WEIGHTS_BACKUP/1618609472952643745#140224752113472,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 22:01:43,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.145775268554688,0.0550235595703124,0.0415244683554281,1618610503482072816#140471833462592,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.115,0.1,0.09,0.1,0.115,0.13,0.175,0.12,0.11,0.015,0.13,0.115,0.105,0.045,0.105,0.68,0.2,0.69,0.625,0.49,0.52,0.475,0.485,0.41,0.905,0.39,0.335,0.31,0.34,0.285,0.77,0.54,0.76,0.69,0.57,0.65,0.565,0.63,0.6,0.905,0.64,0.575,0.51,0.605,0.525,0.115,0.24,0.095,0.17,0.195,0.165,0.245,0.2,0.18,0.015,0.18,0.205,0.225,0.175,0.295,greedy,Train for 500x100 batches of 4 examples,500.0,50000.0,5408.590494136,EVALUATIONS/WEIGHTS_BACKUP/1618610503482072816#140471833462592,Batch of 4 25-cycles with expected 1,
godot,2021/04/16 22:09:46,"2-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.7295634765625,0.0164929809570312,0.0217587072339897,1618610986407295935#139767403444032,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.075,0.03,0.09,0.07,0.045,0.055,0.07,0.055,0.055,0.015,0.05,0.04,0.035,0.05,0.03,0.715,0.375,0.76,0.585,0.59,0.55,0.485,0.53,0.53,0.895,0.485,0.46,0.455,0.42,0.42,0.73,0.65,0.765,0.67,0.69,0.65,0.595,0.705,0.635,0.895,0.64,0.665,0.63,0.66,0.645,0.095,0.165,0.1,0.095,0.1,0.12,0.125,0.15,0.115,0.015,0.1,0.1,0.155,0.165,0.155,greedy,Train for 2000x100 batches of 2 examples,2000.0,200000.0,12957.243205214,EVALUATIONS/WEIGHTS_BACKUP/1618610986407295935#139767403444032,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 22:10:07,"2-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.8419242553710937,0.0468422241210937,0.0084407077785417,1618611007217378176#139842414118720,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.08,0.045,0.06,0.095,0.06,0.07,0.08,0.025,0.025,0.0,0.03,0.06,0.045,0.025,0.035,0.735,0.46,0.765,0.62,0.615,0.525,0.56,0.65,0.595,0.935,0.575,0.565,0.48,0.57,0.48,0.785,0.76,0.785,0.7,0.69,0.665,0.71,0.81,0.74,0.935,0.77,0.73,0.675,0.735,0.71,0.09,0.115,0.07,0.13,0.1,0.135,0.105,0.055,0.08,0.0,0.045,0.13,0.155,0.115,0.105,greedy,Train for 2000x100 batches of 2 examples,2000.0,200000.0,12979.34079667,EVALUATIONS/WEIGHTS_BACKUP/1618611007217378176#139842414118720,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 22:12:00,"2-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.836609252929688,0.0742636108398437,0.0101936985927562,1618611120547242721#140206076311360,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.06,0.035,0.05,0.05,0.045,0.095,0.06,0.055,0.05,0.005,0.07,0.04,0.065,0.055,0.065,0.685,0.49,0.73,0.595,0.545,0.54,0.55,0.55,0.575,0.895,0.54,0.505,0.52,0.515,0.47,0.775,0.79,0.78,0.69,0.65,0.665,0.705,0.74,0.71,0.895,0.725,0.72,0.695,0.745,0.72,0.08,0.135,0.07,0.075,0.09,0.135,0.09,0.125,0.11,0.005,0.145,0.135,0.165,0.14,0.17,greedy,Train for 2000x100 batches of 2 examples,2000.0,200000.0,13091.722788982,EVALUATIONS/WEIGHTS_BACKUP/1618611120547242721#140206076311360,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 22:12:12,"2-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.777593933105469,-0.0276111450195313,0.0093169246000561,1618611132386276466#140600081913664,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.09,0.05,0.06,0.07,0.045,0.055,0.04,0.055,0.05,0.005,0.04,0.045,0.055,0.055,0.035,0.71,0.51,0.745,0.635,0.57,0.605,0.575,0.59,0.585,0.925,0.585,0.48,0.48,0.45,0.44,0.755,0.69,0.78,0.67,0.615,0.665,0.635,0.705,0.66,0.925,0.685,0.63,0.555,0.66,0.615,0.105,0.115,0.08,0.125,0.07,0.08,0.09,0.07,0.095,0.005,0.07,0.09,0.16,0.12,0.115,greedy,Train for 2000x100 batches of 2 examples,2000.0,200000.0,13104.465762318,EVALUATIONS/WEIGHTS_BACKUP/1618611132386276466#140600081913664,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 22:46:16,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.7716032714843752,-0.0099557495117187,0.0004753673365183,1618613176784257845#140253567027008,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-45            2,080
|    |    |    └─ReLU: 4-46              --
=================================================================
Total params: 42,016
Trainable params: 42,016
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.13,0.085,0.07,0.08,0.085,0.09,0.05,0.055,0.05,0.04,0.085,0.075,0.065,0.06,0.075,0.685,0.415,0.76,0.635,0.535,0.575,0.55,0.565,0.545,0.87,0.57,0.465,0.425,0.505,0.405,0.75,0.645,0.8,0.68,0.58,0.665,0.605,0.69,0.675,0.87,0.7,0.63,0.655,0.68,0.635,0.14,0.2,0.08,0.105,0.1,0.135,0.07,0.075,0.075,0.04,0.12,0.125,0.095,0.11,0.17,greedy,Train for 500x100 batches of 4 examples,500.0,50000.0,7504.398329756999,EVALUATIONS/WEIGHTS_BACKUP/1618613176784257845#140253567027008,Batch of 4 25-cycles with expected 1,
godot,2021/04/16 23:04:59,MEMPROFILER-TEST! 3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.80129296875,0.0600405883789063,0.0158877032035036,1618614299481007650#139997206476608,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.28,0.015,0.305,0.22,0.165,0.135,0.17,0.125,0.17,0.01,0.12,0.115,0.105,0.09,0.03,0.34,0.005,0.275,0.245,0.29,0.305,0.185,0.21,0.195,0.905,0.185,0.1,0.075,0.04,0.025,0.465,0.16,0.35,0.36,0.455,0.475,0.43,0.475,0.465,0.905,0.47,0.395,0.35,0.295,0.235,0.31,0.405,0.385,0.405,0.25,0.23,0.295,0.305,0.31,0.01,0.27,0.335,0.4,0.395,0.41,greedy,Train for 100x100 batches of 2 examples,100.0,10000.0,1331.66493495,EVALUATIONS/WEIGHTS_BACKUP/1618614299481007650#139997206476608,Batch of 2 25-cycles with expected 1,
godot,2021/04/16 23:05:03,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.073017639160156,-0.0075552368164062,0.0023157293169351,1618614303383476346#139810447120192,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.06,0.24,0.115,0.11,0.07,0.15,0.155,0.145,0.12,0.015,0.175,0.165,0.19,0.2,0.2,0.715,0.265,0.68,0.62,0.585,0.505,0.515,0.485,0.465,0.89,0.485,0.405,0.35,0.36,0.33,0.765,0.38,0.74,0.675,0.69,0.635,0.595,0.595,0.625,0.89,0.585,0.525,0.48,0.475,0.45,0.065,0.41,0.125,0.155,0.095,0.245,0.225,0.265,0.185,0.015,0.26,0.33,0.27,0.355,0.32,greedy,Train for 500x100 batches of 8 examples,500.0,50000.0,8212.819187191,EVALUATIONS/WEIGHTS_BACKUP/1618614303383476346#139810447120192,Batch of 8 25-cycles with expected 1,
godot,2021/04/16 23:37:46,"4-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.672232299804688,0.0312010498046874,0.0034903909477339,1618616266722751417#140329957377856,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.085,0.085,0.035,0.03,0.045,0.045,0.07,0.055,0.045,0.005,0.075,0.055,0.05,0.055,0.06,0.71,0.39,0.715,0.645,0.565,0.56,0.52,0.595,0.595,0.9,0.535,0.54,0.435,0.51,0.435,0.76,0.625,0.745,0.74,0.635,0.675,0.605,0.7,0.69,0.9,0.685,0.7,0.625,0.67,0.615,0.11,0.175,0.04,0.08,0.09,0.06,0.115,0.13,0.095,0.005,0.12,0.085,0.125,0.145,0.16,greedy,Train for 2000x100 batches of 4 examples,2000.0,200000.0,17731.612307921998,EVALUATIONS/WEIGHTS_BACKUP/1618616266722751417#140329957377856,Batch of 4 25-cycles with expected 1,
godot,2021/04/16 23:38:07,"4-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.6400706176757813,0.0049934082031249,0.0026642681792585,1618616287566489212#140422933006144,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.09,0.095,0.04,0.085,0.05,0.065,0.1,0.05,0.075,0.01,0.06,0.1,0.1,0.1,0.06,0.715,0.495,0.75,0.63,0.61,0.565,0.59,0.615,0.61,0.915,0.645,0.515,0.515,0.555,0.54,0.775,0.635,0.815,0.69,0.7,0.685,0.64,0.715,0.705,0.915,0.725,0.62,0.63,0.67,0.695,0.105,0.15,0.055,0.13,0.07,0.1,0.12,0.07,0.095,0.01,0.1,0.14,0.125,0.14,0.095,greedy,Train for 2000x100 batches of 4 examples,2000.0,200000.0,17753.847679813,EVALUATIONS/WEIGHTS_BACKUP/1618616287566489212#140422933006144,Batch of 4 25-cycles with expected 1,
godot,2021/04/17 00:02:57,MEMPROFILER-TEST! 3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.497317199707031,-0.0147515258789063,0.0067591390683858,1618617777703336439#139997206476608,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.205,0.22,0.17,0.215,0.205,0.225,0.175,0.195,0.19,0.01,0.205,0.185,0.165,0.2,0.17,0.545,0.25,0.58,0.47,0.385,0.32,0.35,0.4,0.315,0.93,0.38,0.295,0.315,0.275,0.255,0.59,0.46,0.61,0.56,0.435,0.465,0.465,0.54,0.445,0.93,0.505,0.465,0.485,0.49,0.45,0.205,0.345,0.18,0.26,0.305,0.32,0.28,0.27,0.295,0.01,0.27,0.325,0.28,0.285,0.295,greedy,Train for 100x100 batches of 2 examples,100.0,10000.0,3211.631608784,EVALUATIONS/WEIGHTS_BACKUP/1618617777703336439#139997206476608,Batch of 2 25-cycles with expected 1,
godot,2021/04/17 00:05:21,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.8559252929687498,-0.0408154907226562,0.0009268498877066,1618617921042003614#139846054152000,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-45            2,080
|    |    |    └─ReLU: 4-46              --
=================================================================
Total params: 42,016
Trainable params: 42,016
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.085,0.07,0.035,0.145,0.1,0.095,0.095,0.11,0.11,0.01,0.07,0.09,0.065,0.115,0.085,0.645,0.335,0.7,0.575,0.525,0.49,0.485,0.57,0.47,0.93,0.54,0.47,0.455,0.425,0.37,0.72,0.605,0.77,0.615,0.565,0.585,0.545,0.665,0.58,0.93,0.65,0.59,0.55,0.56,0.61,0.1,0.145,0.055,0.175,0.145,0.145,0.125,0.15,0.165,0.01,0.12,0.155,0.155,0.195,0.11,greedy,Train for 500x100 batches of 8 examples,500.0,50000.0,11333.867644692,EVALUATIONS/WEIGHTS_BACKUP/1618617921042003614#139846054152000,Batch of 8 25-cycles with expected 1,
godot,2021/04/17 01:28:02,MEMPROFILER-TEST! 3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.719392822265625,0.0499386596679688,0.0314061454645013,1618622882799584256#139997206476608,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-45            2,080
|    |    |    └─ReLU: 4-46              --
=================================================================
Total params: 42,016
Trainable params: 42,016
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.205,0.28,0.245,0.335,0.295,0.315,0.28,0.415,0.285,0.035,0.34,0.285,0.315,0.25,0.315,0.55,0.12,0.425,0.295,0.31,0.245,0.225,0.19,0.19,0.87,0.19,0.175,0.125,0.165,0.125,0.64,0.235,0.515,0.39,0.405,0.34,0.36,0.28,0.285,0.87,0.34,0.28,0.26,0.305,0.25,0.22,0.65,0.265,0.395,0.385,0.42,0.43,0.555,0.48,0.035,0.5,0.56,0.57,0.535,0.64,greedy,Train for 100x100 batches of 2 examples,100.0,10000.0,4447.23713649,EVALUATIONS/WEIGHTS_BACKUP/1618622882799584256#139997206476608,Batch of 2 25-cycles with expected 1,
godot,2021/04/17 02:08:05,MEMPROFILER-TEST! 3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.702390014648437,0.0383656616210938,0.0047059324083704,1618625285911944871#139997206476608,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.19,0.22,0.22,0.24,0.195,0.245,0.25,0.245,0.205,0.005,0.265,0.21,0.18,0.19,0.235,0.52,0.125,0.42,0.345,0.3,0.235,0.22,0.19,0.21,0.875,0.195,0.16,0.2,0.15,0.165,0.595,0.34,0.485,0.48,0.4,0.41,0.39,0.345,0.435,0.875,0.375,0.375,0.45,0.46,0.435,0.23,0.495,0.27,0.31,0.285,0.37,0.38,0.425,0.325,0.005,0.42,0.445,0.43,0.4,0.48,greedy,Train for 100x100 batches of 4 examples,100.0,10000.0,1479.1224440389997,EVALUATIONS/WEIGHTS_BACKUP/1618625285911944871#139997206476608,Batch of 4 25-cycles with expected 1,
godot,2021/04/17 02:11:58,"8-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.4591603393554684,-0.0085234680175781,0.0049808326550442,1618625518507544662#140516795316032,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.055,0.13,0.08,0.07,0.06,0.075,0.07,0.075,0.035,0.01,0.095,0.065,0.04,0.095,0.135,0.765,0.46,0.77,0.72,0.64,0.58,0.63,0.65,0.67,0.89,0.615,0.585,0.535,0.575,0.49,0.78,0.63,0.815,0.765,0.685,0.69,0.695,0.725,0.745,0.89,0.705,0.71,0.7,0.69,0.61,0.09,0.2,0.09,0.115,0.115,0.12,0.11,0.135,0.07,0.01,0.145,0.115,0.09,0.165,0.235,greedy,Train for 2000x100 batches of 8 examples,2000.0,200000.0,26970.36260387,EVALUATIONS/WEIGHTS_BACKUP/1618625518507544662#140516795316032,Batch of 8 25-cycles with expected 1,
godot,2021/04/17 02:12:02,"8-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.5485223693847656,0.0084285278320312,0.0050229963535839,1618625522268137831#140197722974016,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.075,0.03,0.045,0.09,0.045,0.06,0.055,0.015,0.045,0.01,0.035,0.09,0.035,0.05,0.025,0.73,0.62,0.79,0.635,0.63,0.59,0.61,0.635,0.655,0.92,0.635,0.56,0.55,0.545,0.545,0.775,0.755,0.83,0.7,0.735,0.71,0.69,0.78,0.755,0.92,0.79,0.665,0.685,0.74,0.74,0.1,0.1,0.055,0.135,0.075,0.11,0.09,0.07,0.09,0.01,0.075,0.175,0.095,0.12,0.085,greedy,Train for 2000x100 batches of 8 examples,2000.0,200000.0,26977.80831993,EVALUATIONS/WEIGHTS_BACKUP/1618625522268137831#140197722974016,Batch of 8 25-cycles with expected 1,
godot,2021/04/17 03:11:53,MEMPROFILER-TEST! 3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.631817321777343,-0.0159277954101563,0.0060115887088336,1618629113283425680#139997206476608,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.155,0.135,0.165,0.235,0.22,0.185,0.215,0.195,0.205,0.015,0.16,0.16,0.175,0.17,0.09,0.59,0.105,0.525,0.33,0.295,0.305,0.24,0.21,0.21,0.915,0.26,0.18,0.095,0.16,0.135,0.655,0.42,0.58,0.5,0.415,0.505,0.42,0.44,0.385,0.915,0.46,0.405,0.325,0.445,0.46,0.175,0.425,0.205,0.305,0.305,0.305,0.35,0.385,0.39,0.015,0.345,0.395,0.51,0.41,0.39,greedy,Train for 100x100 batches of 4 examples,100.0,10000.0,3554.0640809310007,EVALUATIONS/WEIGHTS_BACKUP/1618629113283425680#139997206476608,Batch of 4 25-cycles with expected 1,
godot,2021/04/17 04:45:26,MEMPROFILER-TEST! 3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.591752258300781,-0.0626052856445312,0.0077661066115766,1618634726483599593#139997206476608,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-45            2,080
|    |    |    └─ReLU: 4-46              --
=================================================================
Total params: 42,016
Trainable params: 42,016
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.305,0.035,0.3,0.265,0.21,0.235,0.21,0.18,0.16,0.15,0.11,0.11,0.04,0.07,0.1,0.35,0.035,0.345,0.28,0.205,0.155,0.135,0.125,0.095,0.735,0.105,0.035,0.04,0.035,0.015,0.435,0.2,0.38,0.4,0.315,0.325,0.26,0.3,0.245,0.735,0.32,0.22,0.225,0.265,0.155,0.34,0.355,0.34,0.39,0.33,0.4,0.395,0.395,0.37,0.15,0.3,0.365,0.32,0.305,0.41,greedy,Train for 100x100 batches of 4 examples,100.0,10000.0,4948.604002418,EVALUATIONS/WEIGHTS_BACKUP/1618634726483599593#139997206476608,Batch of 4 25-cycles with expected 1,
godot,2021/04/17 05:29:59,MEMPROFILER-TEST! 3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.582429931640625,-0.0270540771484375,0.0009755154833364,1618637399012779583#139997206476608,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.155,0.055,0.2,0.205,0.2,0.16,0.15,0.15,0.165,0.015,0.115,0.095,0.13,0.095,0.045,0.48,0.025,0.42,0.35,0.245,0.265,0.225,0.23,0.205,0.925,0.18,0.15,0.07,0.085,0.045,0.545,0.315,0.485,0.495,0.395,0.46,0.4,0.45,0.425,0.925,0.49,0.43,0.36,0.34,0.335,0.215,0.315,0.22,0.275,0.27,0.245,0.27,0.29,0.265,0.015,0.24,0.29,0.33,0.32,0.26,greedy,Train for 100x100 batches of 8 examples,100.0,10000.0,1759.258128417001,EVALUATIONS/WEIGHTS_BACKUP/1618637399012779583#139997206476608,Batch of 8 25-cycles with expected 1,
godot,2021/04/17 06:45:37,MEMPROFILER-TEST! 3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.086350341796875,-3.802490234381395e-05,0.0080742438859999,1618641937613765621#139997206476608,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.115,0.15,0.17,0.225,0.18,0.2,0.195,0.195,0.195,0.015,0.285,0.18,0.18,0.115,0.09,0.58,0.06,0.6,0.49,0.495,0.46,0.425,0.47,0.41,0.91,0.355,0.255,0.125,0.15,0.055,0.65,0.17,0.63,0.525,0.54,0.525,0.495,0.54,0.485,0.91,0.44,0.355,0.25,0.3,0.17,0.12,0.455,0.19,0.27,0.24,0.285,0.27,0.28,0.3,0.015,0.355,0.415,0.415,0.415,0.41,greedy,Train for 100x100 batches of 8 examples,100.0,10000.0,4270.337110346001,EVALUATIONS/WEIGHTS_BACKUP/1618641937613765621#139997206476608,Batch of 8 25-cycles with expected 1,
godot,2021/04/17 08:36:08,MEMPROFILER-TEST! 3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.306092224121093,-0.0397617797851562,0.0022832903651952,1618648568568965579#139997206476608,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-45            2,080
|    |    |    └─ReLU: 4-46              --
=================================================================
Total params: 42,016
Trainable params: 42,016
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.255,0.04,0.255,0.195,0.22,0.24,0.19,0.165,0.16,0.015,0.125,0.055,0.105,0.085,0.045,0.48,0.015,0.44,0.39,0.28,0.165,0.185,0.18,0.135,0.88,0.135,0.08,0.02,0.05,0.015,0.56,0.3,0.475,0.505,0.46,0.41,0.34,0.435,0.335,0.88,0.36,0.33,0.335,0.3,0.305,0.29,0.4,0.295,0.295,0.27,0.365,0.355,0.355,0.395,0.015,0.32,0.355,0.365,0.45,0.395,greedy,Train for 100x100 batches of 8 examples,100.0,10000.0,5960.226125429999,EVALUATIONS/WEIGHTS_BACKUP/1618648568568965579#139997206476608,Batch of 8 25-cycles with expected 1,
godot,2021/04/17 10:25:28,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.829080505371094,0.0560020751953125,0.0075494904742754,1618655128299434095#139810796865344,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.79,0.19,0.71,0.415,0.195,0.23,0.46,0.405,0.365,0.34,0.335,0.255,0.225,0.275,0.215,0.0,0.025,0.015,0.215,0.315,0.295,0.065,0.015,0.02,0.565,0.05,0.08,0.055,0.05,0.025,0.0,0.135,0.015,0.23,0.415,0.44,0.165,0.025,0.025,0.565,0.07,0.175,0.13,0.1,0.1,0.905,0.625,0.81,0.56,0.34,0.345,0.58,0.77,0.69,0.34,0.68,0.615,0.61,0.67,0.585,greedy,Train for 100x100 batches of 2 examples,100.0,10000.0,1160.095804143,EVALUATIONS/WEIGHTS_BACKUP/1618655128299434095#139810796865344,Batch of 2 25-cycles with expected 1,
godot,2021/04/17 11:01:30,3-layer deep NN when constructing messages,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",52.4406416015625,-1.0839853515624995,4.198936768184467,1618657290292951601#140319413462848,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-10            2,080
|    |    |    └─ReLU: 4-11              --
=================================================================
Total params: 10,496
Trainable params: 10,496
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.555,0.0,0.26,0.085,0.005,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.77,0.0,0.445,0.41,0.115,0.035,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,greedy,Train for 10x100 batches of 2 examples,10.0,1000.0,135.537010345,EVALUATIONS/WEIGHTS_BACKUP/1618657290292951601#140319413462848,Batch of 2 25-cycles with expected 1 noise edge per node,
godot,2021/04/19 13:00:33,Trying out self-loops in top epd models,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.286217529296875,-0.0100479125976562,0.0019467940513653,1618837233093049959#140360305907520,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.3,0.0,0.045,0.0,0.0,0.0,0.0,0.0,0.0,0.015,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.845,0.0,0.0,0.0,0.0,0.0,0.17,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.845,0.0,0.0,0.0,0.0,0.0,0.5,0.0,0.09,0.015,0.0,0.0,0.0,0.0,0.0,0.015,0.0,0.0,0.0,0.0,0.0,greedy,Train for 500x100 batches of 8 examples,500.0,50000.0,7913.268660287,EVALUATIONS/WEIGHTS_BACKUP/1618837233093049959#140360305907520,8-batch of 25-cycles with self-loops and expected 1 noise edges per node,
godot,2021/04/19 13:51:50,"16-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.987140808105469,-0.0170538330078125,0.0014047907463066,1618840310747799635#139678342989632,100.0,0.001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",0.055,0.03,0.035,0.035,0.045,0.02,0.045,0.01,0.055,0.005,0.015,0.055,0.04,0.04,0.04,0.7,0.36,0.76,0.57,0.485,0.525,0.495,0.455,0.46,0.63,0.535,0.46,0.315,0.45,0.345,0.755,0.655,0.775,0.665,0.62,0.685,0.64,0.73,0.61,0.63,0.725,0.655,0.595,0.675,0.605,0.075,0.15,0.045,0.08,0.075,0.065,0.095,0.045,0.105,0.005,0.05,0.125,0.155,0.13,0.155,greedy,Train for 500x100 batches of 16 examples,500.0,50000.0,10989.691178103,EVALUATIONS/WEIGHTS_BACKUP/1618840310747799635#139678342989632,Batch of 16 25-cycles with expected 1 noise edge per node,
godot,2021/04/19 13:59:26,"16-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.82819091796875,0.0097811889648437,0.0021224319731807,1618840766389178609#139801494198080,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.115,0.22,0.08,0.1,0.105,0.145,0.15,0.115,0.1,0.01,0.155,0.205,0.195,0.25,0.175,0.69,0.415,0.77,0.655,0.57,0.56,0.55,0.615,0.545,0.925,0.55,0.455,0.425,0.39,0.43,0.735,0.545,0.8,0.69,0.645,0.625,0.6,0.68,0.65,0.925,0.63,0.535,0.545,0.525,0.565,0.125,0.29,0.11,0.14,0.125,0.17,0.18,0.15,0.14,0.01,0.19,0.265,0.255,0.29,0.205,greedy,Train for 500x100 batches of 16 examples,500.0,50000.0,11438.189882419,EVALUATIONS/WEIGHTS_BACKUP/1618840766389178609#139801494198080,Batch of 16 25-cycles with expected 1 noise edge per node,
godot,2021/04/19 15:03:59,"16-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.110929809570313,0.0390575561523437,0.0678230233168726,1618844639573187887#140370001844032,100.0,0.001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",0.065,0.045,0.06,0.05,0.07,0.085,0.07,0.095,0.09,0.015,0.1,0.06,0.08,0.07,0.025,0.7,0.22,0.69,0.525,0.47,0.43,0.41,0.455,0.42,0.91,0.395,0.375,0.285,0.26,0.325,0.75,0.59,0.74,0.69,0.63,0.605,0.615,0.66,0.63,0.91,0.625,0.665,0.57,0.575,0.615,0.08,0.24,0.065,0.075,0.135,0.155,0.125,0.165,0.145,0.015,0.19,0.15,0.225,0.26,0.195,greedy,Train for 500x100 batches of 16 examples,500.0,50000.0,15296.918615178,EVALUATIONS/WEIGHTS_BACKUP/1618844639573187887#140370001844032,Batch of 16 30-cycles with expected 1 noise edge per node,
godot,2021/04/19 15:14:23,"16-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.383658874511718,0.0093858642578124,0.0028721829568123,1618845263179885194#139857558001472,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.095,0.14,0.085,0.13,0.125,0.1,0.105,0.11,0.14,0.015,0.12,0.165,0.115,0.155,0.14,0.68,0.455,0.715,0.59,0.57,0.595,0.56,0.58,0.555,0.86,0.51,0.465,0.455,0.46,0.375,0.71,0.62,0.745,0.64,0.6,0.67,0.62,0.69,0.63,0.86,0.635,0.63,0.61,0.595,0.57,0.1,0.29,0.1,0.155,0.15,0.15,0.14,0.16,0.21,0.015,0.2,0.235,0.175,0.265,0.27,greedy,Train for 500x100 batches of 16 examples,500.0,50000.0,15919.458816937,EVALUATIONS/WEIGHTS_BACKUP/1618845263179885194#139857558001472,Batch of 16 30-cycles with expected 1 noise edge per node,
godot,2021/04/19 16:21:28,"32-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.366439819335937,0.0918732910156249,0.2144212338181965,1618849288835745931#140287671105344,100.0,0.001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",0.09,0.135,0.06,0.08,0.085,0.085,0.08,0.085,0.12,0.01,0.12,0.135,0.1,0.09,0.1,0.65,0.25,0.645,0.5,0.42,0.48,0.405,0.465,0.45,0.89,0.37,0.315,0.2,0.335,0.225,0.725,0.505,0.74,0.63,0.55,0.63,0.545,0.635,0.56,0.89,0.57,0.51,0.48,0.52,0.465,0.1,0.32,0.065,0.13,0.14,0.175,0.155,0.17,0.195,0.01,0.195,0.25,0.25,0.23,0.315,greedy,Train for 500x100 batches of 32 examples,500.0,50000.0,19965.058944373,EVALUATIONS/WEIGHTS_BACKUP/1618849288835745931#140287671105344,Batch of 32 25-cycles with expected 1 noise edge per node,
godot,2021/04/19 16:29:06,"32-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.8542238769531254,-0.0025762329101563,0.0005128644689662,1618849746440367436#139940646291264,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.055,0.08,0.075,0.1,0.085,0.07,0.055,0.055,0.01,0.065,0.045,0.075,0.115,0.075,0.715,0.15,0.745,0.605,0.59,0.56,0.555,0.615,0.545,0.935,0.545,0.445,0.37,0.325,0.275,0.76,0.56,0.785,0.68,0.65,0.645,0.66,0.735,0.7,0.935,0.715,0.65,0.61,0.58,0.555,0.105,0.18,0.095,0.13,0.11,0.105,0.14,0.105,0.095,0.01,0.1,0.105,0.13,0.18,0.195,greedy,Train for 500x100 batches of 32 examples,500.0,50000.0,20415.607279727003,EVALUATIONS/WEIGHTS_BACKUP/1618849746440367436#139940646291264,Batch of 32 25-cycles with expected 1 noise edge per node,
godot,2021/04/19 18:32:10,"32-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.1094210205078125,-0.0576723632812499,0.0304051609592654,1618857129997613682#139917394306880,100.0,0.001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",0.065,0.075,0.085,0.08,0.045,0.045,0.09,0.065,0.055,0.005,0.07,0.05,0.055,0.06,0.04,0.68,0.29,0.615,0.525,0.49,0.425,0.41,0.4,0.425,0.915,0.34,0.34,0.34,0.36,0.255,0.735,0.625,0.675,0.655,0.615,0.655,0.55,0.61,0.625,0.915,0.635,0.645,0.62,0.665,0.595,0.085,0.195,0.09,0.125,0.105,0.08,0.145,0.145,0.14,0.005,0.155,0.145,0.15,0.155,0.175,greedy,Train for 500x100 batches of 32 examples,500.0,50000.0,27812.697065876,EVALUATIONS/WEIGHTS_BACKUP/1618857129997613682#139917394306880,Batch of 32 30-cycles with expected 1 noise edge per node,
godot,2021/04/19 18:44:17,"32-size minibatches in top e-p-d models, 500 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.3013355102539066,0.0067239990234374,0.0016013653184998,1618857856973231844#140216135452480,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.085,0.075,0.065,0.045,0.06,0.065,0.07,0.075,0.0,0.135,0.075,0.05,0.095,0.105,0.635,0.44,0.7,0.63,0.6,0.555,0.52,0.57,0.535,0.905,0.535,0.55,0.44,0.475,0.395,0.695,0.635,0.735,0.675,0.64,0.64,0.625,0.66,0.66,0.905,0.64,0.65,0.615,0.635,0.585,0.105,0.135,0.095,0.09,0.065,0.11,0.08,0.085,0.1,0.0,0.16,0.1,0.105,0.14,0.15,greedy,Train for 500x100 batches of 32 examples,500.0,50000.0,28507.7403993,EVALUATIONS/WEIGHTS_BACKUP/1618857856973231844#140216135452480,Batch of 32 30-cycles with expected 1 noise edge per node,
godot,2021/04/19 19:08:27,"3-layer deep messages, 7-l deep processor, 4-batch","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.858309387207031,-0.0109525756835937,0.0054650494467267,1618859307020689396#140257528211264,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-45            2,080
|    |    |    └─ReLU: 4-46              --
=================================================================
Total params: 42,016
Trainable params: 42,016
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.08,0.065,0.045,0.08,0.075,0.05,0.075,0.065,0.075,0.01,0.07,0.06,0.035,0.055,0.065,0.705,0.28,0.74,0.595,0.56,0.565,0.52,0.47,0.425,0.91,0.49,0.39,0.405,0.36,0.325,0.745,0.58,0.755,0.655,0.63,0.715,0.645,0.635,0.615,0.91,0.675,0.64,0.67,0.625,0.64,0.09,0.24,0.07,0.125,0.09,0.085,0.135,0.14,0.135,0.01,0.14,0.17,0.15,0.175,0.2,greedy,Train for 2000x100 batches of 4 examples,2000.0,200000.0,29945.92447821,EVALUATIONS/WEIGHTS_BACKUP/1618859307020689396#140257528211264,Batch of 4 25-cycles with expected 1 noise edge per node,
godot,2021/04/19 19:34:07,Trying out self-loops in top epd models,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.069454833984375,-0.0672044067382812,0.0125602066996535,1618860847883878797#139850730637120,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.11,0.0,0.115,0.04,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.245,0.0,0.05,0.01,0.0,0.0,0.0,0.0,0.0,0.915,0.0,0.0,0.0,0.0,0.0,0.335,0.0,0.105,0.025,0.005,0.0,0.0,0.0,0.0,0.915,0.0,0.0,0.0,0.0,0.0,0.145,0.0,0.145,0.09,0.025,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,greedy,Train for 2000x100 batches of 8 examples,2000.0,200000.0,31502.656061359,EVALUATIONS/WEIGHTS_BACKUP/1618860847883878797#139850730637120,8-batch of 25-cycles with self-loops and expected 1 noise edges per node,
godot,2021/04/19 21:23:17,"3-layer deep messages, 9-l deep processor, 4-batch","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.609338287353516,0.0067858276367187,0.005710416570694,1618867397063672703#139813783992128,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    |    └─Linear: 4-45            1,056
|    |    |    └─ReLU: 4-46              --
|    |    |    └─Linear: 4-47            1,056
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-48            2,080
|    |    |    └─ReLU: 4-49              --
|    └─MaxMessagePassing: 2-8            --
|    |    └─Sequential: 3-15             --
|    |    |    └─Linear: 4-50            2,112
|    |    |    └─ReLU: 4-51              --
|    |    |    └─Linear: 4-52            1,056
|    |    |    └─ReLU: 4-53              --
|    |    |    └─Linear: 4-54            1,056
|    |    └─Sequential: 3-16             --
|    |    |    └─Linear: 4-55            2,080
|    |    |    └─ReLU: 4-56              --
|    └─MaxMessagePassing: 2-9            --
|    |    └─Sequential: 3-17             --
|    |    |    └─Linear: 4-57            2,112
|    |    |    └─ReLU: 4-58              --
|    |    └─Sequential: 3-18             --
|    |    |    └─Linear: 4-59            2,080
|    |    |    └─ReLU: 4-60              --
=================================================================
Total params: 54,624
Trainable params: 54,624
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.105,0.17,0.115,0.165,0.17,0.105,0.175,0.185,0.155,0.015,0.12,0.14,0.165,0.14,0.17,0.67,0.245,0.66,0.54,0.42,0.495,0.405,0.42,0.405,0.925,0.395,0.31,0.26,0.255,0.22,0.75,0.445,0.71,0.625,0.52,0.58,0.52,0.55,0.53,0.925,0.58,0.52,0.47,0.475,0.39,0.12,0.355,0.145,0.2,0.2,0.14,0.24,0.28,0.24,0.015,0.2,0.29,0.315,0.31,0.325,greedy,Train for 2000x100 batches of 4 examples,2000.0,200000.0,38047.108682858,EVALUATIONS/WEIGHTS_BACKUP/1618867397063672703#139813783992128,Batch of 4 25-cycles with expected 1 noise edge per node,
godot,2021/04/19 23:08:04,"16-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",4.381362670898437,-0.0406475830078125,0.0041444502007799,1618873683978528783#140105410807616,100.0,0.001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",0.05,0.04,0.11,0.075,0.105,0.025,0.1,0.045,0.055,0.015,0.055,0.055,0.035,0.03,0.045,0.62,0.105,0.545,0.435,0.38,0.315,0.27,0.27,0.25,0.89,0.27,0.145,0.165,0.18,0.09,0.72,0.425,0.59,0.6,0.47,0.51,0.44,0.54,0.465,0.89,0.52,0.455,0.41,0.405,0.415,0.06,0.265,0.135,0.095,0.155,0.11,0.215,0.185,0.15,0.015,0.155,0.195,0.21,0.245,0.275,greedy,Train for 2000x100 batches of 16 examples,2000.0,200000.0,44300.37178831,EVALUATIONS/WEIGHTS_BACKUP/1618873683978528783#140105410807616,Batch of 16 25-cycles with expected 1 noise edge per node,
godot,2021/04/19 23:26:09,"3-layer deep messages, 7-l deep processor, 8-batch","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.1977032470703124,-0.0258739624023437,0.0007269206875193,1618874769042078925#139698733610816,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-45            2,080
|    |    |    └─ReLU: 4-46              --
=================================================================
Total params: 42,016
Trainable params: 42,016
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.155,0.255,0.145,0.21,0.175,0.235,0.24,0.195,0.24,0.045,0.2,0.26,0.19,0.205,0.22,0.64,0.26,0.63,0.555,0.45,0.48,0.445,0.44,0.46,0.88,0.39,0.365,0.38,0.32,0.255,0.7,0.405,0.695,0.62,0.53,0.58,0.5,0.555,0.565,0.88,0.515,0.5,0.52,0.49,0.42,0.175,0.48,0.18,0.235,0.24,0.305,0.305,0.31,0.32,0.045,0.34,0.385,0.33,0.37,0.43,greedy,Train for 2000x100 batches of 8 examples,2000.0,200000.0,45410.05102448,EVALUATIONS/WEIGHTS_BACKUP/1618874769042078925#139698733610816,Batch of 8 25-cycles with expected 1 noise edge per node,
godot,2021/04/19 23:39:36,"16-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.5055821533203124,0.0062506713867187,0.0033020178586751,1618875576386957368#139979266955072,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.09,0.09,0.065,0.08,0.07,0.05,0.035,0.06,0.06,0.0,0.075,0.09,0.045,0.07,0.09,0.705,0.46,0.775,0.665,0.625,0.585,0.565,0.61,0.615,0.915,0.575,0.535,0.555,0.52,0.49,0.775,0.64,0.8,0.73,0.68,0.65,0.635,0.7,0.695,0.915,0.7,0.645,0.675,0.68,0.63,0.11,0.185,0.085,0.13,0.1,0.075,0.09,0.13,0.085,0.0,0.14,0.145,0.125,0.135,0.175,greedy,Train for 2000x100 batches of 16 examples,2000.0,200000.0,46232.073138659,EVALUATIONS/WEIGHTS_BACKUP/1618875576386957368#139979266955072,Batch of 16 25-cycles with expected 1 noise edge per node,
godot,2021/04/20 03:12:12,"3-layer deep messages, 9-l deep processor, 8-batch","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.9774877319335937,-0.0017055053710937,0.0079163333350198,1618888332116256963#140518865725248,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    |    └─Linear: 4-31            1,056
|    |    |    └─ReLU: 4-32              --
|    |    |    └─Linear: 4-33            1,056
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-34            2,080
|    |    |    └─ReLU: 4-35              --
|    └─MaxMessagePassing: 2-6            --
|    |    └─Sequential: 3-11             --
|    |    |    └─Linear: 4-36            2,112
|    |    |    └─ReLU: 4-37              --
|    |    |    └─Linear: 4-38            1,056
|    |    |    └─ReLU: 4-39              --
|    |    |    └─Linear: 4-40            1,056
|    |    └─Sequential: 3-12             --
|    |    |    └─Linear: 4-41            2,080
|    |    |    └─ReLU: 4-42              --
|    └─MaxMessagePassing: 2-7            --
|    |    └─Sequential: 3-13             --
|    |    |    └─Linear: 4-43            2,112
|    |    |    └─ReLU: 4-44              --
|    |    |    └─Linear: 4-45            1,056
|    |    |    └─ReLU: 4-46              --
|    |    |    └─Linear: 4-47            1,056
|    |    └─Sequential: 3-14             --
|    |    |    └─Linear: 4-48            2,080
|    |    |    └─ReLU: 4-49              --
|    └─MaxMessagePassing: 2-8            --
|    |    └─Sequential: 3-15             --
|    |    |    └─Linear: 4-50            2,112
|    |    |    └─ReLU: 4-51              --
|    |    |    └─Linear: 4-52            1,056
|    |    |    └─ReLU: 4-53              --
|    |    |    └─Linear: 4-54            1,056
|    |    └─Sequential: 3-16             --
|    |    |    └─Linear: 4-55            2,080
|    |    |    └─ReLU: 4-56              --
|    └─MaxMessagePassing: 2-9            --
|    |    └─Sequential: 3-17             --
|    |    |    └─Linear: 4-57            2,112
|    |    |    └─ReLU: 4-58              --
|    |    └─Sequential: 3-18             --
|    |    |    └─Linear: 4-59            2,080
|    |    |    └─ReLU: 4-60              --
=================================================================
Total params: 54,624
Trainable params: 54,624
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.11,0.165,0.09,0.155,0.135,0.19,0.13,0.22,0.15,0.01,0.16,0.22,0.15,0.2,0.175,0.665,0.22,0.665,0.515,0.5,0.445,0.43,0.365,0.405,0.905,0.42,0.3,0.32,0.31,0.29,0.71,0.47,0.745,0.6,0.57,0.585,0.55,0.52,0.555,0.905,0.575,0.435,0.515,0.5,0.46,0.14,0.34,0.105,0.215,0.165,0.25,0.195,0.275,0.28,0.01,0.25,0.355,0.295,0.35,0.33,greedy,Train for 2000x100 batches of 8 examples,2000.0,200000.0,58945.507745705,EVALUATIONS/WEIGHTS_BACKUP/1618888332116256963#140518865725248,Batch of 8 25-cycles with expected 1 noise edge per node,
godot,2021/04/20 03:44:20,"16-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",6.076227905273438,0.1242144775390626,0.1279184369355945,1618890260212505679#140337051391808,100.0,0.001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",0.25,0.18,0.375,0.38,0.355,0.435,0.395,0.37,0.4,0.03,0.31,0.25,0.25,0.245,0.155,0.485,0.035,0.375,0.285,0.195,0.125,0.155,0.175,0.13,0.895,0.12,0.065,0.055,0.04,0.035,0.545,0.125,0.435,0.36,0.255,0.21,0.25,0.27,0.21,0.895,0.19,0.155,0.145,0.135,0.17,0.275,0.64,0.4,0.455,0.505,0.58,0.495,0.525,0.59,0.03,0.57,0.58,0.585,0.59,0.6,greedy,Train for 2000x100 batches of 16 examples,2000.0,200000.0,60867.558688101,EVALUATIONS/WEIGHTS_BACKUP/1618890260212505679#140337051391808,Batch of 16 30-cycles with expected 1 noise edge per node,
godot,2021/04/20 04:23:56,"16-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.92289990234375,0.0609684448242187,0.0112169531566124,1618892635747789232#140591453947712,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.11,0.045,0.03,0.07,0.07,0.08,0.04,0.06,0.025,0.01,0.045,0.065,0.06,0.035,0.04,0.72,0.49,0.77,0.61,0.615,0.585,0.605,0.625,0.645,0.9,0.64,0.59,0.51,0.525,0.5,0.765,0.75,0.825,0.73,0.745,0.755,0.74,0.775,0.8,0.9,0.795,0.78,0.755,0.765,0.785,0.12,0.17,0.05,0.095,0.085,0.1,0.065,0.11,0.04,0.01,0.09,0.115,0.15,0.13,0.11,greedy,Train for 2000x100 batches of 16 examples,2000.0,200000.0,63277.660657010005,EVALUATIONS/WEIGHTS_BACKUP/1618892635747789232#140591453947712,Batch of 16 30-cycles with expected 1 noise edge per node,
godot,2021/04/20 08:40:20,"32-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",3.950502075195312,-0.036808349609375,0.0010111136814376,1618908019252810086#140609348495168,100.0,0.001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",0.09,0.16,0.095,0.16,0.115,0.195,0.155,0.15,0.18,0.045,0.18,0.215,0.105,0.17,0.195,0.635,0.31,0.655,0.54,0.49,0.405,0.41,0.43,0.465,0.875,0.445,0.355,0.375,0.295,0.265,0.69,0.525,0.71,0.64,0.595,0.555,0.535,0.625,0.605,0.875,0.6,0.535,0.56,0.555,0.51,0.12,0.33,0.11,0.185,0.155,0.265,0.235,0.235,0.25,0.045,0.255,0.335,0.3,0.29,0.37,greedy,Train for 2000x100 batches of 32 examples,2000.0,200000.0,78645.134623096,EVALUATIONS/WEIGHTS_BACKUP/1618908019252810086#140609348495168,Batch of 32 25-cycles with expected 1 noise edge per node,
godot,2021/04/20 09:59:34,"32-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.44405419921875,-0.0030291748046875,0.0004091304924394,1618912774361574616#140623455012672,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,0.06,0.09,0.085,0.07,0.06,0.08,0.05,0.055,0.01,0.09,0.055,0.08,0.08,0.09,0.69,0.585,0.755,0.65,0.625,0.615,0.59,0.63,0.61,0.935,0.605,0.575,0.535,0.58,0.505,0.74,0.655,0.78,0.695,0.65,0.635,0.62,0.65,0.665,0.935,0.655,0.635,0.57,0.64,0.59,0.165,0.09,0.1,0.11,0.1,0.085,0.12,0.105,0.1,0.01,0.14,0.085,0.14,0.11,0.145,greedy,Train for 2000x100 batches of 32 examples,2000.0,200000.0,83341.20916810799,EVALUATIONS/WEIGHTS_BACKUP/1618912774361574616#140623455012672,Batch of 32 25-cycles with expected 1 noise edge per node,
godot,2021/04/20 13:42:29,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",34.77,0.1200000000000002,2.3715375000001586,1618926149151618068#140222363711296,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.905,0.015,0.835,0.64,0.565,0.56,0.46,0.38,0.27,0.995,0.245,0.14,0.07,0.055,0.045,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.935,0.66,0.88,0.815,0.755,0.805,0.705,0.755,0.71,0.995,0.72,0.69,0.68,0.77,0.695,greedy,Trains 20 iterations on 20 examples. 4-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,20.0,20.0,171.715318363,EVALUATIONS/WEIGHTS_BACKUP/1618926149151618068#140222363711296,"ER(20, 0.2943611031936029)",4.0
godot,2021/04/20 13:43:15,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",42.86750000000001,-0.6950000000000003,1.1904124999991836,1618926195304558400#139654870853440,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.85,0.0,0.705,0.36,0.24,0.135,0.06,0.04,0.035,0.965,0.0,0.0,0.0,0.0,0.0,0.025,0.0,0.015,0.0,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.015,0.005,0.0,0.0,0.0,0.0,0.005,0.03,0.005,0.0,0.0,0.0,0.0,0.9,0.175,0.84,0.755,0.635,0.645,0.55,0.57,0.46,0.965,0.475,0.34,0.32,0.225,0.17,greedy,Trains 20 iterations on 20 examples. 4-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,20.0,20.0,217.870732618,EVALUATIONS/WEIGHTS_BACKUP/1618926195304558400#139654870853440,"ER(25, 0.24532699947978218)",4.0
godot,2021/04/20 13:45:57,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",39.39,0.5800000000000012,0.5346343749997686,1618926357576250053#139942835828544,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.795,0.0,0.475,0.19,0.1,0.05,0.01,0.015,0.0,0.97,0.0,0.0,0.0,0.0,0.0,0.005,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.005,0.015,0.005,0.01,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.895,0.01,0.725,0.675,0.41,0.47,0.27,0.27,0.2,0.97,0.23,0.115,0.05,0.05,0.035,greedy,Trains 20 iterations on 20 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,20.0,20.0,380.06836768,EVALUATIONS/WEIGHTS_BACKUP/1618926357576250053#139942835828544,"ER(20, 0.2943611031936029)",8.0
godot,2021/04/20 13:47:31,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",46.05875,0.1237500000000011,0.0494156250006199,1618926451670048852#139644235339584,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.48,0.0,0.185,0.045,0.03,0.005,0.0,0.0,0.0,0.99,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.005,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.715,0.0,0.415,0.335,0.18,0.12,0.08,0.085,0.03,0.99,0.03,0.005,0.015,0.015,0.01,greedy,Trains 20 iterations on 20 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,20.0,20.0,474.107350381,EVALUATIONS/WEIGHTS_BACKUP/1618926451670048852#139644235339584,"ER(25, 0.24532699947978218)",8.0
godot,2021/04/20 13:53:30,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",73.15625,0.1143749999999983,0.2390117187487703,1618926810086895756#140303159150400,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,0.0,0.03,0.01,0.01,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.09,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.625,0.0,0.0,0.0,0.0,0.0,0.315,0.0,0.09,0.03,0.01,0.02,0.0,0.0,0.0,0.625,0.0,0.0,0.0,0.0,0.0,0.27,0.0,0.135,0.225,0.055,0.055,0.015,0.01,0.0,0.02,0.005,0.005,0.0,0.0,0.0,greedy,Trains 20 iterations on 20 examples. 16-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,20.0,20.0,832.315115108,EVALUATIONS/WEIGHTS_BACKUP/1618926810086895756#140303159150400,"ER(20, 0.2943611031936029)",16.0
godot,2021/04/20 13:56:21,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",43.5925,0.1012500000000002,100.17380703124992,1618926981674769045#140173927917376,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.54,0.0,0.26,0.105,0.035,0.015,0.005,0.005,0.0,0.89,0.0,0.0,0.0,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.095,0.0,0.0,0.0,0.0,0.0,0.035,0.0,0.015,0.02,0.005,0.01,0.0,0.0,0.0,0.095,0.0,0.0,0.0,0.0,0.0,0.765,0.005,0.425,0.41,0.225,0.285,0.105,0.12,0.06,0.89,0.08,0.06,0.02,0.03,0.01,greedy,Trains 20 iterations on 20 examples. 16-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,20.0,20.0,1003.796896948,EVALUATIONS/WEIGHTS_BACKUP/1618926981674769045#140173927917376,"ER(25, 0.24532699947978218)",16.0
godot,2021/04/20 13:56:52,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",36.6625,4.452499999999999,7.975250000000187,1618927012003159141#140136109082432,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.905,0.37,0.9,0.775,0.74,0.73,0.725,0.665,0.665,0.945,0.65,0.53,0.5,0.435,0.37,0.015,0.0,0.01,0.015,0.0,0.0,0.0,0.0,0.005,0.045,0.0,0.0,0.0,0.0,0.0,0.015,0.0,0.01,0.015,0.0,0.0,0.0,0.0,0.005,0.045,0.0,0.005,0.0,0.0,0.0,0.915,0.78,0.915,0.84,0.775,0.81,0.8,0.805,0.805,0.945,0.815,0.765,0.76,0.785,0.75,greedy,Trains 20 iterations on 100 examples. 4-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,1034.1503931,EVALUATIONS/WEIGHTS_BACKUP/1618927012003159141#140136109082432,"ER(20, 0.2943611031936029)",4.0
godot,2021/04/20 14:00:50,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",38.3975,0.0,66.26621250000039,1618927250500268349#140006409852736,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.785,0.0,0.405,0.215,0.07,0.03,0.015,0.005,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.915,0.025,0.71,0.66,0.37,0.415,0.23,0.22,0.125,1.0,0.13,0.15,0.085,0.035,0.03,greedy,Trains 20 iterations on 100 examples. 4-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,1272.44873217,EVALUATIONS/WEIGHTS_BACKUP/1618927250500268349#140006409852736,"ER(25, 0.24532699947978218)",4.0
godot,2021/04/20 14:13:53,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",71.4725,-0.0087500000000005,0.0686500000001615,1618928033394240445#140135227356992,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,0.0,0.06,0.01,0.0,0.005,0.0,0.0,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.11,0.0,0.015,0.015,0.0,0.0,0.0,0.0,0.0,0.24,0.0,0.0,0.0,0.0,0.0,0.365,0.0,0.09,0.085,0.04,0.025,0.0,0.0,0.0,0.24,0.005,0.0,0.0,0.005,0.0,0.255,0.0,0.115,0.15,0.075,0.065,0.03,0.04,0.01,0.03,0.02,0.0,0.0,0.0,0.0,greedy,Trains 20 iterations on 100 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,2054.683222633,EVALUATIONS/WEIGHTS_BACKUP/1618928033394240445#140135227356992,"ER(20, 0.2943611031936029)",8.0
godot,2021/04/20 14:22:54,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",89.43499999999999,0.00625,0.1168531250023079,1618928574933422249#140010970105664,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.13,0.0,0.055,0.015,0.0,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.07,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.425,0.0,0.0,0.0,0.0,0.0,0.225,0.0,0.025,0.01,0.0,0.005,0.0,0.0,0.0,0.425,0.0,0.0,0.0,0.0,0.0,0.275,0.0,0.14,0.115,0.035,0.02,0.015,0.0,0.0,0.02,0.0,0.0,0.005,0.0,0.0,greedy,Trains 20 iterations on 100 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,2596.260200271,EVALUATIONS/WEIGHTS_BACKUP/1618928574933422249#140010970105664,"ER(25, 0.24532699947978218)",8.0
godot,2021/04/20 14:49:18,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",75.80875,-0.0112499999999982,0.0508531249997759,1618930158363823800#140264824956736,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,0.0,0.1,0.025,0.0,0.005,0.0,0.0,0.0,0.015,0.0,0.0,0.0,0.0,0.0,0.06,0.0,0.02,0.0,0.005,0.0,0.0,0.0,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.23,0.0,0.06,0.035,0.03,0.005,0.005,0.005,0.0,0.59,0.0,0.0,0.0,0.0,0.0,0.28,0.0,0.195,0.16,0.05,0.075,0.015,0.015,0.005,0.015,0.01,0.005,0.005,0.0,0.0,greedy,Trains 20 iterations on 100 examples. 16-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,4178.693727229,EVALUATIONS/WEIGHTS_BACKUP/1618930158363823800#140264824956736,"ER(20, 0.2943611031936029)",16.0
godot,2021/04/20 15:07:02,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",94.825,0.0262500000000017,0.007664062499316,1618931222313017741#139978288813888,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.18,0.0,0.07,0.03,0.015,0.0,0.0,0.0,0.0,0.035,0.0,0.0,0.0,0.0,0.0,0.08,0.0,0.01,0.0,0.0,0.0,0.0,0.0,0.0,0.61,0.0,0.0,0.0,0.0,0.0,0.31,0.0,0.105,0.025,0.02,0.01,0.0,0.0,0.01,0.61,0.0,0.0,0.0,0.0,0.0,0.375,0.0,0.19,0.175,0.075,0.06,0.025,0.025,0.01,0.035,0.005,0.0,0.01,0.0,0.0,greedy,Trains 20 iterations on 100 examples. 16-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,5242.569472197,EVALUATIONS/WEIGHTS_BACKUP/1618931222313017741#139978288813888,"ER(25, 0.24532699947978218)",16.0
godot,2021/04/20 15:09:44,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",63.4975,-0.2450000000000017,0.185837499999252,1618931384461496545#140181188921152,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,0.0,0.135,0.025,0.065,0.045,0.015,0.0,0.0,0.025,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.075,0.015,0.005,0.0,0.0,0.0,0.0,0.57,0.0,0.0,0.0,0.0,0.0,0.475,0.0,0.185,0.15,0.06,0.04,0.035,0.045,0.035,0.57,0.015,0.005,0.025,0.0,0.0,0.18,0.025,0.215,0.215,0.22,0.225,0.155,0.14,0.095,0.025,0.105,0.035,0.06,0.025,0.0,greedy,Trains 20 iterations on 500 examples. 4-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,5404.179594248,EVALUATIONS/WEIGHTS_BACKUP/1618931384461496545#140181188921152,"ER(20, 0.2943611031936029)",4.0
godot,2021/04/20 15:27:04,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",79.04249999999999,-0.2150000000000005,0.9553500000019994,1618932424610005467#140409878902592,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.085,0.0,0.03,0.0,0.005,0.0,0.0,0.0,0.0,0.02,0.0,0.0,0.0,0.0,0.0,0.12,0.0,0.015,0.005,0.0,0.0,0.0,0.0,0.0,0.565,0.0,0.0,0.0,0.0,0.0,0.28,0.0,0.06,0.04,0.02,0.015,0.0,0.0,0.0,0.565,0.005,0.0,0.0,0.0,0.0,0.175,0.0,0.115,0.1,0.04,0.06,0.04,0.025,0.005,0.02,0.01,0.005,0.005,0.0,0.0,greedy,Trains 20 iterations on 500 examples. 4-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,6443.864369281,EVALUATIONS/WEIGHTS_BACKUP/1618932424610005467#140409878902592,"ER(25, 0.24532699947978218)",4.0
godot,2021/04/20 16:37:24,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",71.92625000000001,-0.0962500000000005,0.0419593749975319,1618936643903198346#140331376518976,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.195,0.0,0.105,0.07,0.015,0.005,0.005,0.005,0.005,0.025,0.0,0.0,0.0,0.0,0.0,0.135,0.0,0.03,0.01,0.005,0.0,0.0,0.0,0.0,0.605,0.0,0.0,0.0,0.0,0.0,0.365,0.0,0.135,0.1,0.055,0.04,0.005,0.02,0.015,0.605,0.0,0.01,0.005,0.0,0.0,0.315,0.0,0.225,0.305,0.165,0.19,0.095,0.115,0.075,0.025,0.1,0.04,0.02,0.005,0.005,greedy,Trains 20 iterations on 500 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,10661.633194979,EVALUATIONS/WEIGHTS_BACKUP/1618936643903198346#140331376518976,"ER(20, 0.2943611031936029)",8.0
godot,2021/04/20 17:20:18,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",89.86,0.0525000000000005,0.0185562500000742,1618939218595087538#140117737703232,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.175,0.0,0.09,0.05,0.01,0.01,0.0,0.005,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.125,0.0,0.03,0.0,0.0,0.0,0.0,0.0,0.0,0.435,0.0,0.0,0.0,0.0,0.0,0.35,0.0,0.09,0.06,0.01,0.035,0.01,0.01,0.01,0.435,0.01,0.01,0.0,0.0,0.0,0.3,0.0,0.23,0.225,0.115,0.15,0.065,0.08,0.025,0.04,0.05,0.015,0.015,0.005,0.0,greedy,Trains 20 iterations on 500 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,13235.211296121,EVALUATIONS/WEIGHTS_BACKUP/1618939218595087538#140117737703232,"ER(25, 0.24532699947978218)",8.0
godot,2021/04/20 17:26:30,"32-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",5.878833984375,-0.0592947998046875,0.0083344227934816,1618939588538667007#140125208864576,100.0,0.001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)",0.17,0.065,0.15,0.13,0.15,0.135,0.18,0.125,0.12,0.015,0.15,0.125,0.15,0.105,0.08,0.555,0.035,0.48,0.335,0.33,0.31,0.19,0.205,0.19,0.865,0.185,0.145,0.095,0.105,0.085,0.635,0.295,0.56,0.525,0.46,0.5,0.38,0.46,0.42,0.865,0.44,0.43,0.315,0.34,0.355,0.19,0.465,0.17,0.205,0.2,0.18,0.31,0.235,0.26,0.015,0.27,0.315,0.37,0.355,0.335,greedy,Train for 2000x100 batches of 32 examples,2000.0,200000.0,110185.909299685,EVALUATIONS/WEIGHTS_BACKUP/1618939588538667007#140125208864576,Batch of 32 30-cycles with expected 1 noise edge per node,
godot,2021/04/20 18:59:02,"32-size minibatches in top e-p-d models, 2000 epochs","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",2.721972351074218,-0.0030266723632812,0.0007734343683738,1618945140500890008#140078402434880,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.07,0.05,0.04,0.08,0.06,0.06,0.045,0.025,0.06,0.025,0.025,0.04,0.035,0.04,0.02,0.73,0.57,0.77,0.67,0.615,0.6,0.59,0.625,0.6,0.87,0.64,0.575,0.52,0.54,0.51,0.77,0.755,0.805,0.715,0.665,0.675,0.7,0.75,0.685,0.87,0.76,0.745,0.72,0.745,0.73,0.11,0.135,0.06,0.12,0.095,0.135,0.11,0.1,0.12,0.025,0.075,0.12,0.1,0.1,0.09,greedy,Train for 2000x100 batches of 32 examples,2000.0,200000.0,115726.765623406,EVALUATIONS/WEIGHTS_BACKUP/1618945140500890008#140078402434880,Batch of 32 30-cycles with expected 1 noise edge per node,
godot,2021/04/20 19:28:12,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",75.02062500000001,-0.5806250000000006,1.431185937499322,1618946891551919540#139803536537408,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.23,0.0,0.185,0.135,0.065,0.03,0.03,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.25,0.0,0.085,0.01,0.0,0.0,0.0,0.0,0.0,0.085,0.0,0.0,0.0,0.0,0.0,0.415,0.0,0.215,0.145,0.075,0.055,0.03,0.015,0.03,0.085,0.015,0.0,0.0,0.005,0.005,0.275,0.02,0.26,0.335,0.185,0.215,0.165,0.155,0.085,0.0,0.09,0.075,0.05,0.035,0.035,greedy,Trains 20 iterations on 500 examples. 16-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,20904.628977792,EVALUATIONS/WEIGHTS_BACKUP/1618946891551919540#139803536537408,"ER(20, 0.2943611031936029)",16.0
godot,2021/04/20 21:04:44,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",93.489375,-0.009375,1.9283851562468044,1618952684641359462#140387967293248,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.23,0.0,0.06,0.01,0.005,0.0,0.0,0.0,0.0,0.045,0.0,0.0,0.0,0.0,0.0,0.04,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.475,0.0,0.0,0.0,0.0,0.0,0.21,0.0,0.035,0.005,0.005,0.005,0.0,0.0,0.0,0.475,0.0,0.0,0.0,0.0,0.0,0.41,0.0,0.235,0.15,0.06,0.035,0.015,0.01,0.0,0.045,0.005,0.0,0.0,0.0,0.0,greedy,Trains 20 iterations on 500 examples. 16-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,26694.849400764,EVALUATIONS/WEIGHTS_BACKUP/1618952684641359462#140387967293248,"ER(25, 0.24532699947978218)",16.0
godot,2021/04/21 08:23:05,Batch simulation in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",71.925,0.025,0.0662500000007639,1618993385649769192#140144894019392,1.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,0.0,0.115,0.02,0.025,0.01,0.0,0.0,0.0,0.025,0.0,0.0,0.0,0.0,0.0,0.13,0.0,0.045,0.0,0.005,0.0,0.005,0.0,0.0,0.565,0.0,0.0,0.0,0.0,0.0,0.32,0.0,0.1,0.08,0.03,0.04,0.02,0.015,0.0,0.565,0.03,0.0,0.005,0.005,0.0,0.22,0.0,0.22,0.225,0.115,0.145,0.09,0.08,0.04,0.025,0.035,0.03,0.01,0.005,0.01,greedy,Trains 1 iterations on 2000 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,2000.0,2000.0,2088.531181857,EVALUATIONS/WEIGHTS_BACKUP/1618993385649769192#140144894019392,"ER(20, 0.2943611031936029)",8.0
godot,2021/04/21 12:53:46,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",24.2475,1.9525,71.87308749999977,1619009626302349455#140333447305024,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.855,0.53,0.855,0.74,0.695,0.73,0.65,0.695,0.69,0.965,0.645,0.625,0.605,0.575,0.59,0.03,0.005,0.03,0.04,0.025,0.025,0.03,0.0,0.005,0.035,0.005,0.005,0.005,0.0,0.0,0.03,0.01,0.03,0.04,0.045,0.035,0.035,0.005,0.005,0.035,0.01,0.01,0.005,0.005,0.0,0.88,0.725,0.88,0.81,0.74,0.77,0.715,0.81,0.8,0.965,0.775,0.74,0.725,0.775,0.755,greedy,Trains 20 iterations on 20 examples. 4-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,20.0,20.0,154.816483698,EVALUATIONS/WEIGHTS_BACKUP/1619009626302349455#140333447305024,"ER(20, 0.2943611031936029)",4.0
godot,2021/04/21 12:54:07,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",22.075000000000003,-3.2475,71.8814999999999,1619009647440911765#140041517528896,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.36,0.0,0.48,0.3,0.25,0.17,0.085,0.07,0.03,0.01,0.025,0.02,0.01,0.0,0.0,0.39,0.0,0.18,0.06,0.045,0.005,0.005,0.005,0.0,0.98,0.0,0.0,0.0,0.0,0.0,0.525,0.025,0.3,0.195,0.15,0.095,0.065,0.13,0.08,0.98,0.055,0.035,0.01,0.035,0.005,0.38,0.3,0.525,0.555,0.48,0.555,0.445,0.465,0.4,0.01,0.465,0.37,0.31,0.375,0.285,greedy,Trains 20 iterations on 20 examples. 4-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,20.0,20.0,175.723834847,EVALUATIONS/WEIGHTS_BACKUP/1619009647440911765#140041517528896,"ER(25, 0.24532699947978218)",4.0
godot,2021/04/21 12:57:03,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",37.58625,0.50625,1.7389906250000422,1619009823205805763#140221203547968,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.93,0.645,0.885,0.795,0.79,0.785,0.72,0.75,0.77,1.0,0.745,0.725,0.68,0.725,0.65,0.005,0.0,0.0,0.0,0.0,0.005,0.005,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.005,0.005,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.945,0.76,0.89,0.81,0.815,0.815,0.755,0.81,0.82,1.0,0.785,0.795,0.74,0.78,0.77,greedy,Trains 20 iterations on 20 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,20.0,20.0,351.529483797,EVALUATIONS/WEIGHTS_BACKUP/1619009823205805763#140221203547968,"ER(20, 0.2943611031936029)",8.0
godot,2021/04/21 12:58:08,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",41.40375,-4.442500000000001,93.37832187500068,1619009888281128095#139772796909376,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.89,0.715,0.885,0.835,0.745,0.73,0.715,0.795,0.73,0.995,0.755,0.725,0.71,0.735,0.68,0.01,0.005,0.0,0.0,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01,0.005,0.0,0.0,0.0,0.0,0.005,0.0,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.92,0.805,0.895,0.86,0.78,0.77,0.765,0.84,0.775,0.995,0.815,0.795,0.76,0.79,0.78,greedy,Trains 20 iterations on 20 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,20.0,20.0,416.395942068,EVALUATIONS/WEIGHTS_BACKUP/1619009888281128095#139772796909376,"ER(25, 0.24532699947978218)",8.0
godot,2021/04/21 13:03:10,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",42.075,-0.6225000000000008,591.3031054687501,1619010190732886657#139802250901312,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.095,0.375,0.135,0.17,0.19,0.16,0.22,0.255,0.245,0.01,0.3,0.29,0.315,0.285,0.335,0.765,0.32,0.725,0.615,0.53,0.55,0.475,0.485,0.46,0.98,0.42,0.415,0.34,0.395,0.37,0.785,0.39,0.74,0.645,0.55,0.57,0.495,0.53,0.52,0.98,0.46,0.455,0.395,0.46,0.425,0.135,0.415,0.155,0.19,0.215,0.195,0.265,0.3,0.265,0.01,0.325,0.35,0.39,0.345,0.36,greedy,Trains 20 iterations on 20 examples. 16-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,20.0,20.0,718.861174826,EVALUATIONS/WEIGHTS_BACKUP/1619010190732886657#139802250901312,"ER(20, 0.2943611031936029)",16.0
godot,2021/04/21 13:06:21,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",34.010000000000005,-0.9150000000000006,181.31046249999963,1619010381565712763#139666337195840,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.795,0.755,0.87,0.82,0.73,0.735,0.695,0.765,0.76,0.42,0.75,0.715,0.69,0.73,0.7,0.13,0.005,0.03,0.045,0.02,0.02,0.03,0.03,0.01,0.57,0.02,0.035,0.025,0.005,0.01,0.135,0.005,0.035,0.05,0.02,0.02,0.03,0.03,0.01,0.57,0.02,0.035,0.025,0.005,0.015,0.81,0.815,0.88,0.83,0.745,0.76,0.725,0.795,0.775,0.42,0.805,0.77,0.755,0.77,0.775,greedy,Trains 20 iterations on 100 examples. 4-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,909.572046113,EVALUATIONS/WEIGHTS_BACKUP/1619010381565712763#139666337195840,"ER(20, 0.2943611031936029)",4.0
godot,2021/04/21 13:06:45,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",43.715,-0.025,88.77917734374978,1619010405611151323#139899393185600,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.895,0.63,0.86,0.765,0.7,0.685,0.685,0.73,0.67,0.945,0.715,0.66,0.63,0.625,0.64,0.025,0.005,0.025,0.015,0.005,0.015,0.015,0.0,0.015,0.045,0.0,0.005,0.0,0.005,0.0,0.025,0.005,0.025,0.015,0.005,0.02,0.015,0.0,0.015,0.045,0.0,0.005,0.0,0.005,0.0,0.905,0.77,0.87,0.8,0.735,0.735,0.77,0.825,0.755,0.945,0.79,0.77,0.74,0.77,0.79,greedy,Trains 20 iterations on 20 examples. 16-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,20.0,20.0,933.576919061,EVALUATIONS/WEIGHTS_BACKUP/1619010405611151323#139899393185600,"ER(25, 0.24532699947978218)",16.0
godot,2021/04/21 13:10:34,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",36.89,-5.5325,204.90758749999983,1619010634059247997#140579833247552,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.925,0.755,0.9,0.805,0.72,0.76,0.72,0.785,0.76,0.995,0.755,0.725,0.71,0.76,0.715,0.005,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.005,0.005,0.0,0.0,0.005,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.005,0.94,0.815,0.92,0.825,0.77,0.8,0.76,0.83,0.805,0.995,0.8,0.77,0.755,0.81,0.765,greedy,Trains 20 iterations on 100 examples. 4-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,1161.85867624,EVALUATIONS/WEIGHTS_BACKUP/1619010634059247997#140579833247552,"ER(25, 0.24532699947978218)",4.0
godot,2021/04/21 13:23:46,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",66.10625,1.4337499999999992,25.46806249999917,1619011426437208341#139832926832448,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.13,0.37,0.115,0.22,0.16,0.18,0.23,0.295,0.265,0.01,0.35,0.305,0.265,0.35,0.355,0.76,0.3,0.715,0.55,0.52,0.485,0.445,0.49,0.43,0.985,0.405,0.39,0.375,0.31,0.34,0.78,0.345,0.735,0.57,0.56,0.51,0.455,0.505,0.465,0.985,0.415,0.42,0.425,0.34,0.37,0.14,0.42,0.12,0.26,0.18,0.205,0.25,0.33,0.3,0.01,0.37,0.34,0.285,0.39,0.37,greedy,Trains 20 iterations on 100 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,1954.013223365,EVALUATIONS/WEIGHTS_BACKUP/1619011426437208341#139832926832448,"ER(20, 0.2943611031936029)",8.0
godot,2021/04/21 13:29:39,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",71.23375000000001,-1.63125,622.2220718749977,1619011779193459721#139764856608576,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.3,0.12,0.125,0.115,0.16,0.17,0.16,0.195,0.015,0.185,0.22,0.225,0.29,0.255,0.725,0.335,0.645,0.575,0.515,0.475,0.455,0.515,0.445,0.985,0.485,0.4,0.385,0.425,0.355,0.785,0.44,0.73,0.64,0.58,0.57,0.54,0.59,0.535,0.985,0.57,0.485,0.465,0.475,0.46,0.12,0.36,0.13,0.18,0.155,0.215,0.22,0.2,0.25,0.015,0.23,0.27,0.275,0.335,0.295,greedy,Trains 20 iterations on 100 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,2306.353253382,EVALUATIONS/WEIGHTS_BACKUP/1619011779193459721#139764856608576,"ER(25, 0.24532699947978218)",8.0
godot,2021/04/21 13:55:08,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",57.58375000000001,1.159375,409.0522046874994,1619013308907287035#140196873422656,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.115,0.43,0.085,0.13,0.19,0.22,0.31,0.33,0.36,0.015,0.355,0.38,0.415,0.415,0.465,0.785,0.315,0.77,0.615,0.51,0.51,0.435,0.43,0.405,0.98,0.405,0.34,0.305,0.32,0.285,0.795,0.34,0.8,0.655,0.56,0.555,0.46,0.465,0.43,0.98,0.445,0.385,0.33,0.365,0.31,0.135,0.465,0.1,0.155,0.195,0.27,0.335,0.36,0.38,0.015,0.375,0.435,0.455,0.465,0.515,greedy,Trains 20 iterations on 100 examples. 16-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,3835.375483639,EVALUATIONS/WEIGHTS_BACKUP/1619013308907287035#140196873422656,"ER(20, 0.2943611031936029)",16.0
godot,2021/04/21 14:14:27,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",83.49624999999999,0.6450000000000017,58.6446617187521,1619014467850283977#140462384518976,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.2,0.475,0.145,0.19,0.215,0.15,0.23,0.22,0.28,0.05,0.3,0.315,0.315,0.365,0.365,0.66,0.255,0.69,0.555,0.48,0.53,0.43,0.495,0.42,0.94,0.42,0.35,0.355,0.34,0.305,0.685,0.285,0.705,0.565,0.485,0.545,0.445,0.51,0.43,0.94,0.455,0.38,0.375,0.365,0.34,0.225,0.51,0.18,0.26,0.265,0.205,0.305,0.285,0.335,0.05,0.35,0.41,0.375,0.44,0.4,greedy,Trains 20 iterations on 100 examples. 16-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,4993.962471102001,EVALUATIONS/WEIGHTS_BACKUP/1619014467850283977#140462384518976,"ER(25, 0.24532699947978218)",16.0
godot,2021/04/21 14:15:31,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",66.69500000000001,-1.769999999999999,37.05709999999908,1619014531121125082#139805678741312,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.29,0.055,0.105,0.12,0.145,0.16,0.2,0.15,0.01,0.2,0.225,0.24,0.225,0.22,0.775,0.36,0.78,0.625,0.535,0.555,0.535,0.47,0.53,0.985,0.49,0.45,0.42,0.455,0.385,0.795,0.455,0.825,0.655,0.615,0.625,0.585,0.535,0.59,0.985,0.565,0.52,0.455,0.525,0.51,0.13,0.365,0.07,0.15,0.165,0.205,0.195,0.275,0.215,0.01,0.275,0.305,0.3,0.28,0.28,greedy,Trains 20 iterations on 500 examples. 4-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,5057.258227187,EVALUATIONS/WEIGHTS_BACKUP/1619014531121125082#139805678741312,"ER(20, 0.2943611031936029)",4.0
godot,2021/04/21 14:39:05,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",50.3925,-14.489999999999998,895.92085,1619015945166817157#140471359113024,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.105,0.45,0.085,0.205,0.22,0.22,0.29,0.32,0.28,0.015,0.265,0.375,0.335,0.395,0.45,0.775,0.26,0.79,0.585,0.545,0.5,0.445,0.445,0.455,0.985,0.47,0.325,0.33,0.31,0.25,0.79,0.295,0.81,0.615,0.575,0.55,0.47,0.49,0.495,0.985,0.51,0.375,0.365,0.345,0.29,0.12,0.505,0.085,0.235,0.235,0.26,0.34,0.36,0.32,0.015,0.305,0.445,0.38,0.445,0.505,greedy,Trains 20 iterations on 500 examples. 4-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,6470.289040132,EVALUATIONS/WEIGHTS_BACKUP/1619015945166817157#140471359113024,"ER(25, 0.24532699947978218)",4.0
godot,2021/04/21 15:33:21,"Batch REINFORCE train, testing embedding depth","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",45.98350000000001,-1.4477499999999992,751.9984121249995,1619019201383293300#140147715307328,100.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.125,0.345,0.15,0.135,0.135,0.16,0.165,0.205,0.235,0.01,0.225,0.235,0.24,0.285,0.33,0.79,0.38,0.735,0.635,0.57,0.535,0.52,0.52,0.495,0.975,0.525,0.48,0.42,0.46,0.36,0.8,0.405,0.745,0.655,0.59,0.555,0.53,0.545,0.51,0.975,0.54,0.51,0.43,0.48,0.395,0.135,0.405,0.155,0.18,0.195,0.205,0.21,0.285,0.27,0.01,0.25,0.27,0.295,0.33,0.4,greedy,Trains 100 iterations on 100 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,9725.170904152,EVALUATIONS/WEIGHTS_BACKUP/1619019201383293300#140147715307328,"ER(20, 0.2943611031936029)",8.0
godot,2021/04/21 15:35:33,"Batch REINFORCE train, testing embedding depth","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",73.97025,14.31075,8.943545875001291,1619019333716157139#140143079810880,100.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,0.09,0.08,0.085,0.06,0.075,0.095,0.09,0.135,0.01,0.135,0.095,0.05,0.115,0.14,0.775,0.48,0.76,0.645,0.58,0.565,0.535,0.605,0.54,0.98,0.54,0.52,0.495,0.5,0.41,0.79,0.535,0.775,0.65,0.605,0.59,0.56,0.635,0.565,0.98,0.565,0.56,0.53,0.545,0.46,0.135,0.25,0.105,0.15,0.105,0.13,0.175,0.175,0.225,0.01,0.23,0.2,0.205,0.215,0.315,greedy,Trains 100 iterations on 100 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,9856.849311344,EVALUATIONS/WEIGHTS_BACKUP/1619019333716157139#140143079810880,"ER(20, 0.2943611031936029)",8.0
godot,2021/04/21 15:40:10,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",64.75625,1.4412500000000024,513.1178281250013,1619019610963067867#140712176715584,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.285,0.09,0.1,0.14,0.14,0.18,0.145,0.15,0.01,0.18,0.205,0.135,0.245,0.275,0.795,0.415,0.785,0.665,0.575,0.575,0.52,0.595,0.56,0.98,0.52,0.435,0.505,0.42,0.39,0.82,0.475,0.805,0.685,0.615,0.625,0.54,0.63,0.595,0.98,0.57,0.48,0.555,0.465,0.435,0.115,0.335,0.095,0.13,0.165,0.175,0.23,0.215,0.215,0.01,0.245,0.27,0.165,0.325,0.365,greedy,Trains 20 iterations on 500 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,10134.461720147,EVALUATIONS/WEIGHTS_BACKUP/1619019610963067867#140712176715584,"ER(20, 0.2943611031936029)",8.0
godot,2021/04/21 15:41:56,"Batch REINFORCE train, testing embedding depth","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",29.68375,-8.019499999999999,687.3492206250003,1619019716618631780#139993508734784,100.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.4,0.58,0.36,0.365,0.425,0.425,0.42,0.485,0.54,0.015,0.53,0.575,0.475,0.555,0.505,0.5,0.145,0.53,0.465,0.3,0.32,0.29,0.29,0.23,0.985,0.225,0.115,0.21,0.19,0.185,0.505,0.18,0.535,0.475,0.315,0.33,0.3,0.3,0.23,0.985,0.235,0.15,0.22,0.195,0.225,0.41,0.63,0.365,0.415,0.445,0.45,0.47,0.51,0.555,0.015,0.555,0.65,0.52,0.585,0.57,greedy,Trains 100 iterations on 100 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,10240.383009502,EVALUATIONS/WEIGHTS_BACKUP/1619019716618631780#139993508734784,"ER(20, 0.2943611031936029)",8.0
godot,2021/04/21 16:15:21,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",73.91625000000002,-14.297500000000005,734.9318687499972,1619021721583759063#140352493172544,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.29,0.145,0.205,0.205,0.215,0.28,0.21,0.25,0.015,0.235,0.26,0.235,0.3,0.305,0.77,0.34,0.76,0.595,0.52,0.555,0.47,0.5,0.465,0.985,0.445,0.415,0.355,0.3,0.3,0.8,0.455,0.77,0.64,0.585,0.605,0.52,0.57,0.545,0.985,0.54,0.495,0.495,0.465,0.425,0.125,0.37,0.17,0.24,0.23,0.26,0.3,0.29,0.315,0.015,0.3,0.315,0.32,0.38,0.395,greedy,Trains 20 iterations on 500 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,12244.576330495,EVALUATIONS/WEIGHTS_BACKUP/1619021721583759063#140352493172544,"ER(25, 0.24532699947978218)",8.0
godot,2021/04/21 18:17:47,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",72.83125000000001,-1.3693749999999991,15.077492187499956,1619029067617507833#140577090684736,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.1,0.255,0.095,0.12,0.185,0.145,0.165,0.2,0.2,0.01,0.185,0.245,0.275,0.25,0.3,0.76,0.4,0.755,0.62,0.54,0.535,0.515,0.5,0.515,0.985,0.505,0.455,0.33,0.43,0.335,0.78,0.48,0.775,0.67,0.575,0.61,0.56,0.57,0.565,0.985,0.575,0.505,0.425,0.51,0.44,0.125,0.315,0.105,0.17,0.195,0.185,0.215,0.235,0.245,0.01,0.23,0.295,0.335,0.31,0.375,greedy,Trains 20 iterations on 500 examples. 16-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,19585.989781043,EVALUATIONS/WEIGHTS_BACKUP/1619029067617507833#140577090684736,"ER(20, 0.2943611031936029)",16.0
godot,2021/04/21 19:34:44,Batch simulation (fixed) in REINFOCE train,"Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",79.27625,15.056875,762.3249984374997,1619033684932328813#139802622744384,20.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.115,0.1,0.095,0.085,0.095,0.09,0.115,0.135,0.125,0.01,0.16,0.095,0.09,0.175,0.12,0.79,0.43,0.765,0.67,0.565,0.615,0.54,0.56,0.54,0.98,0.505,0.51,0.435,0.385,0.355,0.8,0.435,0.775,0.68,0.585,0.62,0.55,0.575,0.545,0.98,0.525,0.545,0.465,0.41,0.38,0.135,0.36,0.115,0.1,0.15,0.12,0.2,0.225,0.235,0.01,0.29,0.25,0.28,0.38,0.415,greedy,Trains 20 iterations on 500 examples. 16-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,24202.266489654,EVALUATIONS/WEIGHTS_BACKUP/1619033684932328813#139802622744384,"ER(25, 0.24532699947978218)",16.0
godot,2021/04/22 03:04:06,"Batch REINFORCE train, testing embedding depth","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",61.56175,8.141499999999999,328.78502287499896,1619060646748331415#140460587059008,100.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.13,0.28,0.13,0.11,0.125,0.145,0.16,0.175,0.125,0.015,0.17,0.21,0.23,0.21,0.275,0.795,0.345,0.78,0.69,0.555,0.57,0.525,0.54,0.57,0.985,0.58,0.465,0.385,0.385,0.315,0.85,0.64,0.85,0.86,0.76,0.745,0.74,0.74,0.765,0.985,0.74,0.71,0.68,0.69,0.595,0.13,0.32,0.13,0.11,0.16,0.17,0.195,0.2,0.17,0.015,0.19,0.245,0.245,0.255,0.32,greedy,Trains 100 iterations on 500 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,51148.809240537,EVALUATIONS/WEIGHTS_BACKUP/1619060646748331415#140460587059008,"ER(20, 0.2943611031936029)",8.0
godot,2021/04/22 03:07:40,"Batch REINFORCE train, testing embedding depth","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",35.83075,-11.578,708.811705375,1619060860619408833#139720077846336,100.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.265,0.455,0.175,0.26,0.275,0.25,0.27,0.295,0.305,0.265,0.365,0.35,0.255,0.365,0.375,0.64,0.275,0.725,0.57,0.495,0.485,0.465,0.49,0.44,0.735,0.405,0.37,0.4,0.38,0.33,0.655,0.29,0.745,0.59,0.53,0.52,0.49,0.495,0.445,0.735,0.43,0.385,0.43,0.405,0.345,0.28,0.55,0.2,0.285,0.285,0.275,0.305,0.35,0.36,0.265,0.4,0.43,0.32,0.405,0.455,greedy,Trains 100 iterations on 500 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,51362.740811681,EVALUATIONS/WEIGHTS_BACKUP/1619060860619408833#139720077846336,"ER(20, 0.2943611031936029)",8.0
godot,2021/04/22 03:21:40,"Batch REINFORCE train, testing embedding depth","Erdos_Renyi(50,0138).pt,Erdos_Renyi(40,0166).pt,Erdos_Renyi(45,0150).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(60,0118).pt,Erdos_Renyi(70,0104).pt,Erdos_Renyi(80,0093).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(110,0071).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(90,0084).pt,Erdos_Renyi(25,0245).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(5,0896).pt,Erdos_Renyi(35,0185).pt,Erdos_Renyi(100,0077).pt",62.73725,0.6762499999999989,231.3427858749997,1619061700583746807#139672672356160,100.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
======================================================================
Total params: 6,540
Trainable params: 6,540
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.165,0.655,0.15,0.22,0.295,0.29,0.365,0.345,0.36,0.055,0.395,0.475,0.505,0.53,0.535,0.77,0.21,0.77,0.62,0.49,0.5,0.425,0.475,0.435,0.945,0.42,0.34,0.265,0.285,0.27,0.795,0.26,0.8,0.75,0.63,0.61,0.545,0.58,0.56,0.945,0.515,0.445,0.38,0.365,0.345,0.18,0.69,0.165,0.235,0.315,0.315,0.38,0.365,0.385,0.055,0.43,0.495,0.56,0.575,0.585,greedy,Trains 100 iterations on 500 examples. 8-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,500.0,500.0,52206.285704776,EVALUATIONS/WEIGHTS_BACKUP/1619061700583746807#139672672356160,"ER(20, 0.2943611031936029)",8.0
fbosnic-FER,2021/04/26 16:09:44,Testing supervised train,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",21.359580078125,-0.308819580078125,0.2118899750761897,1619446184060922118#140187422996288,5.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.565,,0.19,0.065,0.005,0.0,,,,0.98,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.785,,0.4,0.185,0.06,0.005,,,,0.98,,,,,,greedy,Train for 10x5 batches of 1 examples,10.0,50.0,18.588881200000003,EVALUATIONS/WEIGHTS_BACKUP/1619446184060922118#140187422996288,10-cycles with expected 1 extra edges per node,1.0
fbosnic-FER,2021/04/26 16:10:49,Testing supervised train,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",20.99326080322266,-0.3446130371093751,0.2718584789590181,1619446249195768827#140222492546880,5.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.48,,0.32,0.135,0.07,0.075,,,,0.98,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.675,,0.505,0.39,0.25,0.3,,,,0.98,,,,,,greedy,Train for 10x5 batches of 1 examples,10.0,50.0,5.159764192,EVALUATIONS/WEIGHTS_BACKUP/1619446249195768827#140222492546880,10-cycles with expected 1 extra edges per node,1.0
fbosnic-FER,2021/04/26 16:27:55,Testing supervised train,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",1.9518481445312503,-0.0627604980468749,0.0306744030250132,1619447275262865901#140284050663232,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.16,,0.085,0.125,0.2,0.175,,,,0.01,,,,,,0.6,,0.345,0.145,0.1,0.09,,,,0.9,,,,,,0.65,,0.445,0.235,0.135,0.16,,,,0.9,,,,,,0.18,,0.115,0.255,0.34,0.345,,,,0.01,,,,,,greedy,Train for 100x100 batches of 1 examples,100.0,10000.0,916.360625064,EVALUATIONS/WEIGHTS_BACKUP/1619447275262865901#140284050663232,10-cycles with expected 1 extra edges per node,1.0
fbosnic-FER,2021/04/27 11:14:20,TESTING batch in supervised,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",0.808050241470337,-0.0273052024841308,0.0020322235937366,1619514860210484311#140245470431040,10.0,0.0001,Teacher forcing supervised loss. Minimizes mse between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,mse supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.31,,0.0,0.0,0.0,0.0,,,,0.995,,,,,,0.195,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.33,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.53,,0.005,0.0,0.0,0.0,,,,0.995,,,,,,greedy,Train for 10x10 batches of 8 examples,10.0,100.0,37.410847757,EVALUATIONS/WEIGHTS_BACKUP/1619514860210484311#140245470431040,Batch of 8 10-cycles with expected 1 noise edge per node,8.0
fbosnic-FER,2021/04/27 11:15:17,TESTING deep messages supervised,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",17.424592590332033,-1.1543524169921875,2.9308535674847462,1619514917584873629#140245470431040,10.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.15,,0.0,0.0,0.0,0.0,,,,1.0,,,,,,0.09,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.09,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.25,,0.005,0.0,0.0,0.0,,,,1.0,,,,,,greedy,Train for 10x10 batches of 8 examples,10.0,100.0,45.972009546,EVALUATIONS/WEIGHTS_BACKUP/1619514917584873629#140245470431040,Batch of 8 10-cycles with expected 1 noise edge per node,8.0
fbosnic-FER,2021/04/27 11:16:05,TESTING salf-loops supervised,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",17.141348876953124,-1.329518737792969,4.675289754593393,1619514965224931401#140245470431040,10.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.125,,0.0,0.0,0.0,0.0,,,,0.99,,,,,,0.115,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.23,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.305,,0.0,0.0,0.0,0.0,,,,0.99,,,,,,greedy,Train for 10x10 batches of 8 examples,10.0,100.0,35.36339182699999,EVALUATIONS/WEIGHTS_BACKUP/1619514965224931401#140245470431040,8-batch of 10-cycles with self-loops and expected 1 noise edges per node,8.0
fbosnic-FER,2021/04/27 11:16:51,TESTING embed process supervised,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",20.469548339843747,-0.3010360717773437,0.1948727746396343,1619515010980725110#140245470431040,10.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.135,,0.135,0.09,0.06,0.065,,,,0.005,,,,,,0.46,,0.25,0.075,0.06,0.04,,,,0.855,,,,,,0.705,,0.47,0.42,0.29,0.275,,,,0.855,,,,,,0.17,,0.2,0.235,0.205,0.26,,,,0.005,,,,,,greedy,Train for 10x10 batches of 1 examples,10.0,100.0,35.69544175799999,EVALUATIONS/WEIGHTS_BACKUP/1619515010980725110#140245470431040,10-cycles with expected 1 extra edges per node,
fbosnic-FER,2021/04/27 11:19:29,TESTING embed process supervised,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",21.07493408203125,-0.2736581420898439,0.1606757155037144,1619515169478241483#139948145149760,10.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.07,,0.1,0.075,0.065,0.03,,,,0.01,,,,,,0.385,,0.245,0.095,0.045,0.015,,,,0.86,,,,,,0.745,,0.57,0.44,0.37,0.405,,,,0.86,,,,,,0.1,,0.125,0.245,0.145,0.19,,,,0.01,,,,,,greedy,Train for 10x10 batches of 1 examples,10.0,100.0,36.25391686,EVALUATIONS/WEIGHTS_BACKUP/1619515169478241483#139948145149760,10-cycles with expected 1 extra edges per node,
fbosnic-FER,2021/04/27 11:21:34,TESTING embed process supervised,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",20.870260009765623,-0.355337219238281,0.2819066898335336,1619515294234985443#140716017690432,10.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.175,,0.13,0.07,0.035,0.04,,,,0.025,,,,,,0.28,,0.14,0.04,0.01,0.005,,,,0.86,,,,,,0.56,,0.32,0.25,0.125,0.12,,,,0.86,,,,,,0.22,,0.24,0.285,0.24,0.225,,,,0.025,,,,,,greedy,Train for 10x10 batches of 1 examples,10.0,100.0,37.728338832,EVALUATIONS/WEIGHTS_BACKUP/1619515294234985443#140716017690432,10-cycles with expected 1 extra edges per node,
fbosnic-FER,2021/04/27 11:38:37,TESTING embed process supervised,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",19.94103088378906,-0.4794784545898437,0.508622362933238,1619516317285672016#139968976549696,10.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Embed-Process model trained on noisy cycles,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.12,,0.125,0.075,0.05,0.02,,,,0.01,,,,,,0.35,,0.225,0.07,0.02,0.015,,,,0.89,,,,,,0.705,,0.44,0.355,0.225,0.2,,,,0.89,,,,,,0.17,,0.195,0.24,0.18,0.195,,,,0.01,,,,,,greedy,Train for 10x10 batches of 1 examples,10.0,100.0,35.773159086,EVALUATIONS/WEIGHTS_BACKUP/1619516317285672016#139968976549696,10-cycles with expected 1 extra edges per node,
fbosnic-FER,2021/04/27 11:39:58,TESTING batch reinforcement,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",-29.8,1.8,24.959999999999923,1619516398599809293#139968976549696,10.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.655,,0.395,0.185,0.095,0.045,,,,0.97,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.775,,0.585,0.42,0.205,0.255,,,,0.97,,,,,,greedy,Trains 10 iterations on 10 examples. 1-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,10.0,10.0,33.17976217499999,EVALUATIONS/WEIGHTS_BACKUP/1619516398599809293#139968976549696,"ER(10, 0.5151730583335019)",1.0
fbosnic-FER,2021/04/27 12:16:28,TESTING basic reinforcement,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",3.4800000000000004,-0.0399999999999999,0.1855999999999955,1619518588230870977#140329498363712,10.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.72,,0.37,0.15,0.07,0.03,,,,0.995,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.9,,0.665,0.56,0.385,0.435,,,,0.995,,,,,,greedy,Trains 10 iterations on 10 examples. 1-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,10.0,10.0,9.292023983,EVALUATIONS/WEIGHTS_BACKUP/1619518588230870977#140329498363712,"ER(10, 0.5151730583335019)",1.0
fbosnic-FER,2021/04/27 12:16:51,TESTING batch reinforcement,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",3.38,0.1,0.4616000000000007,1619518611527058096#140329498363712,10.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.795,,0.45,0.285,0.135,0.07,,,,0.995,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.91,,0.705,0.63,0.455,0.415,,,,0.995,,,,,,greedy,Trains 10 iterations on 10 examples. 1-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,10.0,10.0,8.812354458999998,EVALUATIONS/WEIGHTS_BACKUP/1619518611527058096#140329498363712,"ER(10, 0.5151730583335019)",1.0
fbosnic-FER,2021/04/27 12:22:00,TESTING gated reinforced,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",3.88,0.2,0.3615999999999992,1619518920599432721#139860417124160,10.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.86,,0.81,0.67,0.59,0.51,,,,1.0,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.88,,0.825,0.7,0.65,0.565,,,,1.0,,,,,,greedy,Trains 10 iterations on 10 examples. 1-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,10.0,10.0,74.838349761,EVALUATIONS/WEIGHTS_BACKUP/1619518920599432721#139860417124160,"ER(10, 0.5151730583335019)",1.0
fbosnic-FER,2021/04/28 11:48:57,TESTING basic supervised,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",12.63319723510742,0.2500805053710928,9.188384981863775,1619603337233044741#140437572892480,5.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.07,,0.0,0.0,0.0,0.0,,,,0.975,,,,,,0.07,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.07,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.15,,0.0,0.0,0.0,0.0,,,,0.975,,,,,,greedy,Train for 200x5 batches of 1 examples,200.0,1000.0,28.207629562,EVALUATIONS/WEIGHTS_BACKUP/1619603337233044741#140437572892480,Batch of 1 10-cycles with expected 1 noise edge per node,1.0
fbosnic-FER,2021/04/28 11:49:00,TESTING supervised with attention and edge features,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",3.9891609086990334,-1.2442462263107288,4.321110704710391,1619603340874420675#140258815354688,5.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (node hidden dim=32, edge attributes dim=16): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─AttentionMPNNWithEdgeFeatures: 2-1     --
|    |    └─Sequential: 3-1                   --
|    |    |    └─Linear: 4-1                  2,592
|    |    |    └─ReLU: 4-2                    --
|    |    |    └─Dropout: 4-3                 --
|    |    |    └─Linear: 4-4                  1,056
|    |    |    └─ReLU: 4-5                    --
|    |    |    └─Dropout: 4-6                 --
|    |    |    └─Linear: 4-7                  1,056
|    |    └─Sequential: 3-2                   --
|    |    |    └─Linear: 4-8                  1,296
|    |    |    └─ReLU: 4-9                    --
|    |    |    └─Dropout: 4-10                --
|    |    |    └─Linear: 4-11                 272
|    |    |    └─ReLU: 4-12                   --
|    |    |    └─Dropout: 4-13                --
|    |    |    └─Linear: 4-14                 272
|    |    └─Linear: 3-3                       81
|    └─AttentionMPNNWithEdgeFeatures: 2-2     --
|    |    └─Sequential: 3-4                   --
|    |    |    └─Linear: 4-15                 2,592
|    |    |    └─ReLU: 4-16                   --
|    |    |    └─Dropout: 4-17                --
|    |    |    └─Linear: 4-18                 1,056
|    |    |    └─ReLU: 4-19                   --
|    |    |    └─Dropout: 4-20                --
|    |    |    └─Linear: 4-21                 1,056
|    |    └─Sequential: 3-5                   --
|    |    |    └─Linear: 4-22                 1,296
|    |    |    └─ReLU: 4-23                   --
|    |    |    └─Dropout: 4-24                --
|    |    |    └─Linear: 4-25                 272
|    |    |    └─ReLU: 4-26                   --
|    |    |    └─Dropout: 4-27                --
|    |    |    └─Linear: 4-28                 272
|    |    └─Linear: 3-6                       81
======================================================================
Total params: 13,250
Trainable params: 13,250
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.255,,0.145,0.0,0.0,0.0,,,,0.805,,,,,,0.515,,0.005,0.0,0.0,0.0,,,,0.185,,,,,,0.61,,0.06,0.0,0.0,0.0,,,,0.185,,,,,,0.3,,0.22,0.01,0.0,0.0,,,,0.805,,,,,,greedy,Train for 200x5 batches of 1 examples,200.0,1000.0,31.399988615,EVALUATIONS/WEIGHTS_BACKUP/1619603340874420675#140258815354688,Batch of 1 10-cycles with expected 1 noise edge per node,1.0
fbosnic-FER,2021/04/28 11:48:56,TESTING neighbor masked supervised,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",2.859729551315308,-0.1282114696502685,0.3186251599567509,1619603336865931570#139622741808960,5.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.345,,0.025,0.0,0.0,0.0,,,,1.0,,,,,,0.435,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.44,,0.02,0.0,0.0,0.0,,,,0.0,,,,,,0.45,,0.045,0.015,0.0,0.0,,,,1.0,,,,,,greedy,Masking neighbors! Train for 200x5 batches of 1 examples,200.0,1000.0,27.623528234,EVALUATIONS/WEIGHTS_BACKUP/1619603336865931570#139622741808960,Batch of 1 10-cycles with expected 1 noise edge per node,1.0
fbosnic-FER,2021/04/28 11:49:16,TESTING gated supervised,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",9.634958999633788,-0.9718667602539056,2.009274035204456,1619603356388977321#139710152382272,5.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.34,,0.0,0.0,0.0,0.0,,,,0.99,,,,,,0.375,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.375,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.48,,0.0,0.0,0.0,0.0,,,,0.99,,,,,,greedy,Train for 200x5 batches of 1 examples,200.0,1000.0,47.124632612,EVALUATIONS/WEIGHTS_BACKUP/1619603356388977321#139710152382272,Batch of 1 10-cycles with expected 1 noise edge per node,1.0
fbosnic-FER,2021/04/28 11:49:32,TESTING batch in supervised,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",7.489498605728148,-2.257602041244507,7.094312704038998,1619603372762100266#140437572892480,5.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.13,,0.0,0.0,0.0,0.0,,,,0.965,,,,,,0.385,,0.0,0.0,0.0,0.0,,,,0.005,,,,,,0.625,,0.0,0.0,0.0,0.0,,,,0.005,,,,,,0.195,,0.0,0.0,0.0,0.0,,,,0.965,,,,,,greedy,Train for 200x5 batches of 1 examples,200.0,1000.0,25.882396847999992,EVALUATIONS/WEIGHTS_BACKUP/1619603372762100266#140437572892480,Batch of 1 10-cycles with expected 1 noise edge per node,1.0
fbosnic-FER,2021/04/28 11:49:38,TESTING salf-loops supervised,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",4.184878311157227,-0.8334545898437509,1.76139978266119,1619603378820441062#139622741808960,5.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.27,,0.07,0.0,0.0,0.0,,,,0.015,,,,,,0.325,,0.0,0.0,0.0,0.0,,,,0.915,,,,,,0.485,,0.03,0.0,0.0,0.0,,,,0.915,,,,,,0.345,,0.115,0.0,0.0,0.0,,,,0.015,,,,,,greedy,Train for 200x5 batches of 1 examples,200.0,1000.0,28.29593022,EVALUATIONS/WEIGHTS_BACKUP/1619603378820441062#139622741808960,1-batch of 10-cycles with self-loops and expected 1 noise edges per node,1.0
fbosnic-FER,2021/04/28 11:49:44,TESTING deep messages supervised,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",10.785222396850587,-0.7605070648193378,1.7235600273052825,1619603384547069481#140258815354688,5.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    |    └─Linear: 4-3             1,056
|    |    |    └─ReLU: 4-4               --
|    |    |    └─Linear: 4-5             1,056
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-6             2,080
|    |    |    └─ReLU: 4-7               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-8             2,112
|    |    |    └─ReLU: 4-9               --
|    |    |    └─Linear: 4-10            1,056
|    |    |    └─ReLU: 4-11              --
|    |    |    └─Linear: 4-12            1,056
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-13            2,080
|    |    |    └─ReLU: 4-14              --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-15            2,112
|    |    |    └─ReLU: 4-16              --
|    |    |    └─Linear: 4-17            1,056
|    |    |    └─ReLU: 4-18              --
|    |    |    └─Linear: 4-19            1,056
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-20            2,080
|    |    |    └─ReLU: 4-21              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-22            2,112
|    |    |    └─ReLU: 4-23              --
|    |    |    └─Linear: 4-24            1,056
|    |    |    └─ReLU: 4-25              --
|    |    |    └─Linear: 4-26            1,056
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-27            2,080
|    |    |    └─ReLU: 4-28              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-29            2,112
|    |    |    └─ReLU: 4-30              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-31            2,080
|    |    |    └─ReLU: 4-32              --
=================================================================
Total params: 29,408
Trainable params: 29,408
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.27,,0.0,0.0,0.0,0.0,,,,0.99,,,,,,0.38,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.435,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.42,,0.005,0.0,0.0,0.0,,,,0.99,,,,,,greedy,Train for 200x5 batches of 1 examples,200.0,1000.0,34.609241215,EVALUATIONS/WEIGHTS_BACKUP/1619603384547069481#140258815354688,Batch of 1 10-cycles with expected 1 noise edge per node,1.0
fbosnic-FER,2021/04/28 14:10:24,"Benchmark supervised, 8-batch epd, 10-train size","Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",1.7150943145751958,-0.0179263244628909,12.376172363898142,1619611824785407310#140089073346368,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.14,,0.0,0.0,0.0,0.0,,,,0.295,,,,,,0.68,,0.0,0.0,0.0,0.0,,,,0.67,,,,,,0.755,,0.0,0.0,0.0,0.0,,,,0.67,,,,,,0.175,,0.0,0.0,0.0,0.0,,,,0.295,,,,,,greedy,Train for 500x100 batches of 1 examples,500.0,50000.0,1105.801943151,EVALUATIONS/WEIGHTS_BACKUP/1619611824785407310#140089073346368,Batch of 1 10-cycles with expected 1 noise edge per node,1.0
fbosnic-FER,2021/04/29 10:02:49,Quick gated reinforcement test,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",3.8540625,1.4528125,-14.85379775390625,1619683369768808629#140172851124032,32.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=4): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       20
|    |    └─Linear: 3-2                       20
|    |    └─Linear: 3-3                       5
|    |    └─Linear: 3-4                       5
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       20
|    |    └─Linear: 3-7                       20
|    |    └─Linear: 3-8                       5
|    |    └─Linear: 3-9                       5
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      20
|    |    └─Linear: 3-12                      20
|    |    └─Linear: 3-13                      5
|    |    └─Linear: 3-14                      5
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      20
|    |    └─Linear: 3-17                      20
|    |    └─Linear: 3-18                      5
|    |    └─Linear: 3-19                      5
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      20
|    |    └─Linear: 3-22                      20
|    |    └─Linear: 3-23                      5
|    |    └─Linear: 3-24                      5
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 260
Trainable params: 260
Non-trainable params: 0
======================================================================
processor (hidden dim=4): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       20
|    |    └─Linear: 3-2                       20
|    |    └─Linear: 3-3                       5
|    |    └─Linear: 3-4                       5
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       20
|    |    └─Linear: 3-7                       20
|    |    └─Linear: 3-8                       5
|    |    └─Linear: 3-9                       5
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      20
|    |    └─Linear: 3-12                      20
|    |    └─Linear: 3-13                      5
|    |    └─Linear: 3-14                      5
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      20
|    |    └─Linear: 3-17                      20
|    |    └─Linear: 3-18                      5
|    |    └─Linear: 3-19                      5
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      20
|    |    └─Linear: 3-22                      20
|    |    └─Linear: 3-23                      5
|    |    └─Linear: 3-24                      5
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 260
Trainable params: 260
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.18,,0.025,0.0,0.0,0.0,,,,1.0,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.545,,0.125,0.04,0.005,0.01,,,,1.0,,,,,,greedy,Trains 32 iterations on 100 examples. 1-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,100.0,100.0,641.234133981,EVALUATIONS/WEIGHTS_BACKUP/1619683369768808629#140172851124032,"ER(10, 0.5151730583335019)",1.0
fbosnic-FER,2021/04/30 04:09:40,Quick gated reinforcement test,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",33.62125,0.9427499999999964,126538.35784843832,1619748580149663918#140235434161984,10.0,0.0001,"Reinforcement loss similar to AlphaZero paper (with state-action function instead of value function).loss(\theta) = G_t ln \pi(A_t, \theta | S_t) + 1*(Q(S_t, A_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and Q(S_t) is the state-action function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.125,,0.06,0.11,0.105,0.085,,,,0.015,,,,,,0.8,,0.795,0.685,0.625,0.625,,,,0.985,,,,,,0.805,,0.815,0.705,0.68,0.67,,,,0.985,,,,,,0.14,,0.09,0.145,0.14,0.125,,,,0.015,,,,,,greedy,Trains 10 iterations on 10000 examples. 4-simulations are run and all visited states are updated. Masking to choose next step only among neighbors.,10000.0,10000.0,58249.756901361005,EVALUATIONS/WEIGHTS_BACKUP/1619748580149663918#140235434161984,"ER(10, 0.5151730583335019)",4.0
fbosnic-FER,2021/04/30 11:16:26,Testing gated reinforce again,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",-32.3243,-14.9411,-1044.8603704900002,1619774186548530405#140273754830656,100.0,0.0001,"Reinforcement loss with learnable value function as baseline.loss(\theta) = -(G_t - V(S_t, \theta)) ln \pi(A_t, \theta | S_t) + 1*(V(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and V(S_t, \theta) is the state value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.125,,0.115,0.15,0.205,0.22,,,,0.0,,,,,,0.76,,0.755,0.585,0.505,0.475,,,,0.985,,,,,,0.825,,0.845,0.8,0.705,0.74,,,,0.985,,,,,,0.13,,0.12,0.17,0.235,0.255,,,,0.0,,,,,,greedy,"Trains for 100 epochs consisting of 100 independent examples. 1 batches (size 1) simulations per example. After the batch of simulation finishes, all visited states are updated. Masking to choose next step only among neighbors.",100.0,100.0,3161.996157377,EVALUATIONS/WEIGHTS_BACKUP/1619774186548530405#140273754830656,"ER(20, 0.2943611031936029)",1.0
fbosnic-FER,2021/04/30 13:11:26,Testing gated reinforce again,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",-17.9122,-5.197200000000002,-320.84690884,1619781086018045314#139653731972928,100.0,0.0001,"Reinforcement loss with learnable value function as baseline.loss(\theta) = -(G_t - V(S_t, \theta)) ln \pi(A_t, \theta | S_t) + 1*(V(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and V(S_t, \theta) is the state value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.695,,0.865,0.79,0.8,0.715,,,,0.345,,,,,,0.25,,0.085,0.1,0.035,0.05,,,,0.655,,,,,,0.27,,0.095,0.135,0.07,0.09,,,,0.655,,,,,,0.72,,0.89,0.85,0.895,0.88,,,,0.345,,,,,,greedy,"Trains for 100 epochs consisting of 100 independent examples. 1 batches (size 1) simulations per example. After the batch of simulation finishes, all visited states are updated. Masking to choose next step only among neighbors.",100.0,100.0,3069.974124877,EVALUATIONS/WEIGHTS_BACKUP/1619781086018045314#139653731972928,"ER(20, 0.2943611031936029)",1.0
fbosnic-FER,2021/05/06 13:50:00,"Benchmark (fixed) supervised, 2-batch epd, 5-train size","Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",,,,1620301800370790504#139857905813312,10.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.0,,0.0,0.0,0.0,0.0,,,,0.175,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.54,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.54,,,,,,0.03,,0.0,0.0,0.0,0.0,,,,0.175,,,,,,greedy,Train for 5x10 batches of 2 examples,5.0,50.0,0.7154169689999996,EVALUATIONS/WEIGHTS_BACKUP/1620301800370790504#139857905813312,Batch of 2 5-cycles with expected 1 noise edge per node,2.0
fbosnic-FER,2021/05/06 13:50:03,Benchmark (fixed) gated reinforced 2-batch,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",,,,1620301803658857929#140186332981056,10.0,0.0001,"Reinforcement loss with learnable value function as baseline.loss(\theta) = -(G_t - V(S_t, \theta)) ln \pi(A_t, \theta | S_t) + 1*(V(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and V(S_t, \theta) is the state value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.59,,0.685,0.65,0.575,0.485,,,,0.03,,,,,,0.305,,0.175,0.045,0.01,0.01,,,,0.96,,,,,,0.32,,0.2,0.11,0.025,0.055,,,,0.96,,,,,,0.6,,0.69,0.75,0.76,0.725,,,,0.03,,,,,,greedy,"Trains for 10 epochs consisting of 10 independent examples. 1 batches (size 2) simulations per example. After the batch of simulation finishes, all visited states are updated. Masking to choose next step only among neighbors.",5.0,5.0,3.99923044,EVALUATIONS/WEIGHTS_BACKUP/1620301803658857929#140186332981056,"ER(5, 0.8963157236301816)",2.0
fbosnic-FER,2021/05/06 13:51:43,Minimal benckmark supervised test,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",,,,1620301903396932741#140694063773504,10.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.02,,0.01,0.0,0.0,0.0,,,,0.1,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.865,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.865,,,,,,0.02,,0.015,0.005,0.01,0.005,,,,0.1,,,,,,greedy,Train for 5x10 batches of 2 examples,5.0,50.0,0.7008078179999999,EVALUATIONS/WEIGHTS_BACKUP/1620301903396932741#140694063773504,Batch of 2 5-cycles with expected 1 noise edge per node,2.0
fbosnic-FER,2021/05/06 13:54:08,Minimal benchmark gated reinforce test,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",,,,1620302048601609391#140707239855936,10.0,0.0001,"Reinforcement loss with learnable value function as baseline.loss(\theta) = -(G_t - V(S_t, \theta)) ln \pi(A_t, \theta | S_t) + 1*(V(S_t, \theta) - G_t)^2 + 0.01*l_2(\theta). G_t is the cumulative reward of a simulation and V(S_t, \theta) is the state value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       33
|    |    └─Linear: 3-4                       33
|    |    └─Linear: 3-5                       2
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       33
|    |    └─Linear: 3-9                       33
|    |    └─Linear: 3-10                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      33
|    |    └─Linear: 3-14                      33
|    |    └─Linear: 3-15                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      33
|    |    └─Linear: 3-19                      33
|    |    └─Linear: 3-20                      2
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      33
|    |    └─Linear: 3-24                      33
|    |    └─Linear: 3-25                      2
======================================================================
Total params: 10,900
Trainable params: 10,900
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.86,,0.83,0.705,0.6,0.61,,,,0.99,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.905,,0.905,0.845,0.7,0.725,,,,0.99,,,,,,greedy,"Trains for 10 epochs consisting of 10 independent examples. 1 batches (size 2) simulations per example. After the batch of simulation finishes, all visited states are updated. Masking to choose next step only among neighbors.",5.0,5.0,4.740037884,EVALUATIONS/WEIGHTS_BACKUP/1620302048601609391#140707239855936,"ER(5, 0.8963157236301816)",2.0
fbosnic-FER,2021/05/06 13:54:04,Minimal benckmark supervised test,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",,,,1620302044610248231#139820570904384,10.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.02,,0.005,0.0,0.0,0.0,,,,0.01,,,,,,0.015,,0.01,0.0,0.0,0.0,,,,0.915,,,,,,0.035,,0.015,0.01,0.015,0.005,,,,0.915,,,,,,0.04,,0.005,0.015,0.005,0.0,,,,0.01,,,,,,greedy,Train for 5x10 batches of 2 examples,5.0,50.0,0.7140136240000001,EVALUATIONS/WEIGHTS_BACKUP/1620302044610248231#139820570904384,Batch of 2 5-cycles with expected 1 noise edge per node,2.0
fbosnic-FER,2021/05/06 14:31:42,Minimal benckmark supervised test,"Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",,,,1620304302574678180#140126289172288,10.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.005,,0.0,0.0,0.0,0.0,,,,0.005,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.03,,,,,,0.005,,0.0,0.0,0.0,0.0,,,,0.03,,,,,,0.1,,0.0,0.0,0.0,0.0,,,,0.005,,,,,,greedy,Train for 5x10 batches of 2 examples,5.0,50.0,0.704311267,EVALUATIONS/WEIGHTS_BACKUP/1620304302574678180#140126289172288,Batch of 2 5-cycles with expected 1 noise edge per node,2.0
fbosnic-FER,2021/05/18 14:33:36,"MAIN supervised model with decay, 25 train size","Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",,,,1621341216932417016#140109134235456,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.705,,0.7,0.47,0.12,0.285,,,,0.97,,,,,,0.0,,0.0,0.155,0.275,0.085,,,,0.0,,,,,,0.0,,0.0,0.16,0.47,0.275,,,,0.0,,,,,,0.775,,0.795,0.6,0.17,0.36,,,,0.97,,,,,,greedy,Train for 20x100 batches of 8 examples,20.0,2000.0,522.761554102,EVALUATIONS/WEIGHTS_BACKUP/1621341216932417016#140109134235456,Batch of 8 25-cycles with expected 3 noise edge per node,8.0
fbosnic-FER,2021/05/18 14:40:25,"MAIN supervised model, 25 train size","Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",,,,1621341625110246791#140287008761664,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.645,,0.325,0.14,0.03,0.005,,,,0.965,,,,,,0.0,,0.0,0.0,0.01,0.005,,,,0.0,,,,,,0.0,,0.0,0.0,0.055,0.035,,,,0.0,,,,,,0.81,,0.555,0.4,0.195,0.08,,,,0.965,,,,,,greedy,Train for 10x100 batches of 8 examples,10.0,1000.0,213.787824719,EVALUATIONS/WEIGHTS_BACKUP/1621341625110246791#140287008761664,Batch of 8 25-cycles with expected 3 noise edge per node,8.0
fbosnic-FER,2021/05/18 14:43:34,"MAIN supervised model with decay, 25 train size","Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",,,,1621341814198246979#140316302874432,100.0,0.0001,Teacher forcing supervised loss. Minimizes entropy between teacher and student next node distributions. Teacher follows a pregenerated hamilton cycle (no preferences) and initial student step is not scored.,entropy supervised loss,Supervised Encode-Process-Decode model trained on noisy cycles,"encoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            1,152
=================================================================
Total params: 1,152
Trainable params: 1,152
Non-trainable params: 0
=================================================================
processor (hidden dim=32): =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─ModuleList: 1-1                        --
|    └─MaxMessagePassing: 2-1            --
|    |    └─Sequential: 3-1              --
|    |    |    └─Linear: 4-1             2,112
|    |    |    └─ReLU: 4-2               --
|    |    └─Sequential: 3-2              --
|    |    |    └─Linear: 4-3             2,080
|    |    |    └─ReLU: 4-4               --
|    └─MaxMessagePassing: 2-2            --
|    |    └─Sequential: 3-3              --
|    |    |    └─Linear: 4-5             2,112
|    |    |    └─ReLU: 4-6               --
|    |    └─Sequential: 3-4              --
|    |    |    └─Linear: 4-7             2,080
|    |    |    └─ReLU: 4-8               --
|    └─MaxMessagePassing: 2-3            --
|    |    └─Sequential: 3-5              --
|    |    |    └─Linear: 4-9             2,112
|    |    |    └─ReLU: 4-10              --
|    |    └─Sequential: 3-6              --
|    |    |    └─Linear: 4-11            2,080
|    |    |    └─ReLU: 4-12              --
|    └─MaxMessagePassing: 2-4            --
|    |    └─Sequential: 3-7              --
|    |    |    └─Linear: 4-13            2,112
|    |    |    └─ReLU: 4-14              --
|    |    └─Sequential: 3-8              --
|    |    |    └─Linear: 4-15            2,080
|    |    |    └─ReLU: 4-16              --
|    └─MaxMessagePassing: 2-5            --
|    |    └─Sequential: 3-9              --
|    |    |    └─Linear: 4-17            2,112
|    |    |    └─ReLU: 4-18              --
|    |    └─Sequential: 3-10             --
|    |    |    └─Linear: 4-19            2,080
|    |    |    └─ReLU: 4-20              --
=================================================================
Total params: 20,960
Trainable params: 20,960
Non-trainable params: 0
=================================================================
decoder: =================================================================
Layer (type:depth-idx)                   Param #
=================================================================
├─Linear: 1-1                            65
=================================================================
Total params: 65
Trainable params: 65
Non-trainable params: 0
=================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.745,,0.565,0.27,0.095,0.04,,,,0.98,,,,,,0.0,,0.0,0.005,0.05,0.005,,,,0.0,,,,,,0.0,,0.0,0.005,0.135,0.08,,,,0.0,,,,,,0.865,,0.73,0.53,0.215,0.21,,,,0.98,,,,,,greedy,Train for 20x100 batches of 8 examples,20.0,2000.0,402.218442681,EVALUATIONS/WEIGHTS_BACKUP/1621341814198246979#140316302874432,Batch of 8 25-cycles with expected 3 noise edge per node,8.0
fbosnic-FER,2021/05/18 14:50:26,"Sigmoid Gated reinforced, regularization weight=0","Erdos_Renyi(5,0896).pt,Erdos_Renyi(10,0515).pt,Erdos_Renyi(20,0294).pt,Erdos_Renyi(15,0371).pt,Erdos_Renyi(30,0211).pt,Erdos_Renyi(25,0245).pt",,,,1621342226754812792#139861976233792,100.0,0.0001,"Reinforcement loss with learnable value function as baseline.loss(\theta) = -(G_t - V(S_t, \theta)) ln \pi(A_t, \theta | S_t) + 1*(V(S_t, \theta) - G_t)^2 + 0*l_2(\theta). G_t is the cumulative reward of a simulation and V(S_t, \theta) is the state value function",Reinfocement_loss,Reinforcement Embedding-Process model trained on Erdos-Renyi graphs,"embedding (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       1,056
|    |    └─Linear: 3-4                       1,056
|    |    └─Linear: 3-5                       1,056
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       1,056
|    |    └─Linear: 3-9                       1,056
|    |    └─Linear: 3-10                      1,056
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      1,056
|    |    └─Linear: 3-14                      1,056
|    |    └─Linear: 3-15                      1,056
======================================================================
Total params: 15,840
Trainable params: 15,840
Non-trainable params: 0
======================================================================
processor (hidden dim=32): ======================================================================
Layer (type:depth-idx)                        Param #
======================================================================
├─ModuleList: 1-1                             --
|    └─GatedGraphConvNetNoBatchNorm: 2-1      --
|    |    └─Linear: 3-1                       1,056
|    |    └─Linear: 3-2                       1,056
|    |    └─Linear: 3-3                       1,056
|    |    └─Linear: 3-4                       1,056
|    |    └─Linear: 3-5                       1,056
|    └─GatedGraphConvNetNoBatchNorm: 2-2      --
|    |    └─Linear: 3-6                       1,056
|    |    └─Linear: 3-7                       1,056
|    |    └─Linear: 3-8                       1,056
|    |    └─Linear: 3-9                       1,056
|    |    └─Linear: 3-10                      1,056
|    └─GatedGraphConvNetNoBatchNorm: 2-3      --
|    |    └─Linear: 3-11                      1,056
|    |    └─Linear: 3-12                      1,056
|    |    └─Linear: 3-13                      1,056
|    |    └─Linear: 3-14                      1,056
|    |    └─Linear: 3-15                      1,056
|    └─GatedGraphConvNetNoBatchNorm: 2-4      --
|    |    └─Linear: 3-16                      1,056
|    |    └─Linear: 3-17                      1,056
|    |    └─Linear: 3-18                      1,056
|    |    └─Linear: 3-19                      1,056
|    |    └─Linear: 3-20                      1,056
|    └─GatedGraphConvNetNoBatchNorm: 2-5      --
|    |    └─Linear: 3-21                      1,056
|    |    └─Linear: 3-22                      1,056
|    |    └─Linear: 3-23                      1,056
|    |    └─Linear: 3-24                      1,056
|    |    └─Linear: 3-25                      1,056
======================================================================
Total params: 26,400
Trainable params: 26,400
Non-trainable params: 0
======================================================================",Adam,"Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0001
    weight_decay: 0
)",0.765,,0.39,0.135,0.06,0.0,,,,1.0,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.0,,0.0,0.0,0.0,0.0,,,,0.0,,,,,,0.935,,0.75,0.655,0.425,0.39,,,,1.0,,,,,,greedy,"Trains for 100 epochs consisting of 100 independent examples. 1 batches (size 1) simulations per example. After the batch of simulation finishes, all visited states are updated. Masking to choose next step only among neighbors.",2.0,2.0,22.38859678,EVALUATIONS/WEIGHTS_BACKUP/1621342226754812792#139861976233792,"ER(25, 0.24532699947978218)",1.0
